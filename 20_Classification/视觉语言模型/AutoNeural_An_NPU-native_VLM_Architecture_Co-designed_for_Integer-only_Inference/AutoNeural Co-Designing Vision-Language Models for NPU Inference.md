---
category: 视觉语言模型
classification_reason: 该论文提出了一种名为AutoNeural的端侧视觉语言模型（VLM）架构，专门针对NPU特性进行了CNN与SSM的混合架构设计，以解决传统ViT在NPU上的量化和I/O瓶颈。虽然涉及量化和架构设计，但其核心产出是一个具体的VLM模型，因此归入现有的'视觉语言模型'分类最为精准。
created: '2026-01-18'
status: unread
tags:
- NPU原生架构
- 状态空间模型
- 全整型推理
- 硬件协同设计
- 混合架构
title: 'AutoNeural: An NPU-native VLM Architecture Co-designed for Integer-only Inference'
---

# AutoNeural: Co-Designing Vision–Language Models for NPU Inference

Wei Chen Nexa AI alexchen@nexa.ai Liangmin Wu Geely Auto Liangmin.Wu@geely.com Yunhai Hu Nexa AI yunhai@nexa.ai Zhiyuan Li Nexa AI zack@nexa.ai

Zhiyuan Cheng Nexa AI perry@nexa.ai Yicheng Qian Nexa AI david@nexa.ai Lingyue Zhu Nexa AI alan@nexa.ai Zhipeng Hu Nexa AI victorhu@nexa.ai

Luoyi Liang Geely Auto Luoyi.Liang@geely.com Qiang Tang Geely Auto Qiang.Tang2@geely.com Zhen Liu Geely Auto Zhen.Liu23@geely.com

> Han Yang† Geely Auto Han.Yang6@geely.com

# Abstract

While Neural Processing Units (NPUs) offer high theoretical efficiency for edge AI, state-of-the-art Vision–Language Models (VLMs) tailored for GPUs often falter on these substrates. We attribute this hardware-model mismatch to two primary factors: the *quantization brittleness* of Vision Transformers (ViTs) and the *I/Obound nature* of autoregressive attention mechanisms, which fail to utilize the high arithmetic throughput of NPUs. To bridge this gap, we propose AutoNeural, an NPU-native VLM architecture co-designed for integer-only inference. We replace the standard ViT encoder with a MobileNetV5-style backbone utilizing depthwise separable convolutions, which ensures bounded activation distributions for stable INT4/8/16 quantization. Complementing this, our language backbone integrates State-Space Model (SSM) principles with Transformer layers, employing efficient gated convolutions to achieve linear-time complexity. This hybrid design eliminates the heavy memory I/O overhead of Key-Value caching during generation. Our approach delivers substantial efficiency gains, reducing quantization error of vision encoder by up to 7× and end-to-end latency by 14× compared to conventional baselines. The AutoNeural also delivers 3× decoding speed and 4× longer context window than the baseline. We validate these improvements via a real-world automotive case study on the Qualcomm SA8295P SoC, demonstrating real-time performance for cockpit applications. Our results highlight that rethinking model topology specifically for NPU constraints is a prerequisite for robust multi-modal edge intelligence.

# 1 Introduction

Vision–language models (VLMs) [\[1,](#page-10-0) [2,](#page-10-1) [3,](#page-10-2) [4,](#page-10-3) [5\]](#page-10-4) have fundamentally advanced the capability to jointly reason over images and text, achieving remarkable success in recognition, grounding, captioning,

<sup>†</sup>Correspondence author

and visual instruction following. The dominant paradigm typically grafts a pretrained Vision Transformer (ViT) onto a decoder-only Large Language Model (LLM) via a projection layer, aligning the modalities through visual instruction tuning. While this GPU-centric design yields high accuracy at large resolutions, it introduces systemic inefficiencies when deployed on edge devices. Specifically, it suffers from: (i) high inference latency, dominated by the heavy computational cost of vision encoding and the autoregressive generation of the LLM (affecting both time-to-first-token and time-per-token); and (ii) inherent brittleness under low-precision execution, owing to the attention-heavy architectures in both vision and language components that are sensitive to quantization.

Why NPUs change the design space. Neural Processing Units (NPUs) [\[6,](#page-10-5) [7,](#page-10-6) [8\]](#page-10-7) have emerged as the default compute substrate for mobile and edge intelligence, delivering high TOPS and energy efficiency through specialized integer operators and on-chip SRAM. However, state-of-the-art VLMs optimized for floating-point GPU execution often degrade significantly under NPU-friendly INT-x quantization constraints. We identify two root causes for this hardware–software mismatch. First, Vision Transformers (ViTs) [\[9,](#page-10-8) [10\]](#page-10-9) exhibit *quantization brittleness*—activations and weights in multi-head attention and RMSNorm [\[11\]](#page-10-10) paths often possess outlier distributions that are sensitive to low-precision scaling, leading to sharp accuracy collapse at INT8/16. Second, Transformer-based language backbones suffer from *memory I/O bottlenecks*. The autoregressive generation process necessitates repeated Key-Value (KV) cache access, creating memory-bound operations that saturate on-chip bandwidth. Consequently, the NPU's compute units stall despite high nominal TOPS, inflating latency and reducing end-to-end throughput.

Prefilling time dominates user-perceived latency. For interactive VLMs, system responsiveness is primarily governed by Time-To-First-Token (TTFT), which aggregates the latency of the vision encoder and the LLM prefilling phase across all input tokens. As input resolution or tiling strategies scale up to improve visual fidelity, the number of visual tokens increases quadratically, disproportionately elongating the prefilling phase. Consequently, the conventional strategy of scaling ViT width or depth yields diminishing returns on NPUs: marginal accuracy gains are negated by non-linear growth in TTFT and heightened sensitivity to quantization errors.

NPU-native co-design. We posit that achieving robust, low-latency multi-modal intelligence at the edge necessitates an *NPU-native co-design* approach across two dimensions: (1) Topology: Replacing global attention mechanisms with operator- and memory-efficient components that naturally stabilize INT-x inference and minimize memory traffic; (2) Runtime: Implementing scheduling strategies that respect NPU buffer hierarchies and bandwidth limits, constraining peak activation footprints and KV cache overhead to optimize both TTFT and generation speed.

Our approach. We introduce AutoNeural, an NPU-native VLM architecture that fundamentally rethinks vision encoding and language modeling for edge efficiency. For the vision encoder, we depart from standard ViTs in favor of a MobileNet-style architecture utilizing depthwise separable convolutions. This design eliminates the quadratic complexity and quantization brittleness of global attention, providing strong inductive biases for feature extraction while maintaining bounded activation distributions inherently stable for INT8/16 inference. For the language backbone, we adopt a hybrid Transformer-SSM architecture (inspired by Liquid AI principles), which interleaves Transformer layers with efficient gated convolutions based on structural state-space models. This topology offers linear-time complexity and compact state representations, obviating the need for explicit KV caching and reducing memory I/O by up to 60% during generation. To our knowledge, this is the first framework to unify convolution-based visual stability with linear-complexity state-space modeling, effectively decoupling multi-modal reasoning capability from the heavy computational burden of global attention. We couple this architecture with an NPU-aware training recipe that integrates quantization-aware fine-tuning (QAT) [\[12,](#page-10-11) [13,](#page-10-12) [14\]](#page-11-0), mixed-precision constraints, and hardware-aligned calibration.

Key findings. Our architecture yields up to 7× lower quantization error and 14× lower endto-end latency versus ViT-Transformer baselines under the same NPU precision constraints. The MobileNet encoder contributes to stable INT8/16 inference with minimal quantization degradation, while the Liquid AI backbone reduces memory I/O by up to 60% during autoregressive generation. Hardware measurements on a commercial automotive SoC (Qualcomm SA8295P NPU) show realtime responsiveness in an in-car assistant scenario (Figure [2\)](#page-5-0), validating that both TTFT and time-per-

<span id="page-2-0"></span>![](_page_2_Figure_0.jpeg)

Figure 1: Architecture overview of AutoNeural. The model comprises: (1) a MobileNet-based vision encoder with Multi-Scale Fusion Adapter (MSFA) that processes 768×768 images into 256 visual tokens, (2) a lightweight two-layer MLP connector without normalization for NPU quantization robustness, and (3) the Liquid AI 1.2B hybrid backbone with 16 layers that interleaves 10 gated-convolution layers with 6 Transformer attention layers to reduce memory I/O.

token can be dramatically reduced without sacrificing accuracy. Ablations isolate the contribution of the MobileNet encoder, SSM integration, and token budgeting to the observed latency–accuracy Pareto improvements.

Contributions. In conclusion, this work makes three primary contributions to enable efficient vision–language modeling on NPUs:

- We propose a novel NPU-native architecture combining a depthwise-convolutional vision encoder with a hybrid Transformer-SSM language backbone. This design solves the twin challenges of quantization brittleness and memory bottlenecks, outperforming traditional ViT-LLM pairs in INT8/16 robustness and efficiency.
- We introduce a proprietary automotive-specific dataset comprising 200k annotated samples. This dataset covers critical cockpit AI tasks—including driver monitoring, vehicle security, and parking localization—spanning diverse demographics and environmental conditions to benchmark domain-specific performance.
- We develop a comprehensive NPU-aware training framework, integrating QAT and calibration procedures specifically designed to minimize post-quantization drift and align vision–language representations for deployment on the Qualcomm SA8295P platform.

# 2 Related Work

Our work sits at the intersection of three research areas: multimodal models for automotive applications, NPU-optimized inference, and efficient vision encoders for on-device deployment.

Multimodal models for automotive deployment. The automotive industry has increasingly adopted vision–language models to enable advanced driver assistance systems (ADAS), in-car assistants, and autonomous driving capabilities [\[15,](#page-11-1) [16\]](#page-11-2). Recent work has explored the application of large multimodal models in automotive contexts, ranging from scenario generation to real-time perception. For instance, multimodal LLMs [\[17,](#page-11-3) [1\]](#page-10-0)have been applied to autonomous driving tasks such as scenario generation and perception, demonstrating the potential of integrating visual and textual reasoning for improved situational awareness[\[18,](#page-11-4) [19,](#page-11-5) [20,](#page-11-6) [21,](#page-11-7) [22\]](#page-11-8). However, most of these systems rely on GPU-based inference and have not been optimized for the power and latency constraints of invehicle NPUs. The deployment of such models in production vehicles requires careful co-design with

edge hardware, a gap our work directly addresses. While existing automotive multimodal systems often compromise on model capacity or resolution to meet latency budgets, our MobileNet-Hybrid Transformer-SSM architecture demonstrates that hardware-aware design enables both high accuracy and low latency.

Models on NPU architectures. Neural Processing Units from vendors such as Qualcomm [\[23,](#page-11-9) [24\]](#page-11-10), MediaTek [\[25\]](#page-11-11), and Apple have become the primary compute substrate for on-device AI across mobile and automotive platforms. Recent efforts have focused on adapting large multimodal models for NPU deployment through quantization [\[26,](#page-11-12) [27\]](#page-11-13), pruning [\[28\]](#page-11-14), and operator fusion [\[29\]](#page-11-15). For example, work on Qualcomm platforms [\[30,](#page-11-16) [31,](#page-11-17) [32,](#page-11-18) [33\]](#page-12-0) has demonstrated that quantization-aware training and low-rank factorization can accelerate inference while maintaining acceptable accuracy. Similarly, frameworks like MindVL [\[34\]](#page-12-1) have explored efficient training pipelines on Ascend NPUs, integrating distributed data loading and system-level scheduling. MiniCPM-V [\[35,](#page-12-2) [36\]](#page-12-3) represents another line of work targeting edge deployment through aggressive model compression, achieving strong performance on mobile devices. However, these approaches primarily focus on post-hoc optimization of GPU-first architectures rather than fundamental architectural redesign. In contrast, our work rethinks both the vision encoder and language backbone to match NPU operator sets and memory hierarchies, yielding more substantial latency and efficiency gains.

Efficient vision encoders for on-device deployment. The choice of vision encoder critically impacts both accuracy and deployment efficiency in multimodal systems. While Vision Transformers have dominated recent VLM architectures due to their strong performance, their global attention mechanisms and quantization brittleness pose significant challenges for on-device deployment. Recent work has explored alternative encoder designs optimized for edge scenarios. MobileNet architectures [\[37,](#page-12-4) [38,](#page-12-5) [39,](#page-12-6) [40\]](#page-12-7), originally designed for efficient image classification, employ depthwise separable convolutions that reduce parameter count and computation while maintaining strong inductive biases for visual features. Notably, PaliGemma [\[41,](#page-12-8) [42\]](#page-12-9), a recent 3B parameter VLM from Google, demonstrates that carefully designed smaller vision encoders can achieve competitive transfer performance. Beyond vision encoders, recent advances in sequence modeling have introduced State Space Models (SSMs) [\[43,](#page-12-10) [44,](#page-12-11) [45,](#page-12-12) [46\]](#page-12-13)as efficient alternatives to Transformers for language modeling. SSMs achieve linear-time complexity through selective state spaces and eliminate the need for explicit key-value caching, making them particularly attractive for memory-constrained NPU deployment. Building on these principles, Liquid Foundation Models [\[47,](#page-12-14) [48,](#page-12-15) [49\]](#page-12-16) demonstrate efficient sequence modeling architectures that combine linear-time complexity with strong representational capacity through liquid structural state-space modeling. Our work adopts the Liquid AI architecture as our language backbone and systematically pairs it with a MobileNet-based encoder optimized for NPUaware quantization. By replacing global attention in both the vision and language components, we address the dual bottlenecks of quantization brittleness and memory I/O that plague existing VLM architectures on NPUs.

# 3 Methodology

We present the architectural design and training protocol of our NPU-native vision–language model. Our approach consists of two key components: (1) a MobileNet-based vision encoder for efficient visual feature extraction; (2) a hybrid Transformer-SSM language backbone that reduces memory I/O during autoregressive generation.

#### 3.1 Model Architecture

Vision encoder Our vision encoder builds on MobileNetV5, exploiting depthwise separable convolutions for efficient feature extraction with bounded activations, and is initialized from the Gemma 3n-E4B vision checkpoint to align with the multimodal design of Gemma 3n [\[50,](#page-12-17) [51\]](#page-12-18). It consumes 768×768 inputs and produces a spatial feature map that is flattened into visual tokens. In contrast to ViTs with global self-attention, we employ a hierarchical stack of inverted residual (IR) blocks that naturally yields multi-scale features while preserving local receptive fields; this local-first design reduces quantization sensitivity, as convolution + normalization maintain stabler activation ranges than LayerNorm and multi-head attention, improving INT8/16 robustness. The network begins with a 3×3 stride-2 stem, followed by four stages in which stride-2 IR blocks

downsample and stride-1 IR blocks refine. To inject long-range context at bounded cost, the late, low-resolution stages interleave sparse multi-query attention (MQA) bottlenecks. We aggregate information via a *Multi-Scale Fusion Adapter* (MSFA): the last two stage outputs are upsampled to the finest tapped resolution, channel-concatenated, and processed by a universal inverted residual layer (pointwise→depthwise→pointwise) with GELU (tanh) activations and RMSNorm; a 3×3 stride-3 average pool yields a fused 16×16×2048 tensor. This spatial feature map is then flattened into 256 visual tokens, where each of the 16×16 spatial locations becomes a token with 2048-dimensional features, balancing fidelity and LLM prefill cost while preserving accuracy and latency benefits.

Vision-Language connector The vision-language connector serves as the alignment bridge between the visual encoder's output space and the language model's embedding space, transforming the visual token representations produced by the MobileNet encoder into the token embeddings expected by the language model backbone.

Our connector architecture employs a lightweight two-layer multi-layer perceptron (MLP) with GELU (Gaussian Error Linear Units) activation between the hidden layer. This design deviates from the Gemma 3n-E4B projector, which incorporates two RMSNorm layers within the projection path. We deliberately avoid normalization layers in the connector to maintain NPU quantization robustness. Contemporary NPU quantization pipelines rely on static per-tensor quantization, where activation ranges must be determined at calibration time and fixed during deployment. RMSNorm introduces significant quantization challenges: its dynamic scaling operation—computing the root-mean-square over features at each position—produces activations with input-dependent distributions that are difficult to calibrate accurately with static ranges.

Language model backbone For the language model, we adopt the Liquid AI 1.2B parameter architecture, a hybrid design that strategically interleaves Transformer self-attention layers with efficient sequence modeling layers based on liquid structural state-space principles. The sequence modeling layers implement gated convolutions with depthwise short-kernel operations, providing linear-time complexity during inference while maintaining compact state representations that avoid explicit key-value cache overhead. The Liquid AI architecture employs a 5:3 ratio with 16 total layers—10 layers use gated-convolution sequence modeling while 6 layers use Transformer selfattention. This design is particularly well-suited for NPU deployment for two reasons: (1) the convolution-based layers provide linear-time complexity during autoregressive generation, eliminating the quadratic scaling of attention and dramatically reducing memory footprint, and (2) the preserved Transformer layers maintain strong in-context learning and reasoning capabilities essential for vision–language tasks. The gated-convolution layers maintain a rolling state cache that operates with bounded memory during inference, requiring only a fixed-size context window rather than the full key-value history. During generation, this hybrid design reduces memory bandwidth pressure on the NPU by up to 60% compared to a pure Transformer baseline, as the majority of layers avoid explicit KV cache reads/writes. Each sequence modeling layer is followed by a feed-forward network with SwiGLU activation, with RMSNorm applied before both types of layers to stabilize training and maintain activation distributions suitable for quantization. The complete architecture is illustrated in Figure [1.](#page-2-0)

# 3.2 Automotive dataset

We collected and labeled 0.2M samples related to the intelligent cockpit mission from scratch. Approximately 400 volunteers participated in the data collection process, encompassing different ages, genders, and skin colors to improve the generalization performance of the dataset. After data collection, we used a combination of automated and manual annotation to accurately label the data. Specifically, the dataset mainly includes the following task types: (1) AI Sentinel (56K samples): After the vehicle is turned off and locked, the camera monitors the surrounding environment of the vehicle in real time and identifies destructive behaviors from the external environment, such as scratching, prying, or painting the vehicle, so as to provide 24-hour real-time protection for the vehicle; (2) AI Greeter (50K samples): When an acquaintance approaches the car, the user's identity is identified and confirmed to assist the acquaintance in unlocking the car or opening the trunk in advance, realizing humanized care; (3) AI Car Finder (44K samples): Identifies important signs related to the vehicle's location in the parking lot, such as the floor, area, parking space, and surrounding vehicles, enabling the car owner to quickly and accurately find the vehicle; (4) Safety

<span id="page-5-0"></span>![](_page_5_Picture_0.jpeg)

# AutoNeural

![](_page_5_Picture_2.jpeg)

Figure 2: Performance of AutoNeural after mixed-precision quantization (vision encoder: W8A16, language model: W4A16) and deployment on Qualcomm SA8295P NPU. Results reflect actual on-device execution, not PyTorch simulation, demonstrating stable accuracy and substantial latency improvements.

when Getting Off or Starting the Car (50K samples): When passengers get off or the car starts, it identifies potential danger factors around the vehicle, such as obstacles, pedestrians, or small animals, so as to provide safety reminders to the user.

#### 3.3 NPU deployment validation

To validate the effectiveness of our architectural choices, we deployed the quantized AutoNeural model on the Qualcomm SA8295P NPU and measured real-world performance characteristics. Figure [2](#page-5-0) presents the performance profile after mixed-precision quantization and on-device deployment, where the vision encoder operates at W8A16 (8-bit weights, 16-bit activations) and the language model backbone at W4A16 (4-bit weights, 16-bit activations). Critically, these measurements reflect actual NPU execution, not PyTorch simulation. The results demonstrate that our MobileNet encoder maintains stable accuracy under W8A16 quantization with minimal degradation, while the hybrid Transformer-SSM backbone achieves up to 60% reduction in memory bandwidth during generation even at aggressive W4A16 precision. These on-device measurements confirm that our NPU-native codesign delivers substantial latency improvements—up to 14× faster than ViT-Transformer baselines under comparable quantization settings—while preserving task accuracy.

#### 3.4 Cockpit AI and automotive applications

Modern automotive cockpits are evolving from traditional human-machine interfaces into intelligent multimodal assistants that understand both visual and textual contexts [\[52,](#page-13-0) [53\]](#page-13-1). Edge-deployed multimodal models enable diverse interaction scenarios critical for driving safety and user experience. These include real-time interpretation of dashboard indicators and warning lights with actionable recommendations based on vehicle sensor data. Scene understanding for navigation assistance under

challenging conditions, such as nighttime or adverse weather when visibility is compromised, and interpretation of traffic signs and road conditions through external cameras help prevent navigation errors. The system also supports analysis of documents captured via in-cabin cameras, multimodal interaction combining gesture recognition with voice commands for hands-free operation, and driver state monitoring to detect fatigue or anxiety and provide timely interventions through environmental adjustments.

Unlike general-purpose VLMs, automotive cockpit AI faces stringent deployment constraints. Latency requirements demand responses within milliseconds to maintain driver attention and safety, while strict power budgets are dictated by vehicle electrical systems. Furthermore, cockpit deployments must maintain robustness across extreme environmental conditions including varying illumination from direct sunlight to nighttime darkness, temperature fluctuations ranging from sub-zero to high heat, mechanical vibration, and diverse user populations with different interaction patterns. These constraints necessitate NPU-native architectures that deliver consistent low-latency performance under quantized inference while maintaining high accuracy across automotive-specific visual and linguistic domains.

# 4 Experiments

### 4.1 Training setup

We utilize the Infinity-MM dataset [\[54\]](#page-13-2), a large-scale, high-quality multimodal instruction dataset designed for comprehensive vision–language training. Infinity-MM contains approximately 44.8 million instruction-following samples spanning diverse visual understanding capabilities. The dataset is carefully curated to balance multiple task categories including general VQA covering everyday visual reasoning, object recognition, and scene understanding; document understanding with forms, receipts, scientific papers, and business documents; chart and diagram reasoning; OCR-centric tasks; multi-turn conversations; and specialized domains such as medical imaging, autonomous driving scenarios, and industrial inspection. Each sample consists of an image, an instruction sequence, and reference responses. The dataset emphasizes high-quality, detailed responses that require multi-step reasoning rather than simple label recognition, making it particularly suitable for training models that need to provide thorough explanations—critical for automotive safety applications where users must understand the model's reasoning process.

#### 4.1.1 Training Protocol

Our training follows the four-stage curriculum prescribed by the Infinity-MM training protocol, progressing from basic image-text associations to complex instruction-following with increasing task diversity and image resolution.

Stage 1: Image-text association learning. In the first stage, we freeze both the vision encoder and the language model, training only the vision-language projection layer to establish basic image-text associations. We use high-quality image-caption pairs from Infinity-MM to optimize a next-token prediction objective where the model learns to generate captions conditioned on visual tokens. This stage runs for 1 epoch with a learning rate of 1e-3, using AdamW optimizer with β<sup>1</sup> = 0.9, β<sup>2</sup> = 0.95, and a cosine learning rate schedule with warmup. The effective batch size is set to 512 with gradient accumulation.

Stage 2: General visual task training. In the second stage, we unfreeze all parameters including both the vision encoder and language model, and train on general visual understanding tasks including object recognition, scene understanding, and basic visual question answering. We train with a learning rate of 1e-5 for 1 epoch, using the same optimizer configuration. For the first 600 training steps, a batch size of 16 is adopted to mitigate gradient explosion. Then the effective batch size is set to 512 with gradient accumulation for the subsequent steps.

Stage 3: Instruction-specific fine-tuning. The third stage focuses on specialized instructionfollowing capabilities across the diverse task categories in Infinity-MM. We train on the full dataset comprising document understanding, chart reasoning, OCR-centric tasks, and multi-turn conversations. Following the Infinity-MM recipe, we employ task-specific mixture weighting that allocates

35% to general VQA, 25% to document understanding, 20% to chart reasoning, 15% to OCR tasks, and 5% to multi-turn and specialized domains. The learning rate is 1e-5, and we train for 1 epoch with an effective batch size of 512. The training objective is standard autoregressive language modeling loss with next-token prediction.

Stage 4: Domain-specific quantization-aware fine-tuning and synthetic data integration. The final stage integrates both high-quality synthetically generated data and our custom automotive cockpit dataset to enhance the model's robustness and domain-specific capabilities. Following the Infinity-MM protocol, we incorporate synthetic samples that augment underrepresented task categories and challenging edge cases relevant to automotive deployment scenarios. Additionally, we introduce 0.2M automotive-specific samples collected and annotated, covering four critical cockpit AI tasks: AI Sentinel (56K samples for vehicle security monitoring), AI Greeter (50K samples for identity recognition and access control), AI Car Finder (44K samples for parking lot localization), and Safety Monitoring (50K samples for passenger egress and ingress safety). This automotive dataset was collected by approximately 400 volunteers across diverse demographics to ensure generalization across varied lighting conditions, vehicle types, and user populations. We use the quantization-aware training (QAT) method to fine-tune all parameters with a learning rate of 1e-5 for 1 epoch with an effective batch size of 512 using a mixture that balances 60% synthetic data and 40% automotive data. This stage employs careful quality filtering to ensure all samples maintain high annotation standards suitable for safety-critical automotive deployment.

#### 4.2 Comparison with other models

We evaluate our NPU-native architecture against state-of-the-art vision–language models across five diverse multimodal benchmarks. Our comparison includes recent VLMs spanning different architectural families and parameter scales: InternVL2 (1B and 2B variants) representing ViT-Transformer architectures optimized for GPU deployment, and Qwen2-VL (2B and 3.75B variants) [\[55\]](#page-13-3) demonstrating strong general-purpose multimodal capabilities. We report performance of our AutoNeural model trained to Stage 3 of the Infinity-MM curriculum, which achieves the best balance between accuracy and training efficiency.

Benchmark suite. We evaluate on five benchmarks targeting distinct multimodal capabilities. MM-Star measures core vision–language reasoning through multiple-choice visual questions requiring multi-step inference and commonsense knowledge. HallusionBench evaluates language hallucination and visual illusion for large vision-language models. MathVista\_MINI assesses mathematical reasoning over diagrams, charts, and geometric figures, testing the model's ability to extract quantitative information and perform numerical computations. AI2D\_TEST evaluates diagram understanding and scientific reasoning on annotated illustrations from textbooks and educational materials. OCRBench tests scene text recognition and document understanding across diverse text-in-image scenarios including natural scenes, documents, and signage. We report accuracy on each benchmark and compute the average score across all tasks to measure overall multimodal competence.

Results. Table [1](#page-8-0) presents the comprehensive benchmark comparison including ablation studies. AutoNeural achieves an average score of 60.75 across all benchmarks, demonstrating competitive performance with its NPU-optimized architecture and 1.47B parameter count. Notably, AutoNeural outperforms InternVL2-1B (55.96 average) and approaches InternVL2-2B (61.97 average) despite having 33% fewer parameters. To isolate the impact of vision encoder and language model backbone, we conduct ablation experiments: InternViT-Qwen (InternViT + Qwen2.5-1.5B, 63.08 average) vs. MobileNet-Qwen (MobileNetV5 + Qwen2.5-1.5B, 65.15 average) demonstrates that the MobileNet encoder is not worse than Vision Transformers while delivering 14× faster latency. Comparing MobileNet-Qwen (65.15) against AutoNeural (60.75) isolates the language backbone impact, showing that the Liquid AI LFM2-1.2B achieves a favorable trade-off: modest accuracy reduction for 2.9× higher decode throughput and 4× larger context length on NPU hardware.

#### 4.3 Latency and quantization analysis

While accuracy metrics demonstrate competitive performance, the primary motivation for our NPUnative architecture is to achieve substantially lower latency under edge deployment constraints. We

<span id="page-8-0"></span>Table 1: Comparison of AutoNeural with state-of-the-art vision–language models including ablation studies across five multimodal benchmarks. All models are evaluated in full precision. Ablation rows (InternViT-Qwen, InternViT-Liquid, MobileNet-Qwen) systematically analyze vision encoder (InternViT vs. MobileNet) and language backbone (Qwen2.5-1.5B vs. LFM2-1.2B) contributions. All ablation models are trained by us using the same training strategy. All scores are accuracy percentages.

| Model                  | Params | AI2D<br>TEST | Bench | Hallusion MathVista MMStar<br>MINI |       | OCR<br>Bench | Avg   |
|------------------------|--------|--------------|-------|------------------------------------|-------|--------------|-------|
| InternVL2-1B           | 0.94B  | 64.09        | 54.26 | 40.30                              | 45.47 | 75.70        | 55.96 |
| InternVL2-2B           | 2.2B   | 73.90        | 58.36 | 49.10                              | 50.07 | 78.40        | 61.97 |
| Qwen2-VL-2B            | 2.2B   | 74.64        | 62.04 | 47.10                              | 47.80 | 80.90        | 62.50 |
| Qwen2.5-VL-3B-Instruct | 3.75B  | 81.51        | 63.62 | 62.00                              | 56.73 | 82.70        | 69.31 |
| InternViT-Qwen         | 1.86B  | 77.17        | 55.31 | 57.50                              | 54.93 | 72.50        | 63.08 |
| InternViT-Liquid       | 1.49B  | 72.73        | 54.26 | 52.60                              | 50.87 | 66.00        | 59.29 |
| MobileNet-Qwen         | 1.84B  | 78.63        | 57.94 | 60.60                              | 55.07 | 73.50        | 65.15 |
| AutoNeural             | 1.47B  | 73.80        | 56.05 | 53.10                              | 49.40 | 71.40        | 60.75 |

conducted on-device latency measurements on the Qualcomm SA8295P NPU to quantify the realworld speedup of our MobileNet-based vision encoder compared to ViT-based alternatives. Figure [3](#page-9-0) presents vision encoder latency across three input resolutions: 256×256, 512×512, and 768×768 pixels. Both models are deployed with W8A16 quantization (8-bit weights, 16-bit activations) to ensure fair comparison under identical precision constraints. Our AutoNeural vision encoder (300M parameters) is compared against InternViT-300M, a Vision Transformer encoder with comparable parameter count.

The results reveal dramatic latency advantages across all tested resolutions. At 256×256 resolution, AutoNeural achieves 28.0ms inference time compared to 163.3ms for InternViT-300M, yielding a 5.8× speedup. At 512×512, the gap widens substantially: AutoNeural processes images in 101.7ms while InternViT-300M requires 1415.0ms—a 14× speedup. Most critically, at the native 768×768 resolution used in our full VLM system, AutoNeural maintains real-time performance at 278.1ms per image, whereas InternViT-300M fails to execute due to memory constraints on the NPU hardware. This inability to support high-resolution inputs fundamentally limits ViT-based architectures for automotive cockpit applications, where detailed visual understanding of dashboard indicators, traffic signs, and scene context requires processing at sufficient resolution.

The latency gap stems from fundamental architectural differences. Vision Transformers employ global self-attention with quadratic complexity in the number of patches, generating large intermediate activation tensors. In contrast, our MobileNet encoder's depthwise separable convolutions maintain bounded activation footprints, enable efficient NPU operator fusion, and exhibit predictable memory access patterns that maximize on-chip buffer utilization. The MSFA module further reduces latency by processing only two downsampled feature maps rather than maintaining full-resolution activations throughout the network. These results validate that NPU-native architectural choices are essential for real-time multimodal intelligence at the edge, where latency constraints are as critical as accuracy metrics.

End-to-end system performance. To evaluate the complete system under realistic deployment conditions, we compare AutoNeural-VL against InternVL 2B as a baseline solution on the Qualcomm SA8295P NPU. Table [2](#page-9-1) presents comprehensive metrics including TTFT, maximum image resolution, signal-to-quantization-noise ratio (SQNR), quantization error (RMS Error), LLM Perplexity, decode throughput, and context length. AutoNeural-VL achieves full vision support with 768×768 resolution compared to InternVL 2B's 448×448 limitation, enabling detailed visual understanding crucial for automotive applications. Critically, AutoNeural-VL delivers 14× faster TTFT (∼100ms vs ∼1.40s for 512×512 image), demonstrating the dramatic latency improvements enabled by our NPU-native architecture. The MobileNet encoder maintains superior quantization robustness with 7× lower RMS error (0.562% vs 3.98%) and 17 dB higher SQNR (45 dB vs 28 dB), validating that it is fundamentally more stable under aggressive quantization. To further validate the language model's quantization robustness, we measure perplexity on a held-out test set: the Liquid AI 1.2B backbone exhibits minimal degradation from 21.13 (FP16) to 21.47 (W4A16), corresponding to only 1.6% perplexity increase under aggressive 4-bit weight quantization. This stability confirms that the hybrid

# Vision Encoder Latency Comparison on Qualcomm 8295P NPU

<span id="page-9-0"></span>![](_page_9_Figure_1.jpeg)

Figure 3: Vision encoder latency comparison on Qualcomm SA8295P NPU across three input resolutions. AutoNeural's MobileNet-based encoder achieves 5.8× speedup at 256×256, 14× speedup at 512×512, and successfully processes 768×768 images in real-time while InternViT-300M exceeds NPU memory capacity. Lower latency is better.

<span id="page-9-1"></span>Table 2: End-to-end system performance comparison between AutoNeural-VL (ViT: W8A16, language model: W4A16) and InternVL 2B (ViT: W8A16, language model: W4A16) baseline on Qualcomm SA8295P NPU. AutoNeural-VL achieves 14× faster TTFT, 3× faster decoding speed, 7× lower quantization error, and supports 4× larger context length while maintaining higher throughput and resolution.

| Metric                                    | InternVL 2B | AutoNeural-VL |
|-------------------------------------------|-------------|---------------|
| Vision encoder latency (1 image, 512×512) | ∼1.4 s      | ∼100 ms       |
| Max Image Size                            | 448×448     | 768×768       |
| SQNR                                      | 28 dB       | 45 dB         |
| RMS Error                                 | 3.98%       | 0.562%        |
| LLM Perplexity (FP16 → W4A16)             | -           | 21.13 → 21.47 |
| Decode Throughput                         | ∼15 tok/s   | ∼44 tok/s     |
| Context Length                            | 1024        | 4096          |

Transformer-SSM architecture maintains text generation quality while achieving substantial memory and compute savings. Additionally, AutoNeural-VL achieves 2.9× higher decode throughput (∼44 tok/s vs ∼15 tok/s) and supports 4× larger context length (4096 vs 1024), enabling more complex multimodal reasoning. These results demonstrate that NPU-native co-design delivers substantial improvements across all critical deployment metrics—latency, quantization robustness, throughput, and context capacity—making real-time automotive cockpit AI practical.

Future work. The benchmark results presented in this work are conducted under full precision conditions. While we provide quantization robustness metrics through SQNR measurements for the vision encoder and perplexity analysis for the language model, comprehensive in-vehicle evaluation under real deployment conditions and exploring better quantization approaches remains essential future work. This includes validating quantized model performance across diverse driving scenarios, extending benchmarks to additional NPU platforms, conducting end-to-end quality assessment in production automotive environments, and exploring automated architecture search for hardwarespecific optimization.

# 5 Conclusion

We presented a NPU-native vision–language model architecture that fundamentally rethinks both vision encoding and language modeling for efficient edge deployment. By replacing the standard ViT-Transformer paradigm with a MobileNet-based encoder and hybrid Transformer-SSM backbone, we address the dual bottlenecks of quantization brittleness and memory I/O that plague existing VLM architectures on NPUs. Empirical evaluation across standard multimodal benchmarks reveals that our architecture delivers up to 7× quantization error and 14× lower end-to-end latency compared to ViT-Transformer baselines under the same NPU precision constraints. The MobileNet encoder achieves stable INT8/16 inference with minimal quantization degradation, while the hybrid Transformer-SSM backbone reduces memory I/O by up to 60% during autoregressive generation. Real-world deployment on the Qualcomm SA8295P NPU validates real-time responsiveness in automotive scenarios.

Our work demonstrates that NPU-native co-design is essential for robust, low-latency multimodal intelligence at the edge. The conventional approach of scaling resolution and model capacity exhibits diminishing returns on NPUs, where quantization brittleness and memory bandwidth become the primary bottlenecks. By carefully selecting operators that align with NPU execution models—depthwise separable convolutions for vision, SSMs for language—we unlock the full potential of edge accelerators. This principle extends beyond vision–language models to any multimodal architecture targeting resource-constrained deployment. Future work includes exploring automated architecture search conditioned on hardware profiles, mixed-precision quantization schemes, and validation across diverse edge accelerators to strengthen generalizability.

# References

- <span id="page-10-0"></span>[1] Wei Chen, Zhiyuan Li, and Shuo Xin. Omnivlm: A token-compressed, sub-billion-parameter visionlanguage model for efficient on-device inference, 2024. URL <https://arxiv.org/abs/2412.11475>.
- <span id="page-10-1"></span>[2] Andrés Marafioti, Orr Zohar, Miquel Farré, Merve Noyan, Elie Bakouch, Pedro Cuenca, Cyril Zakka, Loubna Ben Allal, Anton Lozhkov, Nouamane Tazi, et al. Smolvlm: Redefining small and efficient multimodal models. *arXiv preprint arXiv:2504.05299*, 2025.
- <span id="page-10-2"></span>[3] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. *arXiv preprint arXiv:2504.10479*, 2025.
- <span id="page-10-3"></span>[4] Jingyi Zhang, Jiaxing Huang, Sheng Jin, and Shijian Lu. Vision-language models for vision tasks: A survey. *IEEE transactions on pattern analysis and machine intelligence*, 46(8):5625–5644, 2024.
- <span id="page-10-4"></span>[5] Akash Ghosh, Arkadeep Acharya, Sriparna Saha, Vinija Jain, and Aman Chadha. Exploring the frontier of vision-language models: A survey of current methodologies and future directions. *arXiv preprint arXiv:2404.07214*, 2024.
- <span id="page-10-5"></span>[6] Tianxiang Tan and Guohong Cao. Efficient execution of deep neural networks on mobile devices with npu. In *Proceedings of the 20th International Conference on Information Processing in Sensor Networks (Co-Located with CPS-IoT Week 2021)*, pages 283–298, 2021.
- <span id="page-10-6"></span>[7] Tamador Mohaidat and Kasem Khalil. A survey on neural network hardware accelerators. *IEEE Transactions on Artificial Intelligence*, 5(8):3801–3822, 2024. doi: 10.1109/TAI.2024.3377147.
- <span id="page-10-7"></span>[8] Kyuho J Lee. Architecture of neural processing unit for deep neural networks. In *Advances in computers*, volume 122, pages 217–245. Elsevier, 2021.
- <span id="page-10-8"></span>[9] Salman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir, Fahad Shahbaz Khan, and Mubarak Shah. Transformers in vision: A survey. *ACM computing surveys (CSUR)*, 54(10s):1–41, 2022.
- <span id="page-10-9"></span>[10] Feiyang Chen, Ziqian Luo, Lisang Zhou, Xueting Pan, and Ying Jiang. Comprehensive survey of model compression and speed up for vision transformers. *arXiv preprint arXiv:2404.10407*, 2024.
- <span id="page-10-10"></span>[11] Jingjing Xu, Xu Sun, Zhiyuan Zhang, Guangxiang Zhao, and Junyang Lin. Understanding and improving layer normalization. *Advances in neural information processing systems*, 32, 2019.
- <span id="page-10-11"></span>[12] Yixiao Li, Yifan Yu, Chen Liang, Pengcheng He, Nikos Karampatziakis, Weizhu Chen, and Tuo Zhao. Loftq: Lora-fine-tuning-aware quantization for large language models. *arXiv preprint arXiv:2310.08659*, 2023.
- <span id="page-10-12"></span>[13] Yelysei Bondarenko, Riccardo Del Chiaro, and Markus Nagel. Low-rank quantization-aware training for llms. *arXiv preprint arXiv:2406.06385*, 2024.

- <span id="page-11-0"></span>[14] Mengzhao Chen, Wenqi Shao, Peng Xu, Jiahao Wang, Peng Gao, Kaipeng Zhang, and Ping Luo. Efficientqat: Efficient quantization-aware training for large language models. In *Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*, pages 10081–10100, 2025.
- <span id="page-11-1"></span>[15] Ziang Guo, Zakhar Yagudin, Artem Lykov, Mikhail Konenkov, and Dzmitry Tsetserukou. Vlm-auto: Vlm-based autonomous driving assistant with human-like behavior and understanding for complex road scenes. In *2024 2nd International Conference on Foundation and Large Language Models (FLLM)*, pages 501–507. IEEE, 2024.
- <span id="page-11-2"></span>[16] Zilin Huang, Zihao Sheng, Yansong Qu, Junwei You, and Sikai Chen. Vlm-rl: A unified vision language models and reinforcement learning framework for safe autonomous driving. *arXiv preprint arXiv:2412.15544*, 2024.
- <span id="page-11-3"></span>[17] Wei Chen and Zhiyuan Li. Octopus v3: Technical report for on-device sub-billion multimodal ai agent, 2024. URL <https://arxiv.org/abs/2404.11459>.
- <span id="page-11-4"></span>[18] Wei Chen, Zhiyuan Li, and Mingyuan Ma. Octopus: On-device language model for function calling of software APIs. In Weizhu Chen, Yi Yang, Mohammad Kachuee, and Xue-Yong Fu, editors, *Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 3: Industry Track)*, pages 329–339, Albuquerque, New Mexico, April 2025. Association for Computational Linguistics. ISBN 979-8-89176-194-0. doi: 10. 18653/v1/2025.naacl-industry.27. URL <https://aclanthology.org/2025.naacl-industry.27/>.
- <span id="page-11-5"></span>[19] Wei Chen and Zhiyuan Li. Octopus v2: On-device language model for super agent, 2024. URL [https:](https://arxiv.org/abs/2404.01744) [//arxiv.org/abs/2404.01744](https://arxiv.org/abs/2404.01744).
- <span id="page-11-6"></span>[20] Wei Chen and Zhiyuan Li. Octopus v4: Graph of language models. *arXiv preprint arXiv:2404.19296*, 2024.
- <span id="page-11-7"></span>[21] Wei Chen, Zhiyuan Li, Zhen Guo, and Yikang Shen. Octo-planner: On-device language model for planner-action agents, 2024. URL <https://arxiv.org/abs/2406.18082>.
- <span id="page-11-8"></span>[22] Wei Chen, Zhiyuan Li, Shuo Xin, and Yihao Wang. Squid: Long context as a new modality for energyefficient on-device language models, 2024. URL <https://arxiv.org/abs/2408.15518>.
- <span id="page-11-9"></span>[23] Yajie Zhu and Hongtao Lu. Edge-side npu inference optimization: Adaptation research of multimodal large models on qualcomm platforms. *Intelligent Data Analysis*, page 1088467X251342172, 2025.
- <span id="page-11-10"></span>[24] Daliang Xu, Hao Zhang, Liming Yang, Ruiqi Liu, Gang Huang, Mengwei Xu, and Xuanzhe Liu. Fast ondevice llm inference with npus. In *Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 1*, pages 445–462, 2025.
- <span id="page-11-11"></span>[25] Hyunbin Park and Shiho Kim. Overviewing ai-dedicated hardware for on-device ai in smartphones. In *Artificial Intelligence and Hardware Accelerators*, pages 127–150. Springer, 2023.
- <span id="page-11-12"></span>[26] Lu Wei, Zhong Ma, Chaojie Yang, and Qin Yao. Advances in the neural network quantization: A comprehensive review. *Applied Sciences*, 14(17):7445, 2024.
- <span id="page-11-13"></span>[27] Jihene Tmamna, Emna Ben Ayed, Rahma Fourati, Mandar Gogate, Tughrul Arslan, Amir Hussain, and Mounir Ben Ayed. Pruning deep neural networks for green energy-efficient models: A survey. *Cognitive Computation*, 16(6):2931–2952, 2024.
- <span id="page-11-14"></span>[28] Pierre Vilar Dantas, Waldir Sabino Da Silva, Lucas Carvalho Cordeiro, and Celso Barbosa Carvalho. A comprehensive review of model compression techniques in machine learning. *Applied Intelligence*, 54(22): 11804–11844, 2024.
- <span id="page-11-15"></span>[29] Mahsa Salmani and Ilya Soloveychik. Llm inference acceleration via efficient operation fusion. *arXiv preprint arXiv:2502.17728*, 2025.
- <span id="page-11-16"></span>[30] Marco Federici, Davide Belli, Mart Van Baalen, Amir Jalalirad, Andrii Skliar, Bence Major, Markus Nagel, and Paul Whatmough. Efficient llm inference using dynamic input pruning and cache-aware masking. *arXiv preprint arXiv:2412.01380*, 2024.
- <span id="page-11-17"></span>[31] Sara Rajaee, Kumar Pratik, Gabriele Cesa, and Arash Behboodi. Local look-ahead guidance via verifier-inthe-loop for automated theorem proving, 2025. URL <https://arxiv.org/abs/2503.09730>.
- <span id="page-11-18"></span>[32] Deepti Hegde, Rajeev Yasarla, Hong Cai, Shizhong Han, Apratim Bhattacharyya, Shweta Mahajan, Litian Liu, Risheek Garrepalli, Vishal M. Patel, and Fatih Porikli. Distilling multi-modal large language models for autonomous driving, 2025. URL <https://arxiv.org/abs/2501.09757>.

- <span id="page-12-0"></span>[33] Farzad Farhadzadeh, Debasmit Das, Shubhankar Borse, and Fatih Porikli. Lora-x: Bridging foundation models with training-free cross-model adaptation, 2025. URL <https://arxiv.org/abs/2501.16559>.
- <span id="page-12-1"></span>[34] Feilong Chen, Yijiang Liu, Yi Huang, Hao Wang, Miren Tian, Ya-Qi Yu, Minghui Liao, and Jihao Wu. Mindvl: Towards efficient and effective training of multimodal large language models on ascend npus. *arXiv preprint arXiv:2509.11662*, 2025.
- <span id="page-12-2"></span>[35] Tianyu Yu, Zefan Wang, Chongyi Wang, Fuwei Huang, Wenshuo Ma, Zhihui He, Tianchi Cai, Weize Chen, Yuxiang Huang, Yuanqian Zhao, et al. Minicpm-v 4.5: Cooking efficient mllms via architecture, data, and training recipe. *arXiv preprint arXiv:2509.18154*, 2025.
- <span id="page-12-3"></span>[36] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: A gpt-4v level mllm on your phone. *arXiv preprint arXiv:2408.01800*, 2024.
- <span id="page-12-4"></span>[37] Hong-Yen Chen and Chung-Yen Su. An enhanced hybrid mobilenet. In *2018 9th International Conference on Awareness Science and Technology (iCAST)*, pages 308–312. IEEE, 2018.
- <span id="page-12-5"></span>[38] Debjyoti Sinha and Mohamed El-Sharkawy. Thin mobilenet: An enhanced mobilenet architecture. In *2019 IEEE 10th annual ubiquitous computing, electronics & mobile communication conference (UEMCON)*, pages 0280–0285. IEEE, 2019.
- <span id="page-12-6"></span>[39] Zheng Qin, Zhaoning Zhang, Xiaotao Chen, Changjian Wang, and Yuxing Peng. Fd-mobilenet: Improved mobilenet with a fast downsampling strategy. In *2018 25th IEEE International Conference on Image Processing (ICIP)*, pages 1363–1367. IEEE, 2018.
- <span id="page-12-7"></span>[40] Barlian Khasoggi, Ermatita Ermatita, and Samsuryadi Sahmin. Efficient mobilenet architecture as image recognition on mobile and embedded devices. *Indonesian Journal of Electrical Engineering and Computer Science*, 16(1):389–394, 2019.
- <span id="page-12-8"></span>[41] Lucas Beyer, Andreas Steiner, André Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, et al. Paligemma: A versatile 3b vlm for transfer. *arXiv preprint arXiv:2407.07726*, 2024.
- <span id="page-12-9"></span>[42] Andreas Steiner, André Susano Pinto, Michael Tschannen, Daniel Keysers, Xiao Wang, Yonatan Bitton, Alexey Gritsenko, Matthias Minderer, Anthony Sherbondy, Shangbang Long, et al. Paligemma 2: A family of versatile vlms for transfer. *arXiv preprint arXiv:2412.03555*, 2024.
- <span id="page-12-10"></span>[43] Albert Gu, Karan Goel, and Christopher Ré. Efficiently modeling long sequences with structured state spaces. *arXiv preprint arXiv:2111.00396*, 2021.
- <span id="page-12-11"></span>[44] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. In *First Conference on Language Modeling*, 2024. URL <https://openreview.net/forum?id=tEYskw1VY2>.
- <span id="page-12-12"></span>[45] Tri Dao and Albert Gu. Transformers are ssms: Generalized models and efficient algorithms through structured state space duality, 2024. URL <https://arxiv.org/abs/2405.21060>.
- <span id="page-12-13"></span>[46] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher Ré. Combining recurrent, convolutional, and continuous-time models with linear state space layers. *Advances in neural information processing systems*, 34:572–585, 2021.
- <span id="page-12-14"></span>[47] Junfeng Wu, Yi Jiang, Chuofan Ma, Yuliang Liu, Hengshuang Zhao, Zehuan Yuan, Song Bai, and Xiang Bai. Liquid: Language models are scalable and unified multi-modal generators. *arXiv preprint arXiv:2412.04332*, 2024.
- <span id="page-12-15"></span>[48] Jakub Smékal, Jimmy TH Smith, Michael Kleinman, Dan Biderman, and Scott W Linderman. Towards a theory of learning dynamics in deep state space models. *arXiv preprint arXiv:2407.07279*, 2024.
- <span id="page-12-16"></span>[49] Michael Poli, Armin W Thomas, Eric Nguyen, Pragaash Ponnusamy, Björn Deiseroth, Kristian Kersting, Taiji Suzuki, Brian Hie, Stefano Ermon, Christopher Ré, et al. Mechanistic design and scaling of hybrid architectures, 2024. *URL https://arxiv. org/abs/2403.17844*, page 5, 2024.
- <span id="page-12-17"></span>[50] Gemma Team. Gemma 3n. <https://ai.google.dev/gemma/docs/gemma-3n>, 2025. Technical report, Google DeepMind.
- <span id="page-12-18"></span>[51] Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Rivière, et al. Gemma 3 technical report. *arXiv preprint arXiv:2503.19786*, 2025.

- <span id="page-13-0"></span>[52] Zelun Tony Zhang, Yuanting Liu, and Heinrich Hußmann. Pilot attitudes toward ai in the cockpit: implications for design. In *2021 IEEE 2nd International Conference on Human-Machine Systems (ICHMS)*, pages 1–6. IEEE, 2021.
- <span id="page-13-1"></span>[53] Yurii Kovalyov, Tetiana Shmelova, Yuliya Sikirda, and Maxim Yatsko. Intelligent multimodal humanmachine collaboration system for safety and security in the cockpit. In *2024 14th International Conference on Dependable Systems, Services and Technologies (DESSERT)*, pages 1–8. IEEE, 2024.
- <span id="page-13-2"></span>[54] Shuhao Gu, Jialing Zhang, Siyuan Zhou, Kevin Yu, Zhaohu Xing, Liangdong Wang, Zhou Cao, Jintao Jia, Zhuoyi Zhang, Yixuan Wang, Zhenchong Hu, Bo-Wen Zhang, Jijie Li, Dong Liang, Yingli Zhao, Yulong Ao, Yaoqi Liu, Fangxiang Feng, and Guang Liu. Infinity-mm: Scaling multimodal performance with large-scale and high-quality instruction data, 2024. URL <https://arxiv.org/abs/2410.18558>.
- <span id="page-13-3"></span>[55] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution. *arXiv preprint arXiv:2409.12191*, 2024.