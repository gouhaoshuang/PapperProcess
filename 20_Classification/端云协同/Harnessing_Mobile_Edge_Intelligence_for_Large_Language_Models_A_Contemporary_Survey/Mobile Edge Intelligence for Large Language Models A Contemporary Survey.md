---
category: 端云协同
classification_reason: 论文综述了利用移动边缘智能（MEI）和移动边缘计算（MEC）来支持LLM的部署，涉及端侧与边缘侧的计算卸载、边缘缓存、协同训练与推理（如拆分学习），这属于典型的端边/端云协同技术范畴。
created: '2026-01-18'
status: unread
tags:
- 移动边缘智能
- 移动边缘计算
- 拆分学习
- 边缘缓存
- 综述
title: 'Harnessing Mobile Edge Intelligence for Large Language Models: A Contemporary
  Survey'
---

# Mobile Edge Intelligence for Large Language Models: A Contemporary Survey

Guanqiao Qu, *Graduate Student Member, IEEE*, Qiyuan Chen, Wei Wei, *Graduate Student Member, IEEE*, Zheng Lin, *Graduate Student Member, IEEE*, Xianhao Chen, *Member, IEEE*, and Kaibin Huang, *Fellow, IEEE*

*Abstract*—On-device large language models (LLMs), referring to running LLMs on edge devices, have raised considerable interest since they are more cost-effective, latency-efficient, and privacy-preserving compared with the cloud paradigm. Nonetheless, the performance of on-device LLMs is intrinsically constrained by resource limitations on edge devices. Sitting between cloud and on-device AI, mobile edge intelligence (MEI) presents a viable solution by provisioning AI capabilities at the edge of mobile networks. This article provides a contemporary survey on harnessing MEI for LLMs. We begin by illustrating several killer applications to demonstrate the urgent need for deploying LLMs at the network edge. Next, we present the preliminaries of LLMs and MEI, followed by resource-efficient LLM techniques. We then present an architectural overview of MEI for LLMs (MEI4LLM), outlining its core components and how it supports the deployment of LLMs. Subsequently, we delve into various aspects of MEI4LLM, extensively covering edge LLM caching and delivery, edge LLM training, and edge LLM inference. Finally, we identify future research opportunities. We hope this article inspires researchers in the field to leverage mobile edge computing to facilitate LLM deployment, thereby unleashing the potential of LLMs across various privacy- and delay-sensitive applications.

*Index Terms*—Large language models, foundation models, mobile edge computing, edge intelligence, 6G, split learning.

#### I. INTRODUCTION

#### *A. Background*

The recent advent of large language models (LLMs) has been a milestone in artificial intelligence (AI) technology to enable general-purpose intelligence. LLMs excel in various domains, i.e., not only in the task they are built for, i.e., generating text responses, but also in tasks such as multimodal content analysis, summarization, and generalization. For instance, the GPT-4 multimodal model accepts image and text inputs,

The work was supported in part by the Research Grants Council of Hong Kong under Grant 27213824, in part by HKU-SCF FinTech Academy R&D Funding, and in part by HKU IDS Research Seed Fund under Grant IDS-RSF2023-0012. The work of K. Huang described in this paper was supported in part by the Research Grants Council of the Hong Kong Special Administrative Region, China under a fellowship award (HKU RFS2122- 7S04), the Areas of Excellence scheme grant (AoE/E-601/22-R), Collaborative Research Fund (C1009-22G), and the Grant 17212423. Part of the described research work is conducted in the JC STEM Lab of Robotics for Soft Materials funded by The Hong Kong Jockey Club Charities Trust.

Guanqiao Qu, Qiyuan Chen, Wei Wei, Zheng Lin, Xianhao Chen, and Kaibin Huang are with the Department of Electrical and Electronic Engineering, University of Hong Kong, Pok Fu Lam, Hong Kong SAR, China. Xianhao Chen is also with HKU Musketeers Foundation Institute of Data Science, University of Hong Kong, Pok Fu Lam, Hong Kong SAR, China. (email: gqqu@eee.hku.hk; qiyuanchen@connect.hku.hk; weiwei@eee.hku.hk; linzheng@eee.hku.hk; xchen@eee.hku.hk; huangkb@eee.hku.hk). *(Corresponding author: Xianhao Chen.)*

which produces text outputs exhibiting human-level performance on various professional and academic benchmarks. Apart from these general-purpose models, sometimes called foundation models, LLMs can be fine-tuned to downstream tasks, catering to specific industries and application scenarios. For example, medical LLM, such as Med-PaLM M [\[1\]](#page-34-0), was designed by Google to provide high-quality answers based on rich data modalities spanning text, imaging, genomics, and more. Google DeepMind also developed Robotics Transformer 2 (RT-2) [\[2\]](#page-34-1), a vision-language-action AI model for controlling robots. The broad spectrum of use cases highlights the profound impact of LLMs on our everyday lives.

1

Due to extensive resource demands, most commercial LLMs are confined to cloud data centers for service provisioning. Regrettably, cloud-based LLM provisioning brings inherent drawbacks, including data privacy breaches, high bandwidth costs, and long service latency. Specifically, users must upload their data to cloud centers to access LLM services, resulting in significant communication delays. Moreover, uploading private data poses a serious risk to user privacy, especially in privacysensitive applications like smart health. During the transmissions and storage of data on cloud servers, unauthorized data access can expose sensitive personal information, such as medical records and biometric data, to malicious actors [\[3\]](#page-34-2)– [\[5\]](#page-34-3), leading to the misuse of data for malicious purposes and severely compromising user confidentiality. Given these concerns, there is a continuing development trend in ondevice LLM deployment, sparking a competitive race among major industry players. For instance, Google has launched Gemini Nano on Pixel 8 Pro smartphones with 1.8-billion and 3.25-billion parameters, respectively [\[6\]](#page-34-4). Qualcomm plans to launch Llama 2 support on Snapdragon-powered flagship smartphones and personal computers [\[7\]](#page-34-5). On-device LLM deployment enables local processing of sensitive personal data and provides low response time, which is crucial for delaysensitive applications such as robot planning and autonomous driving.

# *B. Motivation: From Cloud LLMs to On-device LLMs to MEI LLMs*

Although on-device LLM is becoming a fast-growing field, the widespread deployment of on-device LLMs faces severe limitations. In particular, the scarcity of computing, memory, and storage resources on edge devices substantially limits the scale of on-device LLM. On the one hand, the existing industrial efforts focus on sub-10B (10 billion parameters) LLMs due to the extensive resource requirements for ondevice deployment [\[8\]](#page-34-6), [\[9\]](#page-34-7). For instance, Google's Gemini Nano, which relies on 4-bit models with 1.8B and 3.25B parameters, can only support relatively "basic" functionalities like summarizing text, suggesting smart replies in context, and checking grammar [\[6\]](#page-34-4). However, as the desired functionalities become more complex, deploying larger-scale LLMs on the device becomes necessary. On the other hand, while on-device fine-tuning paves the way for personalized and context-aware AI, serving as a fundamental block for superior AI performance, the existing on-device LLM products generally do not incorporate on-device training (fine-tuning) functionalities due to the extensive training cost, which is much more resourceintensive than supporting AI inference alone.

To address the aforementioned dilemma, mobile edge computing offers a promising solution. The 6G mobile network aims to deliver low-latency AI inference and training services for a wide range of mobile devices by leveraging the networkendowed computing capabilities, say, on base stations. This leads to a paradigm called "mobile edge intelligence (MEI)". Specifically, MEI sits between on-device AI and cloud-based AI, featuring a modest scale of computing resources located close to users, which is more capable than edge devices yet less powerful than cloud centers. Benefiting from the short distance between edge devices and edge servers, large-scale LLMs can be supported with lower service latency and bandwidth costs. Meanwhile, the 6G edge can continuously fine-tune LLMs for adapting to ever-evolving environments by exploiting the memory, energy, and computing power on edge servers, which is hard to achieve via edge devices alone. As such, the 6G mobile edge is expected to be essential in democratizing LLMs to edge devices, and providing comprehensive discussions and review on this trend is the focus of our survey paper.

#### *C. Comparisons with Prior Surveys and Our Contributions*

While MEI provisions powerful resources, supporting LLM services is still a non-trivial research task. The deployment of LLMs is much more resource-intensive than conventional deep neural networks (DNNs), such as convolutional neural networks (CNNs), which is the main hurdle in bringing LLMs to the network edge. Apart from communication-computing resource requirements [\[19\]](#page-34-8)–[\[21\]](#page-34-9), improving the resource and energy efficiency of LLMs is also crucial for successfully deploying them at the network edge. This survey paper aims to provide a contemporary survey of this converging trend, i.e., MEI and LLMs, *primarily from the perspective of resourceefficient deployment of LLMs with MEI*, including storage efficiency, computing efficiency, and communication efficiency at the network edge. This paper differs from the prior survey papers on efficient LLM training/fine-tuning and inference, such as [\[10\]](#page-34-10), [\[12\]](#page-34-11), [\[14\]](#page-34-12), [\[16\]](#page-34-13)–[\[18\]](#page-34-14), [\[22\]](#page-34-15), [\[23\]](#page-34-16). While these papers focus on improving computing efficiency, including efficient LLM compression methods [\[12\]](#page-34-11), hardware optimization [\[14\]](#page-34-12), efficient fine-tuning [\[18\]](#page-34-14), and fast decoding [\[16\]](#page-34-13), for efficient LLM training and inference, they overlook the impact of communications on LLM training, inference, and caching and delivery, which is a significant bottleneck in mobile edge networks. This paper also differs from existing surveys/articles on LLM edge deployment, such as [\[11\]](#page-34-17), [\[13\]](#page-34-18), [\[15\]](#page-34-19), [\[24\]](#page-34-20), which explore LLM-empowered AI service provisioning with cloud-edge synergy, including generative AI services [\[13\]](#page-34-18), remote LLM inference [\[24\]](#page-34-20), and LLM-driven sensing, digital twins, and task-oriented communications [\[25\]](#page-34-21) in cloud-edge networks. Unfortunately, they do not discuss *resource-efficient deployments*, such as parameter-efficient fine-tuning, split inference/learning, efficient LLM caching and delivery, and *their interplay with wireless edge networks*. Lastly, it is noted that this survey paper is fundamentally different from these papers on "LLMs for networks" [\[26\]](#page-34-22)–[\[28\]](#page-34-23), where the design objective is to employ LLMs to optimize edge networks [\[26\]](#page-34-22), [\[27\]](#page-34-24) or telecommunication networks [\[28\]](#page-34-23) rather than leveraging edge computing to support LLMs. Our paper, instead, explores the coordination of communication-computing resources in mobile edge networks to support LLM application provisioning, including caching, delivery, training, and inference of LLMs services at the network edge. The comparisons with some relevant surveys/papers are provided in Table [I.](#page-2-0) The major contributions of this paper are summarized as follows.

- We present the application scenarios that motivate the deployment of LLMs at the network edge and the supporting MEI framework for LLM service provisioning. Although the use cases of LLMs have been extensively discussed in other places, we will emphasize the necessity or benefits of provisioning these applications at the mobile edge based on their service requirements, including latency, bandwidth, and privacy needs.
- We provide the first comprehensive survey on edge LLM caching and delivery, edge LLM training, and edge LLM inference within 6G edge networks. We will particularly concentrate on the resource-efficient deployment of LLMs to improve the storage, communication, and computing efficiency of LLMs at the network edge.
- We identify several crucial future research directions for the integration of LLMs and MEI, including green edge AI, secure edge AI, and quality-aware edge training for LLMs.

As outlined in Fig. [1,](#page-3-0) the survey is organized as follows. Section [II](#page-2-1) illustrates four key applications that demonstrate the necessity of deploying LLMs at the network edge. Section [III](#page-6-0) presents an overview of LLMs and MEI, and Section [IV](#page-10-0) introduces the state-of-the-art resource-efficient LLM techniques. In Section [V,](#page-16-0) we present the MEI for LLM (MEI4LLM) framework, which supports the deployment of LLMs at the network edge. This framework consists of AI-native architecture, parameter-sharing LLM caching and delivery, distributed LLM training/fine-tuning, and distributed LLM inference. Sections [VI,](#page-18-0) [VII,](#page-22-0) and [VIII](#page-28-0) delve into efficient techniques for edge LLM caching and delivery, edge LLM training, and edge LLM inference, respectively, considering storage efficiency, computing efficiency, and communication efficiency. Finally, we outline the roadmap for future research opportunities in Section [IX](#page-32-0) and present our conclusions in Section [X.](#page-33-0)

TABLE I: Summary of the related surveys/articles.

<span id="page-2-0"></span>

|      |                                                                                                                                                                                                                                                                                                                           | Scenarios                                  |                                  |                      |                       |                 | Perspectives  |               |  |
|------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------|----------------------------------|----------------------|-----------------------|-----------------|---------------|---------------|--|
| Ref. | Description                                                                                                                                                                                                                                                                                                               | Efficient<br>(on-device) LLM<br>techniques | Edge LLM caching<br>and delivery | Edge LLM<br>training | Edge LLM<br>inference | Storage<br>eff. | Comp.<br>eff. | Comm.<br>eff. |  |
| [10] | Reviews recent progress of LLM<br>pre-training,<br>fine-tuning,<br>usage,<br>and<br>capacity<br>evaluation,<br>along<br>with<br>the<br>public<br>resources<br>for<br>deploying LLM.                                                                                                                                       | ✔                                          | ✗                                | ✗                    | ✗                     | ✗               | ✔             | ✗             |  |
| [11] | Introduces<br>resource-efficient<br>ap<br>proaches to deploying LLMs, in<br>cluding model architectures, train<br>ing and inference algorithms, and<br>practical system designs.                                                                                                                                          | ✔                                          | ✗                                | ✔                    | ✔                     | ✔               | ✔             | ✗             |  |
| [12] | Explores efficient LLM deploy<br>ment via model-centric methods<br>(model compression, training, in<br>ference, and architecture design),<br>data-centric approaches (data se<br>lection and prompt engineering),<br>and<br>framework-centric<br>strategies<br>(specific training, inference, and<br>serving frameworks). | ✔                                          | ✗                                | ✗                    | ✗                     | ✗               | ✔             | ✗             |  |
| [13] | Overviews the deployment of AI<br>generated content applications in<br>mobile networks, including mobile<br>devices, edge servers, and cloud<br>centers.                                                                                                                                                                  | ✔                                          | ✗                                | ✔                    | ✔                     | ✗               | ✔             | ✔             |  |
| [14] | Surveys the current hardware ac<br>celeration<br>methods<br>for<br>energy<br>efficient on-device LLM training<br>and inference.                                                                                                                                                                                           | ✔                                          | ✗                                | ✗                    | ✗                     | ✗               | ✔             | ✗             |  |
| [15] | Reviews<br>resource-efficient<br>tech<br>niques for LLM deployment, cov<br>ering computational, memory, en<br>ergy, economic, and network re<br>sources based on their applicabil<br>ity across architecture design, pre<br>training, fine-tuning, inference, and<br>system design.                                       | ✔                                          | ✗                                | ✗                    | ✔                     | ✔               | ✔             | ✔             |  |
| [16] | Summarizes<br>current<br>research<br>on<br>efficient<br>LLM<br>inference,<br>including<br>compression,<br>fast<br>decoding,<br>and<br>optimization<br>for<br>compiler/system/hardware.                                                                                                                                    | ✔                                          | ✗                                | ✗                    | ✗                     | ✔               | ✔             | ✗             |  |
| [17] | Reviews the evolution of low-cost<br>on-device LLM training and infer<br>ence techniques.                                                                                                                                                                                                                                 | ✔                                          | ✗                                | ✗                    | ✗                     | ✔               | ✔             | ✗             |  |
| [18] | Overviews PEFT algorithms for<br>LLMs, reviews computing-efficient<br>applications and techniques, and<br>introduces system design for PEFT.                                                                                                                                                                              | ✔                                          | ✗                                | ✗                    | ✗                     | ✔               | ✔             | ✗             |  |
| Ours | Reviews<br>the<br>state-of-the-art<br>ap<br>proaches to edge LLM training,<br>inference, caching, and delivery in<br>MEI, with an emphasis on enhanc<br>ing storage, computing, and com<br>munication efficiency of LLM de<br>ployment at the network edge.                                                               | ✔                                          | ✔                                | ✔                    | ✔                     | ✔               | ✔             | ✔             |  |

#### II. APPLICATION SCENARIOS

<span id="page-2-1"></span>Although LLMs can be applied to a broad range of tasks, we focus on the application scenarios that motivate the deployment of LLMs at the network edge. As illustrated in Fig. [2,](#page-4-0) we will present four killer LLM-powered applications: mobile health, humanoid robots, virtual assistants, and autonomous driving. We focus on their service requirements, including latency, bandwidth, and privacy requirements, to underscore the need for deploying the LLMs at the network edge.

Mobile health: Healthcare is one of the most promising applications for LLMs. Google's Med-PaLM 2, for example, is an LLM fine-tuned on medical datasets, capable of delivering answers to medical inquiries [\[34\]](#page-34-26). Recently, Fitbit and Google

<span id="page-3-0"></span>![](_page_3_Figure_1.jpeg)

Fig. 1: The outline of this survey.

Research have joined forces to build an LLM to support personalized health and wellness features, which aims to help people get summarization and recommendations from the data generated from their mobile devices [35]. Specifically, this

<span id="page-4-0"></span>![](_page_4_Figure_1.jpeg)

|                 | Mobile health   | Humanoid robots      | Virtual assistants | Autonomous driving   |
|-----------------|-----------------|----------------------|--------------------|----------------------|
| Latency reqs.   | ≤400 ms [29]    | 10-100 ms [30]       | 200 ms [31]        | 10 ms [30]           |
| Bandwidth reqs. | 10-50 Mbps [32] | 80 Mbps-12 Gbps [30] | 144 Mbps [33]      | 80 Mbps-12 Gbps [30] |

Fig. 2: Killer LLM-empowered applications demonstrating the need for deploying LLMs at the network edge and the corresponding latency and bandwidth requirements on practical cases.

model can deliver personalized coaching capabilities, like actionable messages and guidance, based on personal fitness goals. Apart from this, healthcare LLMs can also assist in medical question answering, diagnosis, treatment, and medical report generation [\[36\]](#page-34-33). With these exciting applications, mobile health has the following service requirements, making it better to deploy LLMs at the mobile edge:

- 1) Latency requirements: Some LLM-empowered mobile health applications demand prompt warning messages to avert undesirable health consequences. For example, the state-ofthe-art fall detection algorithms can achieve a latency of 37 ms [\[37\]](#page-34-34). Moreover, the tolerant audio/video conferencing latency for an emergency accident and a routine checkup varies from 0 to 150 ms and 150 to 400 ms, respectively [\[29\]](#page-34-28). LLMs should also achieve low latency to support the aforementioned applications, i.e., triggering warnings and generating advice upon fall detection. Since these scenarios require analytics of high-dimensional data/features to trigger warnings, uploading the data to cloud centers will experience long latency and high delay jitter through the backbone network.
- 2) Bandwidth costs: Medical LLMs often possess multimodal processing capabilities. For instance, Google's Med-PaLM 2 has a multimodal version, called Med-PaLM M, which processes rich data modalities spanning text, imaging, genomics, and more, to interpret the biometrics of subjects. Emerging 5G-enabled mobile health applications also incorporate medical augmented reality/virtual reality (AR/VR), where the requirement ranges from 10–50 Mbps for 360-degree 4K video [\[32\]](#page-34-31). Centralizing such data on the cloud for training or inference will consume significant network bandwidth, which

is expensive for consumers and service providers.

3) Privacy requirements: Health information is among the most sensitive categories of data as defined by numerous laws. For example. Article 9 of the General Data Protection Regulation (GDPR) classifies health data as special personal data [\[38\]](#page-35-0), the collection/processing of which is subject to the data subject's explicit consent. Given the strict regulations and growing public awareness of privacy, mobile health applications demand deploying LLMs at the network edge to enable localized data processing.

Humanoid robots: By harnessing LLMs, there is great potential for instilling human-like intelligence into humanoid robots. This is a version previously in science fiction. One example is Optimus, a general-purpose robotic humanoid under development by Tesla. The recent version, i.e., Optimus Generation 2, demonstrates smooth action about dancing and poaching an egg. Also, by combining LLMs and humanoid robots, NVIDIA has introduced Project GR00T, an initiative that utilizes a general-purpose foundation model for humanoid robot learning, which takes multimodal instructions and past interactions as inputs to generate robotic actions. With the power of LLMs, humanoid robots can efficiently perform numerous tasks, from assisting in warehouses to executing rescue missions to offering support in hospitals, elderly communities, and homes. However, humanoid robot applications also face strict latency and data privacy requirements:

1) Latency requirements: Robotic applications have very stringent latency requirements to enable robots to act swiftly in ever-changing environments and respond immediately to human instructions. According to 3GPP, 5G remote-controlled robotics requires 10-100 ms E2E latency and 2 ms intermediate data uploading latency [\[30\]](#page-34-29), which is usually unattainable with cloud computing that incurs round-trip latency often larger than 100 ms [\[39\]](#page-35-1).

- 2) Bandwidth costs: Robots are equipped with multimodal sensors, thereby involving intensive data communications to servers, including vision data and high dimensional features. According to 3GPP, considering split inference, the required upload data rate varies from 80 Mbps to 12 Gbps, depending on the neural network architecture [\[30\]](#page-34-29), leading to significant bandwidth costs if deployed in the cloud.
- 3) Privacy requirements: A primary concern of robotic applications, especially in smart home environments, is data privacy. Robots gather personal data daily by monitoring and engaging with people, involving highly sensitive data about the owners' daily activities. This necessitates the localized processing of LLMs.

Virtual assistants: LLMs can facilitate the everyday lives of human beings based on virtual assistants deployed on our smartphones or personal computers. While earlier virtual assistants like Siri and Google Assistant focused on basic functions such as audio recognition and Internet searches, the advent of LLMs has transformed the landscape. The advanced virtual assistants can serve as general-purpose agents, fundamentally altering how we interact with computers and phones, manage business tasks, and navigate our daily lives. For example, Microsoft Copilot, powered by OpenAI's GPT-4, can aid users in drafting documents, emails, presentations, and much more [\[40\]](#page-35-2). Virtual assistants also require real-time responses to enhance user experience and localized data processing to preserve data ownership:

- 1) Latency requirements: A virtual assistant must be able to respond with an accurate answer in almost real time. According to NVIDIA, a 200 ms E2E latency can cause human beings to perceptively experience the delay and hamper user experience [\[31\]](#page-34-30). It is important to note that the communication latency requirement would be even stricter as it only constitutes part of the E2E latency. Moreover, data transmissions across backbone networks often experience significant delay jitter, which can degrade service quality. These factors highlight the importance of deploying LLMs at the mobile edge.
- 2) Bandwidth costs: Future virtual assistants process text, audio, images, and videos. For instance, GPT-4 with vision, sometimes referred to as GPT-4V, allows image input to generate answers, such as instructing users on fixing a bike based on an input image. According to 3GPP, split AI image recognition may require an uplink data rate of 144 Mbps [\[33\]](#page-34-32). Given the potential for millions or billions of users, decentralizing this data for processing at the edge can considerably reduce bandwidth consumption costs.
- 3) Privacy requirements: Virtual assistants assist people in their everyday lives, which inevitably involve personal information such as email exchanges, Internet search records, and location information. Moreover, using LLMs to create emails, documents, or presentations may expose proprietary company information. These concerns underscore the importance of processing data on edge devices to maintain privacy and

protect data ownership.

Autonomous driving: Existing autonomous driving solutions mostly rely on a modular approach, which divides driving into separate components, such as perception, prediction, and planning. The modular design inherently possesses limited capabilities for tasks requiring complex and humanlike reasoning, where LLMs excel [\[41\]](#page-35-3). For example, when construction workers are at an intersection, LLMs have shown the ability to reason and make informed decisions about the right route. In the auto industry, Ghost Autonomy secured a \$5 million investment from OpenAI, the company owning GPT models, to bring large-scale and multi-modal LLMs to autonomous driving to handle the long tail of rare and complex driving scenarios [\[42\]](#page-35-4). In China, Geely develops an LLM for autonomous driving offering services like vehicle-to-outside voice interactions and entertainment functions [\[43\]](#page-35-5). Although the relevant development is still in the early stages, it can be anticipated that LLMs will provide in-car and autonomous driving services to consumers soon. Undoubtedly, autonomous driving relies on in-car and edge computing to support realtime requirements and privacy preservation.

- 1) Latency requirements: Autonomous driving is one of the most mission-critical and delay-sensitive applications [\[44\]](#page-35-6). According to 3GPP, autonomous driving cases may have a 10 ms E2E latency requirement [\[30\]](#page-34-29). To generate real-time responses and actions to rapidly changing vehicular environments, deploying LLMs in the cloud is ill-suited, making it essential to move LLMs to the network edge.
- 2) Bandwidth costs: Autonomous vehicles come equipped with multi-modal sensors, including multiple cameras and LiDAR, which generate up to 4TB of data per day [\[45\]](#page-35-7), [\[46\]](#page-35-8). Centralizing such data from a vast number of vehicles could easily overload the backbone network and the cloud. Moreover, according to 3GPP, in the split inference scenario, the uplink data rate ranges from 80 Mbps to 12 Gbps in different model architectures [\[30\]](#page-34-29).
- 3) Privacy requirements: Uploading vehicle data to cloud centers inevitably results in location privacy leakage, which is widely recognized as sensitive personal data. Processing data on vehicles or at the edge can enhance the data privacy of end users.

Lessons Learned: The stringent QoS requirements of LLMempowered applications highlight the necessity of deploying LLMs in edge networks. First, applications demanding ultralow latency cannot be supported by cloud computing due to the significant transmission delays over mobile backhaul and backbone networks. Second, ultra-high bandwidth costs resulting from large volumes of multimodal data not only raise the costs for both users and application developers (which need to pay the Internet service providers) but also result in network congestion. Third, with the sensitivity of the user input data, raw data sometimes must remain locally to prevent privacy leakage. These factors underscore the importance of deploying LLMs at the edge to provide timely, efficient, and privacypreserving AI services to edge devices.

# <span id="page-6-0"></span>III. PRELIMINARIES I: AN OVERVIEW OF LLMS AND MEI

As discussed in Section [II,](#page-2-1) the emergence of LLMs has unlocked new avenues for enhancing various edge applications. To fully leverage MEI to support LLMs within edge networks, it is essential to comprehend the key concepts behind both LLMs and MEI. Towards this direction, Section [III](#page-6-0) provides preliminaries of LLMs and MEI. Section [IV](#page-10-0) reviews resourceefficient LLM techniques, which are indispensable for the edge deployment of LLMs. By laying out the foundational knowledge, these two sections prepare for the subsequent discussions on the integration of LLMs and MEI.

#### *A. Large Language Models*

*1) The Transformer architecture:* LLMs are mostly built with Transformer-based architecture. Transformer [\[47\]](#page-35-9) have sparked a significant paradigm shift in the domain of natural language processing (NLP), demonstrating exceptional performance on a broad range of language tasks, including text classification [\[48\]](#page-35-10), machine translation [\[49\]](#page-35-11) and question answering [\[50\]](#page-35-12). For example, Bidirectional Encoder Representations from Transformers (BERT) [\[51\]](#page-35-13) have achieved stateof-the-art performance in question-answering tasks, showcasing superiority in capturing contextual information efficiently. The breakthroughs from Transformers have extended beyond NLP by achieving tremendous success in computer vision. Transformer models and their variants have been widely employed in various image processing tasks such as image recognition [\[52\]](#page-35-14), object detection [\[53\]](#page-35-15), and image segmentation [\[54\]](#page-35-16). For instance, the Vision Transformer (ViT) [\[52\]](#page-35-14) segments images into non-overlapping patches and utilizes Transformer encoders to extract features, yielding superior detection accuracy over traditional CNNs [\[55\]](#page-35-17).

A representative Transformer architecture is illustrated in Fig. [3.](#page-6-1) Unlike the recursive connections for short-term context and sequential processing in Recurrent Neural Networks (RNNs), Transformers employ self-attention mechanisms to comprehensively capture intricate dependencies between sequence elements to learn long-range relationships. The core of the Transformer architecture design lies in the encoderdecoder architecture, consisting of stacked layers with multihead self-attention mechanisms. These mechanisms prioritize processing different elements in the input sequence, enhancing the model's ability to generate output tokens effectively.

Transformers typically operate as follows [\[10\]](#page-34-10), [\[11\]](#page-34-17), [\[47\]](#page-35-9). First, input data, such as words or sentences, are segmented into token sequences through a process called tokenization, using tokenizers like WordPiece [\[56\]](#page-35-18), [\[57\]](#page-35-19) or Byte-Pair Encoding [\[51\]](#page-35-13), [\[58\]](#page-35-20). These transformed tokens are then converted into vectors by an embedding layer, which also incorporates position encoding to capture the input's sequential information. Transformers then leverage the self-attention mechanism to evaluate the relationships between words within the inputs using their intermediate representations, i.e. query Q, key K, and value V, obtained by multiplying the input vectors with corresponding weight matrices. Afterward, layer normalization and feed-forward networks (FFNs) are applied. By using the self-attention mechanism and stacking multiple layers that

<span id="page-6-1"></span>![](_page_6_Figure_7.jpeg)

Fig. 3: The Transformer architecture, adapted from [\[47\]](#page-35-9).

integrate self-attention, layer normalization, and FFNs, the encoder converts the input sequence into context-rich representations, while the decoder employs these representations to generate the output sequence, considering the input and previously generated tokens. Finally, the output of the last Transformer layer is passed through a linear layer to generate the final result. Moreover, Transformers typically utilize autoregressive decoding to produce results. Each new token is predicted based on the input token sequence and then appended to the input sequence, forming a longer sequence for subsequent inference steps. To avoid redundant computation of previously processed tokens in earlier steps, a key-value (KV) cache can be employed, where the intermediate states, key, and value are stored after each inference step, allowing for more efficient processing.

Self-attention lies in the heart of Transformers. The selfattention mechanisms embedded within Transformers overcome the limitation of short-term context inherent in RNNs, comprehensively grasping long-range dependencies and enhancing their ability to capture intricate relationships in sequences. Although attention modules have been widely used in feed-forward and recurrent networks [\[59\]](#page-35-21), [\[60\]](#page-35-22), Transformers exclusively rely on attention mechanisms and employ a unique implementation (i.e., multi-head attention (MHA)) for parallelization optimization, facilitating scalability on highcomplexity models and large-scale datasets. Other alternatives, such as hard attention [\[61\]](#page-35-23), are inherently stochastic, which necessitates Monte Carlo sampling for attention position sampling. Moreover, in contrast to convolutional or recursive counterparts [\[62\]](#page-35-24)–[\[64\]](#page-35-25), Transformer requires minimal prior knowledge of problem structure. This characteristic renders it suitable for model pre-training via pretext tasks on largescale unlabeled datasets [\[47\]](#page-35-9), [\[51\]](#page-35-13), enabling the encoding of highly expressive and generalizable representations. These representations effectively capture the relationships among entities in a given dataset, laying the groundwork for subsequent supervised fine-tuning in downstream tasks.

*2) Unimodal LLMs:* LLMs mainly refer to advanced transformer-based language models consisting of billions of parameters. These models are extensively pre-trained on massive datasets using deep learning techniques [\[10\]](#page-34-10), [\[65\]](#page-35-26). LLMs excel in tasks related to language comprehension, reasoning, and text generation, making them capable of solving complex tasks by producing coherent and contextually appropriate content. Notable examples include Meta's LLaMA [\[7\]](#page-34-5) and OpenAI's GPT-4 [\[66\]](#page-35-27). Presently, major players in the AI industry are dedicated to crafting their LLMs and applying them across various domains. For instance, OpenAI has developed the highly-regarded chat LLM, GPT-3 [\[67\]](#page-35-28), demonstrating exceptional performance across various NLP tasks, such as text generation and machine translation. Google has introduced the medical LLM, Med-PaLM [\[34\]](#page-34-26), capable of offering expertlevel medical guidance and diagnoses. Facebook proposed an innovative image classification LLM, DEiT [\[68\]](#page-35-29), which integrates self-supervised learning with the Transformer architecture to achieve race-level image classification performance with limited annotated data. These LLMs are trained on extensive and varied datasets available on the Internet [\[69\]](#page-35-30).

A variety of LLMs have been built and evolved based on the Transformer architecture. LLM architectures can be classified into three categories: encoder-only LLMs, encoderdecoder LLMs, and decoder-only LLMs [\[11\]](#page-34-17), [\[28\]](#page-34-23), [\[65\]](#page-35-26). An overview of some popular LLMs with these three architectures is illustrated in Table [II.](#page-7-0) Encoder-only LLMs, e.g. BERT [\[51\]](#page-35-13) and ALBERT [\[70\]](#page-35-31), exclusively consist of encoder components. The encoder is responsible for processing input sequences to generate contextualized representations for each token. Despite lacking a decoder to produce output sequences, encoderonly LLMs still exhibit exceptional performance in various NLP tasks such as text classification, sentence similarity computation, and language understanding due to their efficient feature extraction capabilities and adaptable representations. Encoder-decoder LLMs, exemplified by models like T5 [\[71\]](#page-35-32), represent a pivotal advancement in the NLP domain, integrating both encoder and decoder components within their architectures. The encoder processes input sequences to generate contextualized representations, while the decoder utilizes these representations to generate output sequences, typically in a sequence-to-sequence manner. Encoder-decoder LLMs find widespread application in tasks such as machine translation, text summarization, and question answering, owing to their ability to capture complex linguistic structures and contextual dependencies. Decoder-only LLMs, epitomized by the wellknown GPT series [\[67\]](#page-35-28), [\[72\]](#page-35-33), constitute a significant branch of LLMs, only consisting of a decoder network. This type of LLMs constitutes a significant branch of LLMs. An example of the decoder-only LLM architecture is illustrated in Fig. [4.](#page-7-1) Decoder-only LLMs adopt autoregressive decoding, which is widely used in both decoder-only and encoder-decoder LLMs, to generate output sequences based on previous tokens in the sequence. This architectural design makes them particularly well-suited for tasks where the model generates text

TABLE II: An overview of some popular LLMs.

<span id="page-7-0"></span>

| LLM arch.          | Model name         | Release Year | Parameters    |  |
|--------------------|--------------------|--------------|---------------|--|
|                    | BERT [51]          | 2018         | 110-340M      |  |
| Encoder            | ALBERT [70]        | 2019         | 12-235M       |  |
| only               | RoBERTa [74]       | 2019         | 355M          |  |
|                    | XLNet [75]         | 2019         | 110-340M      |  |
|                    | GPT-1 [73]         | 2018         | 117M          |  |
|                    | GPT-2 [76]         | 2019         | 1.5B          |  |
|                    | GPT-3 [77]         | 2020         | 175B          |  |
| Decoder<br>only    | PaLM [78]          | 2022         | 8-540B        |  |
|                    | Gemini Nano-2 [79] | 2023         | 3.25B         |  |
|                    | LLaMA [80]         | 2023         | 7-65B         |  |
|                    | GPT-4 [66]         | 2023         | Close-sourced |  |
|                    | T5 [71]            | 2019         | 60M-11B       |  |
|                    | BART [81]          | 2019         | 140M          |  |
| Encoder<br>decoder | mT5 [82]           | 2020         | 300M-13B      |  |
|                    | HuBERT [83]        | 2021         | 90M-1B        |  |
|                    | Claude 3 [84]      | 2024         | Close-sourced |  |

<span id="page-7-1"></span>sequentially, such as language generation, text completion, and dialogue response generation.

![](_page_7_Figure_7.jpeg)

Fig. 4: The decoder-only LLM architecture, which is adopted by GPT models [\[73\]](#page-35-36).

*3) Multimodal LLMs:* Since traditional LLMs [\[77\]](#page-35-38), [\[85\]](#page-36-2), [\[86\]](#page-36-3) are mainly applied to textual data, the unimodal model training for LLMs limits their ability to comprehend other data types beyond text. For instance, traditional LLMs like GPT-3 and BERT [\[51\]](#page-35-13) only rely on textual inputs. However, in numerous real-world scenarios, language comprehension is not limited to textual context but also visual cues, auditory signals, and contextual sensing information from diverse sensors.

To address the above issue, academia and industry extensively delve into the paradigm of multimodal LLMs shown in Fig. [5,](#page-8-0) amalgamating various modalities such as text, images, and audio, into a unified framework, unlocking the potential for handling diverse data types. For instance, GPT-4 [\[72\]](#page-35-33) excels at simultaneously processing both image and text inputs, exhibiting human-comparable performance across various benchmark tests. In image description tasks, GPT-4 utilizes both images and associated textual data to generate

<span id="page-8-0"></span>![](_page_8_Figure_1.jpeg)

Fig. 5: The structure of multimodal LLM.

more precise and vivid descriptions, while in speech recognition tasks, it merges speech signals with textual information to improve speech comprehension and conversion. Multimodal perception plays a pivotal role in the pursuit of general AI, driven by the imperative to process complex real-world data [\[87\]](#page-36-4). This necessitates AI models capable of cross-modal information fusion and interactive learning, boosting training performance across multiple perceptual domains.

Multimodal LLMs inherit powerful learning capabilities of LLMs to empower diverse and complex multimodal tasks by integrating foundation models of various modalities. The LLMs provide robust language generation, zero-shot transfer capabilities, and in-context learning, whereas foundation models of other modalities offer informative representations from other data types [\[88\]](#page-36-5), [\[89\]](#page-36-6). Since foundation models of varied modalities are individually pre-trained, the primary challenge in constructing multimodal LLMs lies in how to connect these models to achieve high-performance collaborative training/inference. The predominant research in this domain focuses on refining modality alignment via multimodal pretraining [\[90\]](#page-36-7), [\[91\]](#page-36-8) and multimodal instruction-tuning [\[92\]](#page-36-9), [\[93\]](#page-36-10). Multimodal pre-training learns a common representation across modalities by training the model with multimodal datasets, such as XText [\[94\]](#page-36-11). During training, the model learns to correlate information from diverse modalities by optimizing predefined objectives, thus achieving alignment across modalities. This alignment enhances the model's understanding of inter-modality relationships, leading to superior performance across various cross-modal tasks. Multimodal instructiontuning is a method of fine-tuning based on pre-trained models, aimed at improving model performance on specific tasks. It combines the models with one or more modality-related tasks, then fine-tunes the model using modality-labeled data to improve its alignment with modality-specific tasks. Multimodal instruction-tuning enables models to learn to empower unseen tasks by following new instructions, thereby improving the model's zero-shot performance and generalization capability.

<span id="page-8-1"></span>*4) Practical applications: Generative/Interactive AI:* The rapid development of LLMs has a profound impact on various applications, particularly in generative AI (GAI) and interactive AI (IAI). GAI focuses on creating a wide range of content, including images, text, music, and video [\[95\]](#page-36-12), collectively known as AI Generated Content (AIGC). By utilizing multimodal LLMs trained on high-quality datasets, GAI can effectively create superior AIGC based on input text [\[96\]](#page-36-13). On the other hand, IAI can be regarded as the next phase of GAI. IAI responds to user queries in applications like chatbots and virtual assistants while enabling AI agents to adapt through user interaction, thereby continually improving the accuracy [\[97\]](#page-36-14), [\[98\]](#page-36-15). By leveraging powerful LLMs and the content generation strengths of GAI, IAI enables AI agents to emulate human interaction and generate meaningful and dynamic conversations with users [\[99\]](#page-36-16), [\[100\]](#page-36-17). In this regard, LLMs are also regarded as the cornerstone of IAI because they facilitate complex interactive dialogues.

To enable AI agents to generate more accurate and up-todate responses, retrieval augmented generation (RAG) can be integrated into LLMs to empower both IAI and GAI [\[101\]](#page-36-18). Specifically, LLMs use the input sequence to retrieve relevant data from the external knowledge sources when generating responses, thus improving content generation performance [\[102\]](#page-36-19), [\[103\]](#page-36-20). For example, Google combines RAG with Gemini to enhance LLMs' ability to generate more accurate and contextually relevant responses for specific tasks [\[104\]](#page-36-21). The main advantages of integrating RAG into LLMs are twofold. First, by connecting to knowledge sources rich in the latest information, RAG grounds LLMs on the most factual, accurate, and up-to-date content, reducing the likelihood of "hallucinations" in generated outputs and eliminating the need for frequently adapting LLMs. Second, RAG enables users to verify the sources of the models' responses for improved trustworthiness [\[105\]](#page-36-22).

*5) Industrial progress for LLMs:* LLMs have seen significant advancements in the industry, driven by the maturation of deep learning algorithms [\[106\]](#page-36-23)–[\[108\]](#page-36-24), increased computing capabilities, and the availability of large-scale datasets. Major technology companies, including OpenAI, Google, Microsoft, and Meta, have made substantial investments in LLM research and development, leading to the creation of prominent models like GPT series [\[67\]](#page-35-28), [\[72\]](#page-35-33) and BERT [\[51\]](#page-35-13). These models have demonstrated exceptional performance across a spectrum of NLP tasks, including language translation, text generation, question answering, and sentiment analysis. Furthermore, multimodal LLMs have expanded beyond their initial domain of NLP and are shining in diverse sectors, such as healthcare, autonomous driving, and smart cities. For instance, in healthcare, Med-PaLM [\[34\]](#page-34-26) is devised for medical image analysis, clinical document processing, and patient diagnosis, facilitating accurate diagnoses and treatment decisions by healthcare professionals. In the realm of autonomous driving, DriveMLM [\[109\]](#page-36-25) bridges the gap between language decisions and vehicle control commands, enabling closed-loop autonomous driving in realistic simulators. As can be seen, the proliferation of LLMs offers substantial value across multiple industries.

Recent advancements in on-device LLMs have garnered attention from the industry. For instance, Meta proposed an on-device LLM, named MobileLLM, utilizing deep and thin architectures, embedding sharing, and grouped-query attention mechanisms [\[8\]](#page-34-6). Google introduced a new instructiontuning approach for building a mobile-centric text rewriting LLM [\[110\]](#page-36-26). Nevertheless, on-device LLMs often underperform compared with powerful LLMs of larger model sizes. For example, Google's Gemini Nano-1, designed for on-device deployment, contains only 1.8 billion parameters in a 4-bit format, which is distilled from larger Gemini models [\[79\]](#page-35-40). Due to its compact size, when the capabilities of such a small LLM are insufficient for the requirements of edge devices, these devices may still need to upload data to access largescale LLMs, i.e., on edge servers.

## *B. Mobile Edge Intelligence*

MEI has emerged as a promising paradigm integrating AI with mobile edge computing, revolutionizing the landscape of mobile services and applications [\[111\]](#page-36-27)–[\[114\]](#page-36-28). As the precursor to MEI, mobile edge computing allows connected edge servers in mobile edge networks, such as base stations and wireless access points, to provide computing services to resourcelimited edge devices. This significantly reduces the computing burden on edge devices and avoids the need to directly transfer massive amounts of data to the cloud, which would otherwise incur long transmission latency [\[115\]](#page-36-29), [\[116\]](#page-36-30). MEI can be viewed as the confluence of AI and mobile edge computing. By leveraging mobile edge computing technology, MEI enables connected edge servers to execute AI algorithms with end-edge-cloud synergy, facilitating data collection, caching, processing, and analysis for data captured by edge devices in mobile AI applications. This greatly enhances both the efficiency and speed of data processing while enhancing data privacy [\[117\]](#page-36-31)–[\[119\]](#page-36-32). Specifically, MEI surmounts the limitations of traditional cloud-centric architectures by coordinating computing resources across a hierarchical structure that includes edge devices, edge servers, and cloud centers. This coordination can substantially optimize the performance of localized AI services, such as AI model caching, training, and inference [\[118\]](#page-36-33), [\[120\]](#page-36-34), thereby facilitating the deployment of intelligent applications at the network edge.

By integrating AI and communications, the MEI framework enables mobile networks to provide services beyond communications, establishing a strong foundation for the Intelligence of Everything. Along this line, the usage case "Integrated AI and Communication" has been included in the IMT Framework Recommendation for 6G [\[121\]](#page-36-35). For standardization, the telecommunication standardization organizations 3GPP and ITU have depicted the prospects of edge intelligence in their white papers, respectively. ITU-3172 [\[122\]](#page-36-36) elucidates the necessity of hosting machine learning (ML) functionalities closer to the network edge based on the latency-sensitive requirements of ML applications. In 3GPP release 18 for 5G standardization, MEI aims to support distributed learning algorithms, split AI/ML, and efficient AI model distribution [\[30\]](#page-34-29). The details are elaborated next. First, edge learning, such as federated learning, will be fully supported in edge networks, which enables edge servers to aggregate model updates and knowledge from multiple distributed edge devices, thereby improving the performance of AI/ML models. Second, the split AI/ML over 5G edge networks can facilitate the deployment of AI applications on devices with conflicting requirements, such as computation-intensive, energy-intensive, privacy-sensitive, and delay-sensitive requirements. For example, in the edge split inference, an AI model is partitioned into sub-models, and the computation-intensive and energy-intensive sub-models are offloaded to 5G edge servers (e.g., base stations). The edge server can execute the inference with the edge-side sub-model and the uploaded intermediate data from edge devices. At last, efficient AI model downloading ensures that an AI model can be delivered to edge devices with low latency when edge devices need to adapt to new AI tasks and environments. For example, autonomous driving vehicles need to download a new AI model within 1 second from a 5G edge server when the driving environments change. To integrate network-based AI algorithms into the 5G networks, the MEI framework needs to cater to the request for high-speed and stable data links between edge servers and edge devices. These links can enable high and constant uplink data rates for continuously uploading intermediate data/model updates to the edge server and high downlink data rates in a burst for downloading AI models to edge devices in a timely fashion. Moreover, the core of MEI is to capitalize on the proximity of data sources to edge computing devices (e.g., smartphones, laptops, and wearable gadgets) to enable intelligent decision-making closer to the data source. This distributed computing paradigm exhibits numerous advantages over conventional centralized architectures, including latency reduction, improved bandwidth utilization, data privacy preservation, and enhanced resilience to network failures.

On the application side, MEI carries substantial implications across various domains, such as smart healthcare, autonomous driving, and smart cities [\[123\]](#page-36-37). For instance, in the healthcare sector, MEI empowers real-time monitoring of patient health data and facilitates prompt interventions during emergencies. Likewise, in smart cities, MEI contributes to intelligent traffic management, environmental monitoring, and energy optimization, thereby fostering sustainability and enhancing the quality of life. Edge Intelligence has also witnessed notable industrial advancements, particularly with the proliferation of edge computing technologies and the advent of 5G networks. Leading enterprises, such as Microsoft, Google, Amazon, and NVIDIA, have developed edge AI platforms to support real-time AI services. For edge AI-empowered Internet of Things (IoT) applications, Microsoft "Azure IoT Edge", Google "Cloud IoT", Amazon "Web Services IoT", and NVIDIA "EGX" provide edge AI platforms to bring real-time AI services across a wide range of applications, spanning from live video analytics [\[124\]](#page-36-38), smart home [\[125\]](#page-36-39), and industrial IoT [\[126\]](#page-36-40).

# *C. Lessons Learned*

In the 6G era, LLMs can be deployed distributively across edge devices and servers within mobile edge networks. However, this deployment poses challenges due to the significant communication overhead and computing demands of LLMs. Therefore, it is necessary to advance MEI to efficiently support LLMs while maintaining superior performance, thereby

<span id="page-10-1"></span>![](_page_10_Figure_1.jpeg)

Fig. 6: The convergence of LLMs and MEI.

expanding the applicability of LLMs to mobile AI applications within edge environments. As illustrated in Fig. [6,](#page-10-1) integrating LLMs into mobile edge networks leads to numerous research problems, such as model caching and delivery, training, inference, and resource management.

# <span id="page-10-0"></span>IV. PRELIMINARIES II: RESOURCE-EFFICIENT LLM TECHNIQUES

As we move forward to discuss the problems outlined in Fig. [6,](#page-10-1) we first elaborate on on-device LLM, i.e., deploying LLMs on a resource-constrained device. On-device LLM deployment, including on-device inference and training, presents the following challenges.

- Excessive computing overhead: The computing overhead of on-device LLM deployment can be excessive. For instance, a LLAMA2-7B [\[80\]](#page-35-41) model with 7 billion parameters requires about 1700 giga floating point operations [\[127\]](#page-36-41) to perform a single forward pass on a sequence of 128 tokens. The Hexagon NPU, which is an advanced neural engine equipped on the Qualcomm Snapdragon X ELITE, can only provide a computing power of 45 tera operations per second. Furthermore, the backward propagation typically requires more computational resources than a forward pass [\[127\]](#page-36-41), implying that training would be even more challenging than inference.
- Huge storage/memory demands: On the one hand, caching LLMs on edge devices consumes significant storage resources. LLMs specifically designed for ondevice deployment even have billions of parameters, e.g., Google's on-device Gemini Nano-2 has 3.25 billion parameters. On the other hand, optimizing the model with the commonly used Adam optimizer during training usually requires 12 times the memory resources needed for inference [\[128\]](#page-37-0), which can be unacceptable for mobile devices with limited memory. These facts indicate that deploying LLMs on edge devices for both training and inference poses stringent requirements on the storage and memory resources of edge devices.
- High energy costs: The limited volume of battery in edge devices hinders the deployment of LLMs on edge devices.

For instance, running an LLM quantized to INT4 with 13 billion parameters using llama.cpp (one of the most lightweight on-device LLM engines) on a Xiaomi 11 smartphone results in an energy consumption of about 56 J/token [\[129\]](#page-37-1). This implies that a smartphone with a battery capacity of 3000 mAh and an output voltage of 3.7 V can generate only approximately 700 tokens if the LLM is deployed on the smartphone.

To mitigate the above challenges, in this section, we will review the enabling techniques tailored for resource-efficient LLM deployment, focusing on resource-efficient inference and fine-tuning. These techniques are summarized in Fig. [7,](#page-11-0) and the comparison of related works is shown in Table [III.](#page-12-0) It is worth noting that the methods discussed in this section can decrease the complexity of LLM deployment on edge devices, edge servers, or in device-server collaboration. Consequently, these key techniques serve as the foundation for MEI4LLM and all subsequent sections.

# <span id="page-10-2"></span>*A. Resource-efficient Inference*

On-device LLM inference eliminates privacy leakage and the need for Internet connections. However, it requires substantial computing, memory, and energy resources. In what follows, we briefly present how to mitigate these problems by introducing efficient on-device LLM inference techniques.

*1) LLM compression:* LLM compression enables the deployment of compressed LLMs on edge devices. The design objective is to compress LLMs without substantially compromising inference accuracy. Although compression has been extensively studied in the field of traditional DNNs, the unique architectures and properties of LLMs necessitate the redesign of existing compression strategies. The compression techniques tailored for LLMs are detailed below.

Quantization: Quantization converts LLM parameters from high-precision floating-point numbers (e.g., FP16) to lowprecision numbers (e.g., INT4), thereby reducing storage usage, computing latency, and energy footprint during inference. Classic model quantization can be divided into two categories: post-training quantization (PTQ) and quantizationaware training (QAT). PTQ involves directly converting the parameters of trained models into lower precision [\[131\]](#page-37-2), while QAT considers quantization errors in the training phase to improve the performance of the quantized models [\[154\]](#page-37-3). Although QAT typically yields better performance, it requires model retraining, which is extremely resource-intensive for on-device LLMs. Therefore, most quantization methods for on-device LLMs rely on PTQ [\[155\]](#page-37-4). In contrast to traditional quantization strategies that target both weights and activations, LLM quantization primarily focuses on weight-only quantization [\[130\]](#page-37-5). The reasons are as follows. First, quantizing activations leads to more significant performance degradation for LLMs [\[156\]](#page-37-6). Second, the primary sources of latency and energy consumption when generating new tokens with LLMs are often due to loading model parameters from memory [\[129\]](#page-37-1). Therefore, weight-only quantization allows for more efficient loading of quantized weights from memory, making inference more efficient without significantly degrading inference accuracy.

<span id="page-11-0"></span>![](_page_11_Figure_1.jpeg)

Fig. 7: An overview of resource-efficient LLM techniques.

In LLM quantization, uneven weight quantization can be adopted to preserve the inference accuracy of quantized LLMs since not all weights in LLMs contribute equally to the final inference results [\[131\]](#page-37-2). For example, the authors in [\[131\]](#page-37-2) propose the activation-aware weight quantization for uneven weight quantization. This method quantizes most of the weights in LLMs from FP16 to INT3/INT4 while only retaining approximately 1% of the critical weights in FP16 format.

Pruning: Unlike quantization, which reduces the bitwidth of LLM parameters, pruning aims to shrink the size of LLMs by directly removing redundant or non-significant parameters, thereby effectively reducing computing workload and storage usage. Based on the granularity of pruning, pruning methods can be classified into structured pruning [\[157\]](#page-37-7) and unstructured pruning [\[158\]](#page-37-8). 1) Structured pruning prunes structured patterns, such as sub-blocks in LLMs. For example, LLM-Pruner [\[132\]](#page-37-9) utilizes gradient information and estimated Hessian matrices to make pruning decisions for coupled structures in LLMs, such as attention heads. Then, the pruned LLMs are fine-tuned with LoRA [\[153\]](#page-37-10) to recover model performance. The compressed LLMs pruned via structural pruning can be directly deployed and executed on standard computational frameworks without additional adjustments since the structured pruning removes entire structures in LLMs. 2) Unstructured pruning aims to remove individual weights, realizing finer-grained pruning by setting unimportant weights to zero. This approach makes the weights sparse, allowing us to leverage the sparsity to accelerate the inference of LLMs. For example, SparseGPT [\[133\]](#page-37-11) transforms the pruning problem into a series of large-scale sparse regression problems and addresses them with an innovative approximate solver. The proposed SparseGPT enables up to 60% sparsity for OPT-175B [\[159\]](#page-37-12) before significant accuracy loss occurs. Although this approach can better preserve the performance of pruned LLM for on-device inference, it results in sparse models that require specialized hardware or software platforms during deployment, which is the major limitation of LLM unstructured pruning.

Knowledge distillation: Knowledge distillation (KD) [\[160\]](#page-37-13) involves transferring knowledge from a large and complex teacher model to a smaller student model. This technique enables the small-size student model to learn the behavior of the teacher model, making it suitable for deploying on resourceconstrained edge devices while ensuring a competitive performance. 1) When the teacher model is fully accessible, the KD process is known as white-box KD, which allows the student model to learn the output distributions, intermediate features, and activations of the teacher model [\[16\]](#page-34-13), [\[161\]](#page-37-14). For example, MiniLLM [\[134\]](#page-37-15) addresses the limitations of traditional KD loss functions, which do not perform well for text-generation tasks, by minimizing the reversed Kullback-Leibler divergence between the student and teacher model output distributions, thereby enhancing the performance of the student model. Additionally, the authors derive an effective optimization strategy to update the student model. 2) When the internal structure of the models is inaccessible, the form of KD is referred to as black-box KD [\[161\]](#page-37-14). This approach is particularly suitable for closed-source LLMs since most LLM access is limited to outputs via APIs. One method of black-box KD for LLMs involves generating numerous prompt-response pairs by the teacher LLM and enabling the student LLM to learn from the outputs of the teacher LLM. For instance, the authors in [\[135\]](#page-37-16) generate a set of 2.58 million instructions

TABLE III: Summary of related works on resource-efficient LLM techniques.

<span id="page-12-0"></span>

| Scenarios                       | Techniques                                | Ref.  | Objectives                                                                                                                                                                                                                                                                                                                               |  |  |
|---------------------------------|-------------------------------------------|-------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--|--|
|                                 | LLM compression                           | [130] | Focuses on weight-only quantization for LLMs with post-training quantization.                                                                                                                                                                                                                                                            |  |  |
|                                 |                                           | [131] | Adopts uneven weight quantization for LLMs to quantize the critical weights in high precision for<br>preserving inference accuracy.                                                                                                                                                                                                      |  |  |
|                                 |                                           | [132] | Proposes LLM-Pruner to make pruning decisions for coupled structures in LLMs via structured<br>pruning.                                                                                                                                                                                                                                  |  |  |
|                                 |                                           | [133] | Proposes SparseGPT for unstructured pruning and transforms the unstructured pruning problem into<br>large-scale sparse regression problems.                                                                                                                                                                                              |  |  |
|                                 |                                           | [134] | The proposed MiniLLM targets to minimize the reversed Kullback-Leibler divergence between the<br>student and teacher model output distributions to enhance the performance of the student model via<br>white box knowledge distillation.                                                                                                 |  |  |
|                                 |                                           | [135] | Uses the black box knowledge distillation to distillate knowledge from the teacher model through<br>theGPT-3.5 Turbo APIs and generated instructions.                                                                                                                                                                                    |  |  |
|                                 |                                           | [136] | Proposes speculative decoding, where a lightweight language model first generates a sequence<br>autoregressively, and a more powerful LLM then verifies and corrects the sequence.                                                                                                                                                       |  |  |
|                                 |                                           | [137] | Inserts early exit modules in Transformer blocks for obtaining final outputs at exit points to skip the<br>computation in subsequent blocks.                                                                                                                                                                                             |  |  |
|                                 |                                           | [138] | Forwards the hidden states of the current token from the early exit layer to later layers for key-value<br>cache computation.                                                                                                                                                                                                            |  |  |
| Resource-efficient<br>inference |                                           | [139] | Proposes EdgeMoE for efficient inference with mixture-of-experts-based LLMs, where the nox-expert<br>weights that occupy less storage but need more computing resources are kept in memory all the time,<br>and the required expert weights are only transferred between disks and memory as needed.                                     |  |  |
|                                 | Fast decoding                             | [140] | Proposes Deja Vu and inserts lightweight predictors after multi-layer perceptron and attention blocks<br>to predict contextual sparsity of the next block, which can enable edge devices to activate and load<br>only a small set of parameters of the next block for efficient inference.                                               |  |  |
|                                 |                                           | [141] | Proposes Medusa and introduces extra decoding heads on top of the last hidden states of an LLM<br>to predict multiple subsequent tokens in parallel. After verifying all candidates, only one reasonable<br>candidate is accepted for the next decoding phase.                                                                           |  |  |
|                                 |                                           | [142] | Introduces Lookahead Decoding and formulates autoregressive decoding as a non-linear system, which<br>is solved by fixed-point Jacobi iteration. The Lookahead Decoding generates disjoint n-grams in<br>parallel and verifies promising n-grams in parallel, ensuring continuity by matching the last token of<br>the ongoing sequence. |  |  |
|                                 |                                           | [143] | Demonstrates that only the top-k key tokens need to be attended to for each query, which can generate<br>a sparse attention mask, accelerate the inference process, and reduce computing resources without the<br>significant performance trade-off.                                                                                     |  |  |
|                                 |                                           | [144] | Develops a 2-bit quantization algorithm to quantize the key cache per channel and the value cache<br>per token, respectively.                                                                                                                                                                                                            |  |  |
|                                 |                                           | [145] | The proposed MiniCache merges the key and value caches with high similarity at the same position<br>across consecutive layers into a single cache, respectively, while those with substantial semantic<br>meanings remain unmerged.                                                                                                      |  |  |
|                                 |                                           | [146] | The proposed Scissorhands maintains KV cache memory usage within a fixed budget. Non-influential<br>tokens are dropped from the cache when the buffer is full.                                                                                                                                                                           |  |  |
|                                 |                                           | [147] | Dynamically allocate different KV cache budgets across various layers and strategically select<br>important KV vectors to cache.                                                                                                                                                                                                         |  |  |
|                                 | Parameter-efficient<br>fine-tuning (PEFT) | [148] | Inserts adapter modules into the Transformers and freezes the other parameters during fine-tuning via<br>adapter tuning.                                                                                                                                                                                                                 |  |  |
|                                 |                                           | [149] | Uses prompt tuning to add soft prompt tokens at the beginning of the input tokens for fine-tuning.                                                                                                                                                                                                                                       |  |  |
|                                 |                                           | [150] | Adds trainable prefix parameters to the keys and values of the MHA in Transformer layers for fine<br>tuning via prefix tuning.                                                                                                                                                                                                           |  |  |
| Resource-efficient              |                                           | [151] | Uses a second-order approximation method to solve the reformulated trainable parameter selection in<br>PEFT to choose the sparse trainable parameter for unstructured selective PEFT.                                                                                                                                                    |  |  |
| fine-tuning                     |                                           | [152] | Proposes Structured Diff Pruning to partition parameters into groups and remove some groups before<br>fine-tuning to realize structured selective PEFT.                                                                                                                                                                                  |  |  |
|                                 |                                           | [153] | Proposes LoRA and introduces two additional trainable matrices with ranks much smaller than that<br>of the pre-trained weight matrix. Only the newly introduced low-rank matrices are trainable during<br>fine-tuning.                                                                                                                   |  |  |
|                                 | Zeroth-order<br>optimization              | [128] | Proposes MeZO to estimate model gradients only through forward propagation, and the estimated<br>gradients are used for model parameter updating.                                                                                                                                                                                        |  |  |

and use GPT-3.5 Turbo APIs to produce responses. These instructions are subsequently used to fine-tune various student language models. The distilled language model can achieve comparable performance or even outperform the 7B LLaMA [\[80\]](#page-35-41).

*2) Fast decoding for LLMs:* Fast decoding methods can be used in on-device LLM deployment to save computing resources of edge devices in inference and facilitate ondevice inference. As explained in Section [III,](#page-6-0) LLMs typically generate tokens in an autoregressive manner, producing one token at a time based on previously generated tokens. However, as the size of LLMs grows, the volume of LLM parameters and intermediate data involved during inference also increases, leading to higher computational demands and prolonged inference latency. Additionally, as the output length increases, the LLM must repeat the decoding phase multiple times while caching all intermediate data, placing a significant burden on computing memory. To address these challenges and enable timely inference on resource-limited devices, fast decoding techniques have been developed. These techniques include leveraging lightweight LLMs for initial token generation, minimizing the volume of activated parameters/blocks, generating multiple subsequent tokens in a single forward pass, and reducing the volume of intermediate data. By applying these methods, memory footprint and inference latency can be significantly reduced. Typical fast decoding methods are elaborated below.

Speculative decoding: LLMs typically generate text outputs in an autoregressive manner, producing one token based on the previously generated tokens in each forward propagation. Consequently, the number of iterations required to generate a sequence is equal to the length of the output sequence, resulting in significant latency and heavy computing overhead. To tackle this issue, the authors in [\[136\]](#page-37-17) propose speculative decoding. In this method, a lightweight language model generates a sequence autoregressively. Then, these output tokens are processed by a more powerful LLM for verification and correction in a single inference step.

Speculative decoding can significantly reduce LLM inference latency. In the conventional LLM inference paradigm, when the model's memory requirements exceed the capacity of edge devices, each inference process necessitates dynamically releasing inferred parameters from memory and loading new parameters from disk to memory [\[162\]](#page-37-34). This process can account for up to 90% of the inference latency [\[129\]](#page-37-1). In contrast, in speculative decoding, the lightweight model can remain in memory for continuous token generation on edge devices, and the powerful LLM verifies and corrects the entire token sequence generated by the lightweight model in a single inference step. This process reduces memory loading operations, thereby decreasing inference latency. In [\[129\]](#page-37-1), the authors demonstrate that speculative decoding can halve both per-token generation latency and energy consumption while maintaining the quality of the generated content, outperforming traditional autoregressive methods in inference.

Early exit: To minimize computing latency, the early exit strategy can be employed to bypass certain subsequent layers that input tokens traverse, effectively accelerating inference time. In the case of Transformers, early exit modules may be incorporated following some early blocks [\[137\]](#page-37-18), [\[163\]](#page-37-35), where the output from intermediate layers is converted into final outputs at exit points, provided that the desired confidence value is attained. However, the conventional early exit techniques may not be effective in LLM inference since the hidden states of the token, which output the results in an early exit layer, may be missing in the later layers [\[164\]](#page-37-36). For example, upon early exit, the hidden states and the KV caches of the corresponding token in the subsequent layers will be missing, hindering the generation of future token sequences [\[165\]](#page-37-37). To address this challenge, the hidden states of the current token can be forwarded from the exiting layer to subsequent layers for KV cache computation [\[138\]](#page-37-19), [\[166\]](#page-37-38) if the token outputs the result in the exiting layer.

Mixture-of-experts: Mixture-of-experts (MoE) can effectively scale up the LLM capacity and increase performance across various downstream tasks [\[167\]](#page-37-39)–[\[169\]](#page-37-40). Specifically, the original FFN in the Transformer can be replaced with an expert network, which consists of multiple FFN experts and a router. During inference, the router directs the given input token to the best-fit FFN expert or experts. In this context, to alleviate the burden of loading substantial parameters into memory when deploying LLMs on edge devices, the MoE architecture can be leveraged so that only part of the LLM is activated and loaded during inference. For example, in [\[139\]](#page-37-20), the authors propose the EdgeMoE to enhance memory efficiency for inference with MoE-based LLMs on edge devices. The non-expert weights, which occupy less storage but require more computation, are kept in memory all the time. In contrast, the large-size expert weights, which consume less powerful computing resources, are saved on disks. Only the required expert weights for specific tasks are activated and transferred between disks and memory as needed. To further reduce memory consumption, EdgeMoE quantities the expert weights into different bitwidths according to accuracy degradation thresholds specified by users in downstream tasks. Compared with the method that loads all model weights into running memory, EdgeMoE can save between 2.6 to 3.2 times the running memory.

Contextual sparsity prediction: Contextual sparsity prediction involves predicting the small and input-dependent sets of attention heads and multi-layer perceptron (MLP) parameters needed for inference computation [\[140\]](#page-37-21), [\[170\]](#page-37-41). For example, the authors in [\[140\]](#page-37-21) propose the Deja Vu inference system, where the lightweight predictors are inserted after MLP and attention blocks. Given the input to the current block, the contextual sparsity of the next block is predicted. Using the predicted sparsity, only a small set of MLP parameters or attention heads in the next block are activated and loaded into running memory by edge devices for inference computation. This method reduces computing overhead and inference latency while maintaining approximately the same inference accuracy. For example, with the Deja Vu inference system, the average accuracy across tasks of the OPT-175B does not drop until 75% sparsity. Additionally, compared with the FasterTransformer, the Deja Vu inference system can reduce the inference latency of the OPT-175B by about half at around 75% sparsity. To further reduce inference latency, the frequently activated parameters across various tasks in LLMs can be predicted in advance using general datasets. Edge devices can preload these parameters into running memory first and keep the other parameters in the storage space, which can be loaded into memory as needed during the online inference stage.

Parallel decoding: Parallel decoding enables an LLM to generate multiple subsequent tokens in a single forward pass without relying on a lightweight language model used in speculative decoding [\[16\]](#page-34-13), [\[171\]](#page-37-42). For example, the authors in [\[141\]](#page-37-22) propose the Medusa and introduce extra decoding heads on top of the last hidden states of an LLM. During inference, each extra decoding head can predict multiple subsequent tokens in parallel for its designated position. These predictions are assembled into candidates and then processed in parallel with the tree-based attention mechanism. After verifying all candidates, a reasonable candidate is accepted for the next decoding phase. Additionally, in [\[142\]](#page-37-23), the authors propose the Lookahead Decoding, where the autoregressive decoding is formulated as a non-linear system and solved by the fixedpoint Jacobi iteration method. In each inference step, the LLM generates several disjoint n-grams in parallel and verifies the promising n-grams (subsequent tokens) in parallel from the n-gram pool, which caches the historically generated ngrams. The promising n-gram starts with a token that exactly matches the last token of the current ongoing sequence. These two methods fully exploit the computing resources that the autoregressive decoding would otherwise leave idle. Compared with the autoregressive decoding, Medusa achieves 41 times the operational intensity in attention matrix multiplication with a batch size of 16 and Llama 33B, indicating better utilization of computing resources of edge devices and reduced inference latency.

Sparse attention: Sparse attention is designed to reduce the computing overhead and memory usage associated with the attention mechanism in Transformers [\[172\]](#page-37-43). Traditionally, the attention mechanism computes the relationships between all query elements and key sequences, resulting in substantial computing delay. However, it is found that ignoring less important interactions between queries and keys can save computing resources without degrading inference performance significantly [\[173\]](#page-38-0)–[\[175\]](#page-38-1). In [\[143\]](#page-37-24), the authors propose a hierarchically pruned attention, demonstrating that appending only the top-k key tokens to each query can accelerate the inference process and reduce memory usage with minimal or no performance degradation. Furthermore, the authors in [\[172\]](#page-37-43) present a comprehensive theoretical analysis of sparsity in attention mechanisms, offering valuable insights into potential trade-offs between computational efficiency and model performance. Furthermore, sparse attention is also advantageous in LLM fine-tuning. For example, in [\[176\]](#page-38-2), Gui et al. introduce a sparse MHA module for LLM fine-tuning. By substituting the original MHA module in LLMs with the sparse MHA module, the output becomes a sparse matrix, effectively reducing peak memory consumption and accelerating fine-tuning processes.

KV cache optimization: During the autoregressive decoding phase, LLMs need to store the KV cache of previous tokens to generate future tokens. This process results in a continuously increasing size of the KV cache as more tokens are generated, leading to higher memory consumption and increased inference latency. To enable efficient on-device LLM inference on edge devices, it is crucial to reduce the KV cache size while maintaining inference performance [\[11\]](#page-34-17), [\[12\]](#page-34-11). The first approach to reducing KV cache size is through KV cache compression. For example, the authors in [\[144\]](#page-37-25) develop a 2-bit quantization algorithm to quantize the key cache per channel and the value cache per token, respectively. This algorithm can achieve nearly identical inference accuracy while reducing peak memory usage by 2.6 times. Additionally, to preserve the layer-specific information during compression, the authors in [\[145\]](#page-37-26) introduce MiniCache. It merges the key and value caches with high similarity at the same position across consecutive layers into a single cache, respectively, while those with substantial semantic meanings remain unmerged. With 4-bit quantization, this method can reduce memory usage by 41% while ensuring near-lossless model performance. Another approach involves KV cache eviction, which employs an eviction policy to dynamically select KV cache [\[177\]](#page-38-3), [\[178\]](#page-38-4). For example, in [\[146\]](#page-37-27), the authors propose Scissorhands, which maintains KV cache memory usage within a fixed budget. The proposed system first identifies pivotal tokens with higher attention scores, which significantly influence future generations. Then, non-influential tokens are dropped from the cache when the buffer is full. This method can reduce inference memory usage of the KV cache by 5 times without degrading performance. Similarly, the authors in [\[147\]](#page-37-28) dynamically allocate different KV cache budgets across various layers and strategically select important KV vectors to cache. The proposed method can maintain model performance with only 12% of the KV cache compared to a full KV cache.

Real-world implementations: The above efficient LLM inference techniques have already been deployed in many commercial products for on-device LLM applications nowadays. For example, Google DeepMind has reported that their most efficient LLM, Gemini Nano, designed for on-device deployment, is trained by distilling knowledge from larger Gemini models to enable efficient inference while preserving inference accuracy [\[79\]](#page-35-40). Besides, Gemini Nano is quantized with 4-bit for on-device deployment. Moreover, Apple's newly released Apple Intelligence, integrated into the iPhone, iPad, and Mac, brings powerful generative AI to users. To optimize model performance on edge devices, Apple Intelligence employs a range of technologies, including speculative decoding, to accelerate the inference process [\[179\]](#page-38-5).

# *B. Resource-efficient Fine-tuning*

Compared to inference, on-device LLM training demands significantly higher memory and computing resources. For example, computing the gradients of the LLM OPT-13B [\[159\]](#page-37-12) consumes 12 times the memory needed for inference [\[128\]](#page-37-0). However, since LLM fine-tuning requires much less computing resources than full-parameter training, LLM fine-tuning is widely adopted in on-device LLM deployment. In this subsection, we will explore techniques that can effectively fine-tune LLM parameters under limited resources.

*1) Parameter-efficient fine-tuning:* Parameter-efficient finetuning (PEFT) [\[18\]](#page-34-14) has emerged as a prominent solution for LLM fine-tuning by updating only a small number of parameters in the fine-tuning process. Popular PEFT techniques can be categorized into three main types, i.e., additive

<span id="page-15-0"></span>![](_page_15_Figure_1.jpeg)

Fig. 8: PEFT methods for resource-efficient fine-tuning.

PEFT, selective PEFT, and reparameterized PEFT, which are elaborated next.

Additive PEFT: To mitigate the computing burden of finetuning, the additive PEFT introduces trainable components with minimal parameters into LLMs while maintaining the pre-trained LLM parameters frozen. Additive PEFT can be further categorized into three types based on the locality of the introduced components, i.e., adapter tuning, prompt tuning, and prefix tuning, as shown in Fig. [8.](#page-15-0) 1) As first introduced by [\[148\]](#page-37-29), *adapter tuning* inserts adapter modules into the Transformer layers and freezes the other parameters in the Transformer. In this approach, only the adapters are updated during fine-tuning. 2) *Prompt tuning* adds soft prompt tokens at the beginning of the input tokens for fine-tuning [\[149\]](#page-37-30). This method leverages the characteristic of LLMs that the encoding and token generation are based on the preceding tokens. 3) *Prefix tuning* adds trainable prefix parameters to the keys and values of the MHA in each Transformer layer [\[150\]](#page-37-31). Although this approach increases the number of trainable parameters compared with prompt tuning, it allows for direct modification of representations within LLMs, enabling LLMs to respond more precisely to specific tasks.

Selective PEFT: Though additive PEFT enables fine-tuning LLMs with fewer trainable parameters, it introduces additional inference latency by adding more parameters [\[18\]](#page-34-14). To address this problem, selective PEFT preserves the model architecture by freezing most parameters and updating only a smaller subset of parameters. Selective PEFT can be broadly classified into unstructured selective PEFT and structured selective PEFT, which are shown in Fig. [8.](#page-15-0) 1) *Unstructured selective PEFT* determines the selection of trainable parameters individually, which can enhance the performance of fine-tuned models [\[18\]](#page-34-14). For example, the authors in [\[151\]](#page-37-32) reformulate the trainable parameter selection in PEFT as an optimization problem with a second-order approximation of the loss function and provide a second-order approximation method to solve this problem. By choosing the sparse trainable parameters, the sparse fine-tuned models outperform the fully fine-tuned models. However, the sparsity of the trainable parameters may not be well supported by existing deep learning frameworks and hardware, introducing additional workload for on-device LLM deployment during fine-tuning [\[180\]](#page-38-6). 2) *Structured selective PEFT* selects regular combinations of parameters, such as specific model layers, to enhance hardware computational efficiency in on-device LLM deployment [\[180\]](#page-38-6). For example, the authors in [\[152\]](#page-37-33) propose Structured Diff Pruning, which partitions parameters into groups based on matrix/bias vectors and strategically removes some groups. Then, only the parameters in the remaining groups are updated, thereby saving computing resources during LLM fine-tuning.

Reparameterized PEFT: Reparameterized PEFT techniques leverage low-rank matrices to reduce the number of trainable parameters during model fine-tuning. One of the most well-known methods in reparameterized PEFT for LLMs is LoRA [\[153\]](#page-37-10). For a pre-trained weight matrix in an LLM, LoRA introduces two additional trainable matrices with ranks much smaller than that of the pre-trained weight matrix, as illustrated in Fig. [8.](#page-15-0) In the fine-tuning process, the pretrained weight matrix is frozen, and only the newly introduced two low-rank matrices are trainable. This approach allows for efficient model fine-tuning because updating the low-rank matrices requires significantly less computational capability than updating the pre-trained weight matrix. Besides, LoRA does not add any additional inference latency since LoRA's fine-tuned weights are merged into the original weights of LLMs during inference [\[18\]](#page-34-14). Due to its salient advantages, LoRA has inspired numerous subsequent research works. For example, Quantized LoRA (QLoRA) [\[181\]](#page-38-7) aims to minimize memory consumption by combining quantization techniques with LoRA, enabling the fine-tuning of a language model with 65B parameters with a 48 GB GPU within 24 hours. The finetuned LLM achieves 99.3% of the ChatGPT's performance on the evaluated tasks, demonstrating the effectiveness of QLoRA.

*2) Zeroth-order optimization:* Zeroth-order optimization [\[182\]](#page-38-8) is a novel technique for model training, which estimates gradient updates only through the forward pass. This method considerably reduces the computational burden since the forward pass, equivalent to inference, demands far fewer computing resources than the backpropagation during training. Specifically, compared with prevalent first-order optimizers, such as Adam, zeroth-order optimizers do not need to store the intermediate results of the backpropagation during the training process, significantly reducing the memory consumption of LLM training. For example, the authors in [\[128\]](#page-37-0) propose the zeroth-order optimizer, MeZO, which adopts simultaneous perturbation stochastic approximation to estimate model gradients only through the forward propagation and uses the estimated gradients to update model parameters. Compared with the fine-tuning with Adam, the model fine-tuned with MeZO demonstrates competitive performance on 7 out of 11 tasks, using only 1/12 of the running memory while only causing less than a 1% accuracy degradation. Moreover, to further enhance the efficiency of LLM fine-tuning, the zerothorder optimization technique can be combined with PEFT methods, such as LoRA and prefix fine-tuning [\[128\]](#page-37-0).

#### *C. Lessons Learned*

Resource-efficient LLM techniques enable efficient LLM training and inference on resource-limited edge devices and edge servers. Resource-efficient inference techniques focus on two main areas: model compression reduces computational demands by shrinking model sizes, whereas fast decoding improves inference speed and efficiency by optimizing data processing, reducing parameter use, and minimizing intermediate data generation. Resource-efficient fine-tuning techniques also have two primary focuses: they either shrink the number of parameters for updating or execute the forward pass only.

The overarching goal of these resource-efficient techniques is to minimize the computing workload and memory footprint, which is essential for deploying LLMs on devices with limited resources. However, there are still limitations to these approaches, as they tend to focus on single devices and do not fully exploit networked computing resources, thus placing less emphasis on communications between devices. By connecting intelligent devices, more effective model inference and training can be achieved through collaboration among edge devices and edge servers, which is the primary focus of MEI.

# V. AN OVERVIEW OF MEI4LLM

<span id="page-16-0"></span>As alluded to in Section [IV,](#page-10-0) while there are great efforts in pushing LLMs to the network edge, the aforementioned schemes focus on implementations on a single device, limiting its applicability and efficiency. By deploying LLMs in MEI, edge devices can collaborate with edge servers for collaborative learning and inference, which largely mitigates the resource scarcity of edge devices. In line with the "NET4AI" (network for AI) vision for the 6G era [\[183\]](#page-38-9), this section presents an overview of the MEI framework that supports the deployment of LLMs, called MEI4LLM, as shown in Fig. [9.](#page-17-0) The primary objective of MEI4LLM is to enable efficient collaboration among networked edge devices/servers with enhanced communication, computing, and storage efficiency. Clearly, this framework should also seamlessly integrate with security defense mechanisms to combat various attacks, such as data poisoning and model inversion attacks during training, evasion attacks during inference [\[184\]](#page-38-10), [\[185\]](#page-38-11), and unauthorized access during model and data storage, to safeguard the security of the framework. However, since the security aspect is not our main focus, we will briefly discuss it in Section [IX](#page-32-0) for future research.

This section will begin by discussing the AI-native architecture specifically designed for MEI4LLM. Then, we will introduce the core components – covering parameter-sharing LLM caching and delivery, distributed LLM training (fine-tuning), and distributed LLM inference – that enable MEI4LLM to efficiently support the deployment and operations of LLMs within mobile edge networks. Further technical discussions of these core components will be elaborated in Sections [VI,](#page-18-0) [VII,](#page-22-0) and [VIII,](#page-28-0) respectively.

# *A. The AI-native Architecture*

6G is evolving into an AI-native architecture. To effectively support pervasive LLM applications, mobile networks are expected to have the following transformative aspects: 1) Adopting the "task-oriented" design principle. Instead of maximizing throughput or minimizing latency, the design goal can be to optimize the quality of the output, say, the cross entropy of output tokens from LLMs. 2) The mobile networks must support native model partitioning across edge servers and devices to facilitate real-time LLM inference/training. 3) information-centric networking protocols can be supported to enable fast LLM delivery over the edge networks with reduced latency.

Network virtualization is of paramount importance in achieving task-oriented design. Following the design principle of software-defined networking, the MEI4LLM needs central controllers that orchestrate network-wide computing resources and data transmissions, with the decoupled control and data plane. Moreover, since the design goal and optimization principles can vary across different LLM tasks, the protocols on the control plane must be fully programmable for specific LLM services. By collecting global network knowledge, such as the accuracy of LLMs, various quantization levels, user requirements on LLM services, channel conditions, battery status of users, and computing resource availability, the controller executes the customized algorithm to coordinate model training/inference and delivery across the distributed edge computing systems, with intermediate smashed data (i.e., intermediate activations and back-propagated gradients), model

<span id="page-17-0"></span>![](_page_17_Figure_1.jpeg)

Fig. 9: The MEI architecture for LLMs in 6G with four basic components, i.e., AI-native architecture, parameter-sharing LLM caching and delivery, distributed LLM training (fine-tuning), and distributed LLM inference.

parameters, or user data exchanged across edge routers and servers.

Further edge networks will evolve into the "neural edge" [\[183\]](#page-38-9), where neural network layers are distributed across edge nodes for collaborative computing. Analogous to cloud data centers with many GPUs to support large-scale LLMs, MET4LLM must feature flexible and model splitting for training and inference across distributed edge devices and servers. The optimal model splitting, placement, and data routing for large-scale models should be concertedly supported over edge networks. Moreover, both air interfaces and network designs should have native support for federated learning, split learning, and split inference for AI models, including LLMs. Since model training and inference are robust to packet errors, task-oriented wireless transmissions, say, for smashed data at the cut layer, can be carried out with appropriate error control to achieve the best efficiency-reliability trade-off.

Lastly, since LLMs can be communication-intensive, information-centric networking can be implemented to ensure seamless model, feature, and data transmission across edge networks for efficient delivery of LLMs. In this respect, MEI4LLM should support LLM parameter block naming and name-based transmission protocol. By assigning names for popular LLM parameter blocks, the central controller in the MEI4LLM architecture can forward the parameter request to edge caches located nearby and support parameter block multicasting for users, thereby reducing latency and bandwidth consumption for delivering large-scale models across networks and to end users.

#### *B. Parameter-sharing LLM Caching and Delivery*

Given the limited storage capacities of edge devices and real-time model fine-tuning in changing environments, LLMs must be delivered over mobile networks frequently to support subsequent usage. Since parameter blocks can be shared among various downstream LLMs [\[153\]](#page-37-10), [\[186\]](#page-38-12) and even reused within the same LLM [\[70\]](#page-35-31), LLM caching and delivery schemes must take advantage of this fact to reduce caching and delivery costs of shared LLM parameter blocks. Besides, to enable real-time model delivery, MEI4LLM can construct a lookup table, assigning names for LLM parameter blocks to facilitate content search and management, following the principle of information-centric networking [\[187\]](#page-38-13), [\[188\]](#page-38-14). By doing so, the MEI4LLM paradigm places LLMs at appropriate sites, retrieves LLMs of interest from nearby edge servers, and enables routing/multicasting of LLM parameter blocks to mobile users. Details will be further discussed in Section [VI.](#page-18-0)

## *C. Distributed LLM Training (Fine-tuning)*

It is foreseen that 6G MEI systems can efficiently fine-tune LLMs to adapt to local environments. Edge LLM fine-tuning can be triggered whenever the inference accuracy decreases or after a certain period when local environments have changed. For example, LLM-empowered virtual assistants should be fine-tuned regularly to better adapt to new trends in news media, top local restaurants, and popular attractions, resulting in improved decision-making and interactions with users [\[189\]](#page-38-15), [\[190\]](#page-38-16). LLM-empowered mobile health applications should be personalized to provide better predictions and health or fitness suggestions [\[191\]](#page-38-17).

In next-generation mobile networks, edge training for LLMs must answer two questions: 1) how to preserve user privacy and data ownership, and 2) how to support large-scale model training through the collaboration of edge nodes. To enhance the data privacy of users, federated learning (FL) and split learning (SL) serve as two promising distributed learning frameworks to implement at the network edge [\[192\]](#page-38-18). Specifically, FL allows edge devices to train models locally while only sharing model parameters with an edge server for aggregation, thus harnessing collective intelligence without sharing personal data [\[193\]](#page-38-19). Alternatively, SL and its variant split federated learning (SFL), can be implemented to enable device-server co-training without sharing local raw data, which is particularly suitable for large-scale LLM fine-tuning for edge devices [\[194\]](#page-38-20), as model splitting allows for workload balancing across diverse edge nodes. To effectively support the intensive training, various resource-efficient training techniques, as detailed in Section [IV,](#page-10-0) can be combined with FL or SL. These discussions will be provided in Section [VII.](#page-22-0)

In addition, considering RAG [\[195\]](#page-38-21), [\[196\]](#page-38-22), external knowledge sources should also be cached at the network edge, ensuring timely acquisition of the data/knowledge for an LLM. However, RAG involves extra storage costs and delivery latency for retrieving external knowledge. In comparison, fine-tuning an LLM consumes extra computing power while eliminating the need for external databases. In this regard, an important research problem is determining data selection for either LLM training (fine-tuning) or RAG in MEI systems.

#### *D. Distributed LLM Inference*

To accommodate for resource-intensive LLMs, edge servers and edge devices must perform distributed inference for LLMs in a concerted manner, depending on the communicationcomputing workload and privacy requirements. There are different ways for edge inference. On-server inference requires users to upload raw data to the server. This approach eliminates the computing burden on edge devices while potentially violating users' privacy requirements. For instance, multimodal LLMs may collect sensitive audio and video data in home environments, where users are often reluctant to share. Conversely, on-device inference preserves privacy and eliminates communication costs while imposing an intensive computing workload on edge devices. Split inference sits in between, with edge devices and servers holding parts of the AI models, which has been mentioned in 3GPP 5G technical specifications. Split inference involves uploading features from edge devices to edge servers for co-inference. To facilitate LLM inference, MEI4LLM can tailor consumer services by appropriately selecting from these schemes based on communication-computing resource status and privacy requirements, as elaborated in Section [VIII.](#page-28-0)

#### *E. Lessons Learned for MEI4LLM*

Clearly, MEI4LLM is nothing but a special case of MEI. However, the need to train and deploy LLMs at the edge serves as a key stimulus for the evolution of MEI towards its next stage. Specifically, the extreme resource demands of LLMs necessitate a fundamental rethinking of traditional MEI. For this reason, MEI4LLM must feature the following characteristics: *1) the native support of model splitting and parallel training/inference across interconnected edge nodes to facilitate the deployment of large-scale models 2) the integrated design of wireless communications and resourceefficient LLM training/inference techniques such as parameterefficient fine-tuning and token (representation) reduction to make LLM deployment cost-effective.* The remainder of this survey paper will discuss these two critical aspects in detail.

#### VI. EDGE CACHING AND DELIVERY FOR LLMS

<span id="page-18-0"></span>Edge LLM caching and delivery play indispensable roles in both the training and inference of LLMs, serving as cornerstones of edge LLM deployment. For this reason, we begin with discussions on edge caching and delivery and then present edge training and inference in the subsequent sections. Compared with conventional edge service/content caching and delivery, *the main distinction of edge LLM caching and delivery is the exploitation of parameter shareability*, which is commonly observed in LLMs, aiming to increase storage and communication efficiency over edge networks. While parameter shareability can exist in traditional DNNs, it is much more prevalent and significant in LLMs due to the widespread adoption of PEFT techniques, requiring our special design attention. In what follows, this section presents the techniques that leverage this characteristic of LLMs for optimizing edge caching and delivery. The related works for efficient edge LLM caching and delivery are outlined in Table [IV](#page-19-0) for readers' convenience.

# *A. Edge LLM Caching*

Edge model caching can realize low model downloading latency by distributing AI models to wireless edge servers in advance. Distinct from service placement for computation offloading, edge model caching focuses on caching AI models for downloading from edge servers to end users [\[186\]](#page-38-12), [\[200\]](#page-38-23). This paradigm enables users to fetch AI models from edge servers directly instead of accessing remote cloud data centers, which incurs excessive downloading latency [\[197\]](#page-38-24), [\[199\]](#page-38-25). However, implementing edge LLM caching presents several challenges: 1) *Limited storage capacity for LLM caching*: Service providers aim to place as many popular LLMs as possible on edge servers to enhance the cache hit ratio and decrease model downloading latency for users. Nevertheless, the immense size of LLMs presents a significant challenge for their storage on edge servers; 2) *High LLM edge cache (re)placement costs*: Over time, previously cached LLMs may no longer align with the changing user requests. To address this, service providers may replace LLMs on edge servers to better accommodate up-to-date requests, imposing a substantial burden on mobile backhaul networks. In what follows, parameter-sharing model caching is presented to address the above challenges.

*1) Parameter-sharing LLM caching:* Parameter-sharing model caching can be adopted to improve storage and transmission efficiency at the network edge. As discussed in Section

<span id="page-19-0"></span>

| Scenarios            | Techniques                       | Ref.  | Objectives                                                                                                                                                                                                        |
|----------------------|----------------------------------|-------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Edge LLM caching     | Parameter-sharing<br>LLM caching | [197] | Proposes TrimCaching framework for LLMs, where shared parameter blocks of LLMs only need to cached once in an edge server for storage efficiency.                                                                 |
| Edge LLM<br>delivery | Parameter-sharing wireless model | [198] | Proposes a model multicasting and assembling framework, where shared parameter blocks of models requested by users are multicast to users, and the specific parameter blocks are unicast to each user separately. |
|                      | downloading                      | [199] | Compresses model weights in low bitwidth for fast model downloading                                                                                                                                               |

TABLE IV: Summary of related works for edge LLM caching and delivery.

IV, PEFT, such as LoRA, is widely adopted for adapting LLMs to downstream tasks. In LoRA, the pre-trained LLM parameters are frozen, and only the newly introduced parameters are trainable, typically accounting for less than 1% of the original LLM parameters. Therefore, most parameters of various LLMs fine-tuned with LoRA for downstream tasks are shared from pre-trained LLMs, which should be leveraged to enhance caching efficiency significantly. Take LoRA and GPT-2 as an example. Fig. 10 demonstrates that the inference performance almost remains unchanged even when 99.97% of parameters in the GPT-2 large model are frozen parameters from the pre-trained GPT-2 large. Based on this observation, in [197], we propose an AI model placement strategy, called TrimCaching, to maximize the cache hit ratio under server storage capacity and service latency constraints by exploiting the parameter-sharing properties of AI models, particularly LLMs. In the TrimCaching framework, only one copy of the shared parameter blocks across LLMs is cached on one edge server, thereby improving storage efficiency, as illustrated in Fig. 11. Compared with the Independent Caching strategies for edge LLM caching [201], [202], which do not consider parameter sharing across LLMs, the TrimCaching strategy can significantly improve the cache hit ratio as shown in Fig. 12. In other words, even with storage-limited edge servers, a large number of popular LLMs can still be cached on network edge servers under the TrimCaching framework, thereby considerably reducing LLM downloading latency compared with fetching them from the cloud.

While the parameter-sharing model caching under multi-cell scenarios has been investigated in [197], this paradigm can be extended to consider many different scenarios in cellular networks, such as centralized-RAN (C-RAN) and heterogeneous networks (HetNets). Moreover, mobility-aware edge caching can also be developed by exploiting the knowledge of user mobility patterns. For instance, in [203], a distributed approximation algorithm based on large deviation inequalities is developed for content placement based on the assumption that users move randomly based on a discrete-time Markov chain model. Similar algorithms can be developed to address the parameter-sharing model caching problem with high user mobility.

2) Edge LLM cache replacement: Since the popularity of models can evolve over time, another fundamental research problem in edge caching is LLM replacement. By replacing outdated content with new data, edge servers can continuously refresh their caches with new content to satisfy ever-changing user requests [204]. The two most classic replacement strategies are the recency-based and the frequency-based strategies,

<span id="page-19-1"></span>![](_page_19_Figure_6.jpeg)

Fig. 10: BLEU v.s the frozen parameter ratio of the fine-tuned GPT-2 large. In this example, LoRA enables the integration of fine-tuned parameters ranging from hundreds of kilobytes to tens of megabytes into a pre-trained GPT-2 large model, which has a size of around 3.02 GB. This process creates multiple GPT-2 large models tailored for specific downstream tasks. The frozen parameter ratio refers to the ratio of the pre-trained GPT-2 large model size to the fine-tuned GPT-2 large model size. BLEU is used to evaluate machine-translated texts against high-quality reference translations. "Full FT" in the figure indicates the full parameter fine-tuning.

<span id="page-19-2"></span>![](_page_19_Figure_8.jpeg)

Fig. 11: The TrimCaching mechanism for caching LLMs in wireless edge networks. Popular LLMs are placed on edge servers, where users can download the requested LLMs from the edge network. To enhance storage efficiency, shared parameters across LLMs are cached only once on an edge server.

which remove the least recently used (LRU) objects and least frequently used (LFU) objects and then replace them with

<span id="page-20-0"></span>![](_page_20_Figure_1.jpeg)

Fig. 12: Cache hit ratio v.s edge server capacity. A cache hit occurs if the requested GPT-2 model can be served by any edge server in the edge networks within the E2E latency requirements of end users. The GPT-2 models, including GPT-2 small, medium, and large, are fine-tuned with LoRA for parameter sharing in 100 downstream tasks. 40 users and 12 edge servers are located in a square area of 1 km<sup>2</sup>. The coverage radius of edge servers is 275 m. The other parameter settings can be referred to [197].

updated content. However, such naive strategies fail to exploit the cooperative caching among edge nodes and parameter shareability among LLMs. Two directions can be further explored to improve the performance. First, shared parameter blocks among LLMs and cooperation among neighboring edge servers can be considered to achieve parameter replacement with minimal backhaul transmission costs. Second, model replacement should be optimized with inference accuracy in mind, as LLMs are often more effective in general tasks than specific ones. This suggests that users may request an LLM that closely matches their needs, i.e., achieving required prediction accuracy rather than a specific LLM, thereby leading to less frequent replacement.

To implement the replacement policies, one direction is centralized proactive caching, say the scheme in [186], after a certain period. However, under high user mobility, such methods may involve high system complexity and communication costs for transferring LLMs. The other direction is to develop distributed algorithms, often based on the Markov Decision Process or reinforcement learning, to make replacement decisions without knowing the full information of other edge nodes [205]. Both of the directions are worth exploring further.

3) Edge caching for RAG: As mentioned in Section III-A4, integrating LLMs and RAG, which enables LLMs to retrieve relevant data from external knowledge sources, is essential for generating reliable and up-to-date responses without retraining/fine-tuning. However, since retrieving the information from the remote cloud can be time-consuming, the most popular external knowledge should be cached at the network edge to enable LLMs to fetch the most factual, accurate, and up-to-date content. Intriguingly, this edge caching problem naturally differs from traditional ones as the caching should be optimized by considering the training status or internal knowl-

edge of LLMs. Specifically, if LLMs can already memorize or easily infer certain content, such knowledge can be removed from the external knowledge sources to save storage space at the network edge, thereby enhancing storage efficiency for caching external knowledge sources. Besides, edge servers can cache specific external knowledge sources frequently requested by their associated users, which can significantly increase the OoS for responses to users in different regions. On the other hand, edge caching for RAG is tightly coupled with on-the-fly LLM fine-tuning, i.e., we need to choose which data to fine-tune outdated LLMs and which data to cache for RAG on edge servers. With this in mind, it is worth investigating a joint LLM fine-tuning and knowledge source caching problem under the context of RAG to enhance the reliability of LLMs under latency constraints, taking into account the data retrieval latency from external knowledge sources at the edge/cloud servers and the fine-tuning costs by feeding new data to LLMs.

#### B. Edge LLM Delivery

An essential step in fetching the models from where they are cached to end users is delay-efficient model delivery. This process encompasses both model routing within backhaul/backbone networks and model downloading via wireless access links, facing the following challenges: 1) Excessive backhaul/backbone delivery latency: When the requesting LLMs are not cached on the associated edge server, the LLMs need to be routed within the edge network. However, compared with traditional AI models, LLMs have significantly larger model sizes. Therefore, LLM routing needs to be carried out among edge servers with moderate backhaul traffic; 2) Significant wireless downloading latency: AI model downloading needs to be finished with low latency to fulfill the OoS requirements of end users. As envisioned by 3GPP, autonomous driving applications require AI model downloading within 1 second [30]. However, the large model size of LLMs hinders fast model downloading, making it extremely hard to meet the stringent service latency requirements. These call for the following solution approaches.

1) Parameter-sharing backhaul/backbone model delivery: To reduce the model delivery costs within the backhaul/backbone networks, parameter-efficient model delivery can be developed by exploiting the parameter shareability across LLMs, as illustrated in Fig. 13. When an edge server does not cache an LLM but caches other LLMs with the shared parameter blocks, only the missing parameter blocks of the LLM need to be delivered, thereby reducing data delivery costs. For example, for LLMs fine-tuned with LoRA, only the specific LoRA parameters must be transmitted if the shared backbone has been cached on the edge server. Moreover, upon delivery of parameter blocks, the parameter blocks can be fetched from different serving edge servers. The entire LLM can be assembled as long as all the needed parameter blocks reach the destination (e.g., requesting server or user). For this reason, considering multi-hop backhaul/backbone communication networks, caching-aware data routing can be developed to exploit multicasting of overlapped parameter blocks to improve network throughput.

<span id="page-21-0"></span>![](_page_21_Figure_1.jpeg)

Fig. 13: The parameter-sharing backhaul/backbone LLM delivery framework. In this example, the AI assistant and autonomous driving applications request LLM 1 and LLM 2, respectively, which are not cached on the associated edge server. Therefore, LLM 1 and LLM 2 need to be delivered from another edge server. In this framework, to reduce communication overhead and latency, only the specific LLM parameter blocks need to be transmitted since the shared backbone/parameter blocks are already cached on the associated edge server. The entire LLM can be assembled before downloading, or on edge devices once all the needed parameter blocks have been received. To further reduce delivery latency, parameter-sharing LLM delivery can be combined with various compression techniques, where the compression ratio can be determined by jointly considering model performance and channel/backhaul conditions.

<span id="page-21-1"></span>![](_page_21_Figure_3.jpeg)

Fig. 14: Parameter-sharing wireless LLM downloading.

2) Parameter-sharing wireless model downloading: To reduce the costs of wireless model downloading from base stations (edge servers) to users, it is essential to consider parameter-sharing wireless model downloading. As illustrated in Fig. 14, to decrease downloading latency, the key idea is to multicast the reusable parameter blocks, thereby achieving timely downloading. In [198], the authors propose a model multicasting and assembling framework by exploiting the shareable parameters across AI models, i.e., Transformers as considered in the paper. This framework multicasts shared parameter blocks to multiple requesting users while unicasting specific parameter blocks to each user. A user then assembles the downloaded parameter blocks to obtain the desired

LLM. There is a trade-off between parameter sharing and downloading latency. Although a model with more shared parameter blocks can be downloaded with lower latency, this will also degrade model performance in downstream tasks. To reduce the total model downloading latency and ensure the QoS requirements, the proposed framework aims to maximize the occurrence of the same parameter block across different models within the accuracy QoS requirement of downstream inference tasks.

Parameter-sharing LLM delivery/downloading can also be integrated with various LLM compression techniques. Take quantization as an example; by compressing LLM weights in low bitwidth, LLM downloading can be accomplished with low latency [199], [206]. However, as illustrated in Section IV-A, setting a uniform quantization bitwidth in LLMs degrades the inference performance significantly since not all weights contribute equally to the final outputs [131]. Therefore, weights in LLMs can be assigned with nonuniform bitwidths according to weight importance [207]. For example, weights corresponding to larger activations or higher quantization errors can be quantized in high bitwidth before downloading, which can preserve the performance of the quantized LLMs [131], [207], [208]. Moreover, quantization should also be optimized by considering the popularity/shareability of parameter blocks across various downstream LLMs. By integrating LLM compression methods, the design for LLM delivery/downloading by jointly considering wireless channel conditions, weight importance, parameter block shareability, and model performance is worth further investigation.

3) Joint parameter-sharing model caching and delivery: Parameter-sharing model caching and delivery are tightly coupled over wired networks, wireless networks, or the hybrid of both. On the one hand, model placement considerably affects backhaul and wireless link traffic across edge networks. On the other hand, radio resource allocation and data routing influence the optimal decisions of model placement. In view of the shared parameter blocks across LLMs, this joint problem significantly differs from existing caching and delivery schemes because of the shared parameters of various contents and the fact that a model can be recovered by fetching various blocks from different source nodes. In multihop networks, the joint caching and data routing problem can be studied by considering the parameter shareability and multicasting of the shared model parts. In cache-aided cellular networks [\[209\]](#page-38-35), one can jointly optimize the model placement, fronthaul/backhaul cost, and radio resource allocation to facilitate fast AI model downloading to edge devices.

# *C. Lessons Learned*

Edge caching and delivery are highly challenging in the context of LLMs because of their enormous size. To mitigate these issues, the aforementioned techniques aim to 1) only transmit a small proportion of the model (i.e., task-specific parameter block), 2) store or multicast the shared parameter blocks across communication networks, and 3) jointly optimize LLM capabilities and external knowledge sources. *All these features arise from exploiting "reusable knowledge" in LLMs to save bandwidth and storage resources as much as possible*. Clearly, these new features will generate a rich set of research problems, particularly considering the various scenarios in edge networks and multi-dimensional resource management therein. For instance, diverse network architectures such as multi-cell wireless networks, heterogeneous wireless networks, and a combination of wireless and wired networks can be explored, along with optimizing various sets of radio resources, including spectrum resources and transmit power. The guiding design principle is to optimize the usability of LLMs after caching and delivery while minimizing resource utilization based on the aforementioned principles regarding reusable knowledge.

## VII. EDGE TRAINING FOR LLMS

<span id="page-22-0"></span>Edge training performs model training at the network edge to extract intelligence from data sources. The main distinction of edge LLM training from traditional edge training lies in *the large scale of AI models, which can be too large to fit into an edge server, as well as the optimization of PEFT over wireless networks.* As shown in Fig. [15,](#page-23-0) this section elaborates on edge LLM training through four categories: centralized edge learning, federated edge learning, split learning, and hierarchical collaborative learning. For readers' convenience, the related works on edge LLM training are summarized in Table [V.](#page-24-0)

#### <span id="page-22-1"></span>*A. Centralized Edge Learning*

The most straightforward model training approach in MEI systems is gathering data from edge devices and conducting model training on edge servers. While edge servers are generally more powerful than edge devices, centralized edge learning presents several key challenges: 1) *Significant computing burden on edge servers*: LLM training or fine-tuning demands significant computing resources and storage/memory capacity. For instance, training LLama-2 7B in FP32 [\[80\]](#page-35-41) empirically demands 112 GB of GPU memory [\[230\]](#page-39-0), which can be challenging for an edge server provided that a powerful H100 GPU only possesses 80 GB memory. 2) *Excessive communication overhead for raw data uploading*: Considering multimodal raw sensing data, a tremendous amount of data should be uploaded to a centralized data center. These two challenges can be overcome by the following solution approaches.

*1) Scaling-up LLM training:* Scaling-up training is essential for LLM training in centralized edge training. Due to memory and computing constraints, training large-scale LLMs on a single GPU is highly challenging. Therefore, leveraging distributed computing and memory resources, such as multiple GPUs on edge servers, come to the rescue. Scaling-up LLM training can be categorized into two main approaches: parallel training and GPU memory optimization, which are elaborated below.

Parallel training for LLMs: Considering the extreme training workload of LLMs, parallel computing must be employed to leverage resources across edge nodes to split LLM training to reduce training latency and share required memory space. The three most prominent parallel computing strategies are data parallelism, pipeline parallelism, and tensor parallelism [\[231\]](#page-39-1), [\[232\]](#page-39-2): 1) Data parallelism enables replicating models across different processors (processors can refer to multiple GPUs in one server or different edge servers), with the uploaded dataset being shuffled and distributed among the processors. After each processor finishes the forward and backward propagation with the corresponding model replica in parallel, all the gradients are aggregated for synchronization to update the model parameters [\[210\]](#page-38-36). 2) In pipeline parallelism, models are divided into different sub-models; each processor owns one sub-model. A mini-batch of training samples is further divided into micro-batches in each training iteration. The pipeline training process can be executed by feeding micro-batches into the sub-model, and different processors can update the sub-models with different micro-batches at the same time [\[211\]](#page-38-37). 3) In tensor parallelism, the parameters and computations of each layer are divided across multiple processors for separate processing by multiple processors in parallel. For example, considering Transformers, the authors in [\[212\]](#page-38-38) split per attention head parameters in self-attention modules across multiple processors such that the matrix multiplication corresponding to each attention head is stored locally on one processor. To leverage the advantages of these three parallel computing paradigms, in [\[213\]](#page-38-39), Narayanan et al. combine tensor, pipeline, and data parallelism to efficiently train LLMs by harnessing a massive number of distributed GPUs.

GPU memory optimization: Effective GPU memory optimization, such as Zero Redundancy Optimization (ZeRO), can be adopted to reduce memory usage in edge LLM training with distributed GPUs [\[214\]](#page-38-40), [\[215\]](#page-38-41), [\[234\]](#page-39-3). In [\[214\]](#page-38-40), Rajbhandari et al. develop ZeRO, where each processor only holds a portion

<span id="page-23-3"></span><span id="page-23-2"></span><span id="page-23-1"></span><span id="page-23-0"></span>![](_page_23_Figure_1.jpeg)

Fig. 15: Edge training frameworks for LLMs. This figure presents the associated challenges and corresponding techniques to address them in each sub-figure. In Fig. 15(a), edge devices upload raw data to the edge server for global LLM training. In Fig. 15(b), FL is adopted, where the edge server aggregates the locally trained LLMs from edge devices. In Fig. 15(c), an LLM is partitioned into two sub-models, with the split learning process executed by exchanging intermediate data between the edge server and the edge device. At last, hierarchical collaborative learning can be categorized into cloud-edge, and cloud-edge-end collaborative learning. We take cloud-edge collaborative learning as an example in Fig. 15(d), where edge devices upload the data to edge servers for training, and the cloud aggregates different edge-trained LLMs.

<span id="page-23-5"></span>![](_page_23_Figure_3.jpeg)

Fig. 16: Per-round latency for split edge learning based on the number of edge servers participating in multi-hop split edge learning. We train a LLaMA-2 model on the E2E dataset [233] to evaluate performance, assuming a wireless communication data rate of 200 Mbps between two edge servers. Per-round latency refers to the time taken to train one mini-batch of 128 samples.

of optimizer states, gradients, and parameters during training, and the rest can be obtained from other data processes as needed, thereby reducing the required GPU memory space for each processor. To further alleviate the pressure on GPU memory, the authors in [215] propose ZeRO-Offload, which effectively leverages the CPU memory. ZeRO-Offload partitions gradients and optimizer states across GPUs, offloading

<span id="page-23-4"></span>them to CPU memory for the entire training. During backward propagation, gradients are computed and averaged on the GPU, and each GPU offloads the averaged gradients belonging to its partition to the CPU memory. Once the gradients are available on the CPU, the optimizer states are updated directly on the CPU before being gathered back to the GPU.

2) Parallel LLM training at the network edge: Parallel computing for LLM training can be extended to wireless edge networks. As illustrated by our simulations in Fig. 16, pipeline parallelism significantly reduces end-to-end training latency by splitting the model into multiple sub-models and placing them on edge servers appropriately. Different from cloud-based approaches, parallel computing of LLMs encounters a more significant communication bottleneck at the network edge. Specifically, there exist heterogeneous communication and computing capabilities of edge servers. This situation, however, is normally oversimplified in large-scale GPU clusters. Since high-dimensional smashed data should be exchanged among edge servers via wired/wireless links, model splitting and placement should be optimized judiciously under computing-communication resource constraints.

Similar problems have been considered for multi-hop split inference [235], [236], where the model splitting and/or placement is often mapped to a graph, which can be further

TABLE V: Summary of related works on edge LLM training.

<span id="page-24-0"></span>

| Scenarios                                 | Techniques                                          | Ref.  | Objectives                                                                                                                                                                                                                                                                                          |
|-------------------------------------------|-----------------------------------------------------|-------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|                                           | Scaling-up<br>training                              | [210] | Adopts data parallelism to replicate models and distribute datasets across different processors. The<br>gradients from all processors are aggregated for model updating.                                                                                                                            |
|                                           |                                                     | [211] | Leverages pipeline parallelism to divide models into sub-models and divide a mini-batch of training<br>samples into micro-batches in each training iteration. Different processors can update the sub-models<br>with different micro-batches at the same time.                                      |
|                                           |                                                     | [212] | Splits per attention head parameters in self-attention modules across multiple processors to realize<br>the parallel processing for the matrix multiplication via tensor parallelism.                                                                                                               |
| Centralized<br>edge learning              |                                                     | [213] | Combines tensor, pipeline, and data parallelism to efficiently train LLMs.                                                                                                                                                                                                                          |
|                                           |                                                     | [214] | Proposes ZeRO where each processor only holds a portion of optimizer states, gradients, and<br>parameters during training, and the rest can be obtained from other data processes as needed.                                                                                                        |
|                                           |                                                     | [215] | Introduces ZeRO-Offload, which efficiently optimizes GPU memory by utilizing CPU memory. It<br>partitions gradients and optimizer states across GPUs and offloads them to the CPU during training.                                                                                                  |
|                                           | Importance-aware                                    | [216] | Demonstrates that the convergence rate and model accuracy during training can be improved by<br>allocating more radio resources to training data samples with higher importance.                                                                                                                    |
|                                           | token uploading                                     | [217] | Illustrates that models can converge faster via appropriate user scheduling by considering the signal<br>to-noise ratio and data uncertainty metric.                                                                                                                                                |
|                                           | Parameter-efficient<br>federated LLM<br>fine-tuning | [218] | Combines LoRA and FL, enabling edge devices to update and upload only the LoRA parameters to<br>the edge server.                                                                                                                                                                                    |
|                                           |                                                     | [219] | Introduces FederatedScope-LLM, which provides PEFT techniques and versatile programming inter<br>faces to facilitate FL for LLMs.                                                                                                                                                                   |
| Federated edge<br>learning                |                                                     | [220] | Integrates prompt tuning in FL for real-world meteorological forecasting tasks with distributed sensors.                                                                                                                                                                                            |
|                                           |                                                     | [221] | Proposes FedPepTAO, which focuses on updating parameters in proper prompt layers of LLMs.                                                                                                                                                                                                           |
|                                           |                                                     | [222] | Proposes pFedPG, where clients update the client-specific prompts during FL for personalization.                                                                                                                                                                                                    |
|                                           | Token representation<br>reduction in SL             | [223] | Edge devices can send the most informative smashed data to the server in SL for reducing<br>communication overhead.                                                                                                                                                                                 |
| Split                                     |                                                     | [224] | Quantizes smashed data in SL/SFL/PSL with low precision for communication-efficient SL.                                                                                                                                                                                                             |
| learning                                  | U-shaped SL                                         | [225] | Adopts U-shaped SL for LLMs. The framework places the computation-sensitive decoders on edge<br>servers and places the bottom layers with the text input module and the top layers with the text output<br>module on edge devices, which can protect the privacy of the ground-truth target tokens. |
|                                           | Cloud-end<br>collaboration                          | [223] | Proposes DLoRA that dynamically identifies and fine-tunes the most relevant personal PEFT modules<br>on edge devices while keeping the frozen parameters of LLMs in the cloud for cloud-end collaborative<br>training.                                                                              |
|                                           |                                                     | [226] | Places a larger multimodal LLM and a smaller multimodal LLM in the cloud and the edge device,<br>respectively. Edge devices select and upload informative multimodal data to the cloud for training<br>adapters, which are then downloaded to the edge devices.                                     |
| Hierarchical<br>collaborative<br>learning | Cloud-edge<br>collaboration                         | [13]  | Generative AI models are first pre-trained in the cloud and then fine-tuned on edge servers for<br>customization.                                                                                                                                                                                   |
|                                           |                                                     | [227] | The cloud trains a large-scale model, which is combined by models of multiple edge servers for<br>multi-task and multi-modal learning and then distributes task-specific lightweight models back to the<br>edge servers for personalized fine-tuning.                                               |
|                                           | Cloud-edge-end<br>collaboration                     | [228] | Proposes a hierarchical FL framework, which enables edge servers to aggregate the locally trained<br>models from end users and the cloud to aggregate models from different edge servers.                                                                                                           |
|                                           |                                                     | [229] | Adopts model quantization in the hierarchical FL framework for efficient model aggregation.                                                                                                                                                                                                         |

mapped to shortest path problem [\[236\]](#page-39-6) or min-cut problem [\[235\]](#page-39-5). Unfortunately, these problems are not directly applicable to parallel training of large-scale models, such as pipeline parallelism and tensor parallelism under edge networks. For instance, unlike multi-hop inference problems that often handle one input data at a time, pipeline parallelism for model training considers processing a batch of training data samples, which can be divided into multiple micro-batches as inputs to form the pipeline process. These characteristics set apart parallel training for large-scale models from inference problems in edge computing networks. Besides, by exploiting the special structure of LLMs, mostly relying on Transformers, the workload splitting problem might be simplified because it consists of repeated Transformer blocks with the same size of intermediate data output. Furthermore, model splitting, tensor splitting, and sub-model/layers placement can be jointly optimized with radio resource management problems, such as spectrum allocation and data routing [\[237\]](#page-39-21), to mitigate network congestion with reduced transmission delay.

*3) Importance-aware token uploading:* Another challenge in centralized edge training for LLMs lies in the communication bottleneck in input token uploading. Particularly, multimodal LLMs involve massive multimodal data uploading from edge devices, such as text, audio, high-dimensional images/videos, and LiDAR, which can cause network congestion and intolerable delay for delay-sensitive applications. To address this concern, existing works on importance-aware training data transmission schemes in edge learning can be extended to the context of LLMs. In [\[216\]](#page-39-7), the authors demonstrate that the convergence rate and model accuracy during training can be improved by allocating more radio resources to training data samples with higher importance. Besides, it is shown in [\[217\]](#page-39-8) that a faster model convergence can be achieved via appropriate user scheduling by taking into account the signal-to-noise ratio and data uncertainty metric.

In the context of LLMs, there are two additional characteristics that can be further exploited for efficient token uploading. First, compared with other types of training samples (i.e., images only), the input and target tokens in LLMs can vary in size significantly, implying that batching input/target tokens for training should consider their lengths. For instance, batching a long sequence with a short sequence for training may not be a wise choice because it can result in the idleness of GPU resources, considering that the short sequence can be finished shortly. It is noted that batching on the server side is tightly coupled with the scheduling of token uploading from edge devices. Second, the importance level of tokens can be obtained by exploiting the correlation across different modalities; for example, text data may reveal the important part of the image for training. As such, it is promising to explore the selection of important data across multiple modalities, such as text and images, from edge devices to improve the accuracy of LLMs while minimizing the overall data transmission overhead.

#### <span id="page-25-0"></span>*B. Federated Edge Learning*

Centralized edge learning allows edge servers to directly access the personal data of edge devices, which raises significant privacy concerns and potentially breaches data collection regulations. As such, there is a pressing need to employ FL for LLM training at the network edge [\[238\]](#page-39-22). In FL, clients cooperatively train a global model by sending their local model updates to an edge server for aggregation [\[239\]](#page-39-23), [\[240\]](#page-39-24). When FL meets LLMs, there are some crucial challenges to address: 1) *Excessive on-device training workload:* Compared with the memory bandwidth of deep learning accelerators (up to 7.8 TB/s), the memory bandwidth of embedded edge devices remains much lower (up to 0.2 TB/s) [\[241\]](#page-39-25), leading to severe training time penalties. 2) *Massive model transfer traffic:* Ondevice LLMs contain billions of parameters [\[7\]](#page-34-5). Uploading LLMs from massive edge devices to edge servers for model aggregation results in a heavy communication burden on telecommunication infrastructure, which can be extremely expensive for mobile subscribers. 3) *Resource heterogeneity:* The straggler issue is commonly observed in FL, where the training time is determined by the slowest client with the most scarce communication-computing capabilities. To mitigate this issue, integrated communication-computing resource allocation should be considered to fulfill the requirements of time-sensitive FL tasks. As such, we present the following strategies to tackle the aforementioned challenges.

*1) Parameter-efficient federated LLM fine-tuning:* To tackle the above challenges, parameter-efficient federated LLM finetuning can be adopted. Parameter-efficient federated LLM finetuning integrates PEFT into FL, enabling each client only to update and upload a small proportion of parameters, thereby reducing both communication and computing overhead [\[218\]](#page-39-9), [\[219\]](#page-39-10), [\[242\]](#page-39-26)–[\[245\]](#page-39-27). In [\[243\]](#page-39-28), Zhang et al. apply parameterefficient tuning to FL for training LLMs. The authors demonstrate the effectiveness of parameter-efficient federated tuning methods in training LLMs and their capabilities to defend against data inference attacks. In [\[218\]](#page-39-9), Jiang et al. propose a low-parameter FL approach that operates under limited communication and computing resource constraints. By combining LoRA and FL, edge devices update and upload only the LoRA parameters to the edge server instead of the full parameters in LLMs. Some works push the practical deployment of FL for LLMs. For instance, FederatedScope-LLM [\[219\]](#page-39-10) provides PEFT algorithms and versatile programming interfaces to facilitate FL for LLMs with low communication and computation costs without accessing the full model, making it well-suited for closed-source LLMs. Additionally, in [\[244\]](#page-39-29), an industrial-grade FL framework, called FATE-LLM, has been developed to support efficient FL for LLMs through PEFT methods, such as LoRA and P-Tuning-v2 [\[246\]](#page-39-30).

*Federated LLM prompt tuning* is another parameter-efficient federated LLM fine-tuning technique, which adapts LLM by optimizing a small amount of task-specific prompt vectors on edge devices during the FL process. For example, MetePFL [\[220\]](#page-39-11) focuses on addressing communication and computation efficiency challenges in real-world meteorological forecasting tasks with distributed sensors via prompt tuning. From the communication perspective, prompt tuning adjusts only a small set of parameters for specific tasks, significantly reducing the size of parameters to be uploaded for aggregation. For example, if a model has hundreds of millions of parameters, a prompt might consist of only a few thousand parameters, i.e., 0.01% to 0.1% of the entire model size [\[149\]](#page-37-30). From the computing perspective, prompt tuning requires updating only a small subset of LLM parameters, enabling resource-limited edge devices to participate. For instance, FedPepTAO [\[221\]](#page-39-12) devises parameter-efficient prompt tuning with adaptive optimization in FL. FedPepTAO focuses on updating parameters in proper prompt layers with small communication costs of LLMs, reducing the number of parameters transmitted and optimizing computing efficiency. Moreover, to leverage robust representations from LLMs while enabling efficient model personalization for clients in FL, the authors in [\[222\]](#page-39-13) propose pFedPG. The server-side personalized prompt generation module creates personalized prompts for all clients based on optimization direction information collected from clients. Then, clients update their client-specific prompts for personalization. Additionally, this approach ensures that only necessary and targeted parameters are updated, further optimizing communication and computing efficiency.

*2) Resource management in FL for LLMs:* Under the context of LLMs, the optimization of PEFT for FL over wireless networks can be a significant future direction, which is still in its nascent stage. The principle is to adapt the proportion of trainable parameters over time by considering both wireless channel conditions and model training status. Intuitively, a larger set of trainable parameters leads to better training performance while incurring more significant communication and computing latency. In [\[247\]](#page-39-31), based on the observation that model parameters gradually stabilize prior to model convergence, Chen et al. propose an adaptive parameter freezing scheme that freezes the non-synchronized stable parameters during the training process, eliminating the need for synchronizing the full models. However, this work does not consider the dynamic channel conditions in wireless networks. Considering PEFT techniques for LLMs such as LoRA, the rank of trainable matrices largely influences training accuracy and communication-computing latency in FL, as shown in [\[238\]](#page-39-22). Consequently, one essential research problem is how to jointly optimize the ranks in LoRA in LLMs and radio resource allocation in FL over wireless networks.

#### <span id="page-26-0"></span>*C. Split Learning*

While FL can be combined with various efficient fine-tuning techniques for training LLMs, it is still extremely resourceintensive for lightweight edge devices. Specifically, models such as GPT-3 or BERT contain billions of parameters. It is challenging for edge devices, such as smartphones or IoT devices, to perform computation-intensive parameter updates locally even with PEFT [\[248\]](#page-39-32). To address these concerns, SL can be a promising LLM training paradigm in edge networks, which enables the co-training of large-scale models through the collaboration among edge servers and edge devices [\[249\]](#page-39-33).

SL allows an edge server to take over the major training load from edge devices based on model splitting [\[250\]](#page-39-34). Since its invention, SL has been applied to various scenarios, such as healthcare [\[251\]](#page-39-35), [\[252\]](#page-39-36). Unlike FL, SL only places a submodel on edge devices for training, thereby considerably decreasing the workload of edge devices. The vanilla SL involves sequential interactions between an edge server and edge devices, which is a major bottleneck due to the waiting time of idle edge devices. The variants of SL, including parallel split learning (PSL) [\[253\]](#page-39-37), [\[254\]](#page-39-38) and SFL [\[255\]](#page-39-39), [\[256\]](#page-39-40), can be applied to enable parallel training while utilizing the resources on multiple edge devices. Clearly, analogous to FL, SL can also be integrated with PEFT or other resource-efficient techniques in Section [IV](#page-10-0) to alleviate the workload on edge devices further. For instance, edge devices are only required to execute the forward pass by freezing client-side parameters, considerably reducing the computing workload and memory usage.

Although SL can facilitate LLM training by harnessing edge servers, several challenges still exist in employing SL for LLMs. 1) *Communication costs for high-dimensional smashed data transmissions:* Although model partitioning leverages distributed computing resources and alleviates the computing load on edge devices, the communication overhead incurred by uploading cut-layer smashed data can be a major bottleneck. Considering GPT-3 Medium and an edge device with 100 data samples, each with 1024 tokens, the total smashed data volume at the cut layer can be approximately 400 MB for one training round [\[77\]](#page-35-38). 2) *Privacy leakage of target tokens:* While it is generally difficult to recover the raw training data of LLMs based on the received smashed data in SL [\[257\]](#page-39-41), there is a privacy risk of target token leakage (i.e., label leakage). During the process of SL, one commonly used LLM splitting scheme is to place the sub-LLM with the input module on edge devices and the sub-LLM with the output module on edge servers. In such a case, edge devices need to upload the target tokens of the input data to edge servers for LLM training, which leads to target token leakage. As presented below, several methods can be employed to tackle the said challenges.

- *1) Token representation reduction in SL:* First, importanceaware token representation pruning can be used to eliminate the unimportant token representation at the cut layer from uploading. In [\[223\]](#page-39-14), edge devices can selectively send the most informative smashed data to the server based on an online distillation method to perform SL. This approach improves performance while reducing communication costs by around 50% compared with the benchmark without smashed data selection. Second, the smashed data compression can be adopted before transmissions. In this respect, quantization can be employed to efficiently reduce the communication overhead in SL/SFL/PSL [\[224\]](#page-39-15). Specifically, quantization can convert float values (usually 32-bit) to low-bit representations, thereby enhancing the efficiency of smashed data transmission in SL. These methods can be adopted in the context of LLMs to shrink the data volume for uploading during the SL process.
- *2) U-shaped SL for LLMs:* In the SL framework, another common issue is the privacy leakage of ground-truth target tokens. In general, edge devices are required to transmit the target tokens of training samples to the server to calculate the loss function, leading to severe privacy concerns. For instance, the ground-truth target tokens of an LLM might be the disease type and health recommendations for a patient, which are considered sensitive personal data. To address this issue, Ushaped SL can be adopted for LLMs. U-shaped SL [\[258\]](#page-40-0), [\[259\]](#page-40-1) leaves both the head neural layers or transformer blocks with the input module and the tail layers/blocks with the output module on edge devices while only placing the intermediate layers/blocks on edge servers, thus effectively preserving label privacy. When applying this approach to LLM training, the computation-sensitive decoders can be placed on edge servers, and the bottom layers with the text input module and the top layers with the text output module can be stored on edge devices [\[225\]](#page-39-16), leading to better privacy preservation.
- *3) Resource management in SL for LLMs:* To support SL for LLMs effectively and efficiently, the joint design of SL and radio resource allocation is necessary. Such joint problems of model splitting and/or radio resource allocation have been studied for SFL or PSL [\[253\]](#page-39-37), [\[256\]](#page-39-40). In [\[253\]](#page-39-37), we propose an efficient PSL approach by employing last-layer backpropagated gradient aggregation to reduce the computing, communication, and memory load on an edge server when serving multiple clients. Then, we design the joint model splitting and channel allocation strategies to mitigate the straggler issue in wireless PSL. However, it is shown that such an optimization problem is often integer programming due to the model splitting decisions, which can be NP-hard and compute-intensive when the number of edge devices is large [\[260\]](#page-40-2). With Transformer-based LLMs, the transformer blocks are generally repeated (unlike CNNs with varying output sizes for each layer) within an encoder or decoder,

thus making the optimal optimization problem more solvable. This provides an opportunity to design the optimal radio resource allocation and model splitting strategy using a much more efficient algorithm. Particularly, when userside computing capability is the limiting factor, a shallower model split point results in reduced communication-computing latency, as the communication costs remain consistent across different cutting blocks. However, it is important to note that a shallow split point may lead to increased privacy leakage of raw data; therefore, the split layer should not infringe on users' privacy requirements. In summary, the unique structure of Transformer-based LLMs can be harnessed to develop effective and efficient model splitting and resource allocation strategies under the SL framework.

## *D. Hierarchical Collaborative Learning*

The hierarchical collaborative learning paradigm can facilitate LLM training at scale [\[13\]](#page-34-18), [\[261\]](#page-40-3), [\[262\]](#page-40-4). Compared with the previous edge-only paradigm, hierarchical collaborative learning provides improved flexibility to accommodate varied task complexity and resource availability by exploiting the synergy among clouds, edge servers, and edge devices. For instance, computation-intensive training tasks can be offloaded to cloud servers, whereas relatively easy ones can remain on the network edge to save communication latency/bandwidth. Besides, hierarchical collaborative learning is indispensable for LLMs to learn global knowledge across large geographical regions, where LLM updates can be synchronized at a central cloud. As presented below, hierarchical collaborative learning can be divided into three categories: cloud-end collaboration, cloud-edge collaboration, and cloud-edge-end collaboration.

- *1) Cloud-end collaboration:* The cloud and edge devices can collaboratively train LLMs together. To reduce the computing workload of edge devices and enhance user privacy, Gao et al. in [\[263\]](#page-40-5) propose a distributed PEFT framework called DLoRA. This framework maintains and fine-tunes personal PEFT modules on edge devices while storing the frozen parameters of LLMs in the cloud. By exchanging activations and gradients, edge devices and the cloud can collaboratively train the LLM. Besides, to reduce the communication overhead of edge devices, the authors adopt the Kill and Revive mechanism to dynamically identify and fine-tune the most relevant and significant PEFT module. In [\[226\]](#page-39-17), Wang et al. propose a cloud-end collaborative learning framework for multimodal LLMs, in which a larger multimodal LLM is located in the cloud while a smaller multimodal LLM is allocated to the edge device. Edge devices, such as robots, upload multimodal data to the cloud to perform KD to train the adapters, which are then downloaded to the edge devices. To reduce the communication overhead in data uploading, the authors adopt the uncertainty-guided token sampling strategy, enabling edge devices to upload only the most informative tokens to the cloud.
- *2) Cloud-edge collaboration:* Cloud-edge collaborative LLM training enables the cloud and edge servers to collaboratively train LLMs. On the one hand, LLMs can initially undergo pre-training in the cloud, followed by additional finetuning on edge servers to enhance performance. In [\[13\]](#page-34-18), Xu et

- al. propose a cloud-edge collaborative training and fine-tuning framework for generative AI models, where models are pretrained in the cloud to learn general features, after which the models are fine-tuned with context-aware data stored on edge servers for customization. On the other hand, edge servers can train LLMs locally and then send the model updates to the cloud for knowledge sharing, such as model aggregation in the FL framework, which can improve the generality of LLMs. In [\[227\]](#page-39-18), the edge models of multiple edge servers are integrated using gating neural networks and linear projection connections to form a large-scale model suitable for multi-task and multimodal learning. The cloud trains this large-scale model with the cloud common dataset and then distributes task-specific lightweight models back to the edge servers for personalized fine-tuning.
- *3) Cloud-edge-end collaboration:* Cloud-edge-end collaborative learning, which has a three-tiered architecture, brings in more distributed computing resources and flexibility in resource management. In [\[228\]](#page-39-19), the authors propose a hierarchical FL framework, where edge servers aggregate the locally trained models from their associated end users, and the cloud aggregates models from different edge servers. Furthermore, to reduce the communication cost in the hierarchical FL framework, the authors in [\[229\]](#page-39-20) adopt model quantization to compress model sizes for efficient model aggregation, where a convergence upper bound is provided to optimize the aggregation intervals. The hierarchical collaborative learning paradigm is suitable for LLM training. Since the raw data in some LLM applications, such as healthcare, is highly sensitive, users can join the training process with the help of both cloud and edge servers while keeping private data on their local devices [\[264\]](#page-40-6). However, despite these advantages, model delivery and data transmission among the cloud, edge servers, and edge devices must be appropriately handled since delivering model parameters of LLMs incurs significant communication latency, especially in links from the cloud to the edge server.
- *4) Resource management in hierarchical collaborative learning for LLMs:* In hierarchical collaborative training, model aggregation intervals (in FL or SFL) and model splitting must be carefully designed to maximize training accuracy with reduced latency and data traffic costs for LLMs based on the aforementioned techniques in Section [VII-B](#page-25-0) and [VII-C.](#page-26-0) For instance, the proportion of uploaded parameters in LLMs must be dynamically adjusted based on PEFT by taking into account the status of model training, wireless channel conditions, and the Internet delay in the backbone networks. Also, the model splitting and uploaded token representation pruning must be carefully devised across different tiers of the cloud-edge-end architecture. The increased number of network tiers adds more complexity to the optimization problem, giving rise to more challenges and opportunities in this direction.

# *E. Lessons Learned*

Analogous to edge training for other AI models, edge LLM training aims to optimize training accuracy under limited communication-computing resources. Nevertheless, since LLM training is much more resource-demanding, the underpinning principle should be splitting the training and "only

<span id="page-28-4"></span><span id="page-28-1"></span>![](_page_28_Figure_1.jpeg)

(a) Centralized edge inference.

<span id="page-28-2"></span>(b) Split inference.

<span id="page-28-3"></span>(c) Collaborative inference.

Fig. 17: Edge inference frameworks for LLMs. This figure presents the associated challenges and corresponding techniques in each sub-figure. In Fig. [17\(a\),](#page-28-1) edge devices centralized the data to the edge server with LLMs for centralized edge inference. In Fig. [17\(b\),](#page-28-2) edge devices feed the data into the user-side sub-LLM and upload the intermediate features to the edge server. Subsequently, the edge server conducts the inference with the server-side sub-LLM and returns the inference results to edge devices. In Fig. [17\(c\),](#page-28-3) edge devices first generate preliminary inference results with the user-side lightweight LLMs and upload the results to the edge server with the larger LLM for verification.

updating a little". In other words, the process must borrow the wisdom from large-scale model training, i.e., parallel training across multiple edge devices/servers, and PEFT over wireless networks. This implies that SL can be an important enabling edge learning framework in the era of LLM. To make the MEI tailored for LLMs, the key principle is to enable flexible model splitting based on the dynamic communication-computing resources within edge networks and determine the fine-tuning portion of LLMs by jointly considering communicationcomputing resources and the training status of LLMs. To ground the network optimization, a theoretical understanding of fine-tuning of LLMs, such as LoRA, is required, i.e., characterizing the relationship between training accuracy and the percentage of trainable parameters, to determine the optimal freezing ratio for fine-tuning over wireless networks.

# VIII. EDGE INFERENCE FOR LLMS

<span id="page-28-0"></span>Edge LLM inference leverages edge computing to provide inference services to end users based on well-trained LLMs. Inference services in edge networks involve processing input data, such as text, images, and audio, to generate results like classifications, predictions, and responses with AI models for edge devices, including mobile users and IoT devices. One example is LLM-empowered robotic applications, where a robot uploads captured audio, video, and radar/lidar data across different modalities for feeding into a well-trained LLM on an edge server, thus facilitating the robot to understand the environments and execute actionable policies. The major difference between edge inference for LLMs and conventional edge inference framework lies in *the exploitation of LLM properties, such as multimodality, parameter-shareability, and the autoregressive process, to accelerate the inference process*. As illustrated in Fig. [17,](#page-28-4) this section elaborates edge LLM inference through three categories: centralized edge inference, split inference, and collaborative inference, with the related works summarized in Table [VI.](#page-29-0)

#### *A. Centralized Edge Inference*

In centralized edge LLM inference, edge devices offload their input token to edge servers for AI inference [\[24\]](#page-34-20). Note that efficient LLM inference methods, mentioned in Section [IV-A,](#page-10-2) can be implemented on edge servers to accelerate edge LLM inference. Centralized edge inference involves raw data uploading from edge devices to edge servers, which shares similar challenges with centralized edge learning in Section [VII-A,](#page-22-1) i.e., *communication latency for uploading data* and *computing burden (including excessive memory usage) on edge servers*. In what follows, we will elaborate on the techniques to overcome these challenges.

*1) LLM inference with cross-modal input token reduction:* Analogous to centralized LLM training, input token reduction decreases the volume of data transmissions for fast edge inference. By removing some unimportant tokens from input text sequences and images before inference, the sizes of data that need to be offloaded to edge servers are reduced [\[265\]](#page-40-7), [\[279\]](#page-40-8), [\[280\]](#page-40-9). It has been shown that input token pruning does not introduce significant inference accuracy loss for LLMs [\[266\]](#page-40-10). For example, before inputting images into the ViT, removing some image patches unrelated to the visual contents of the images may not degrade the prediction results [\[265\]](#page-40-7). Since the significant communication overhead of input tokens comes from visual tokens, an effective approach to reducing communication overhead is to exploit text/audio data tokens to decrease the unrelated visual tokens in multimodal LLM inference. In other words, cross-modality can be fully explored to eliminate communication-computing redundancy in LLM inference.

*2) Parameter-sharing service placement/migration for LLM inference:* For service provisioning in multi-user systems, the scarcity of computing resources, say, memory space, on an edge server is a major concern. This characteristic severely limits the number of edge LLM inference services to be supported by an edge server. For this reason, properly designing service placement in centralized edge inference be-

TABLE VI: Summary of related works on edge LLM inference.

<span id="page-29-0"></span>

| Scenarios<br>Techniques<br>Ref.<br>Objectives |                                                                       |       |                                                                                                                                                                                                                                                                                                                     |
|-----------------------------------------------|-----------------------------------------------------------------------|-------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|                                               | LLM inference<br>with cross-modal                                     | [265] | Removes some image patches unrelated to the visual contents of the images that input into the ViT,<br>which can be used to reduce communication overhead in centralized edge LLM inference.                                                                                                                         |
|                                               | input token reduction                                                 | [266] | Illustrates that input token pruning does not degrade inference accuracy significantly for LLMs.                                                                                                                                                                                                                    |
|                                               |                                                                       | [202] | Optimizes the service placement and inference task offloading strategies in a cloud-edge collaborative<br>inference framework.                                                                                                                                                                                      |
| Centralized<br>edge inference                 | Parameter-sharing<br>service                                          | [267] | The requests of end users for the same LLM are offloaded to the same edge server that loads the<br>request LLM, improving memory/storage efficiency.                                                                                                                                                                |
|                                               | placement/migration<br>for LLM inference                              | [268] | The inference requests of users are forwarded to an edge server with the lowest estimated startup<br>time, thereby providing inference services with low latency.                                                                                                                                                   |
|                                               |                                                                       | [24]  | Users offload LLM inference tasks to an edge server under the jointly optimized communication and<br>computing resource allocation policies.                                                                                                                                                                        |
|                                               |                                                                       | [269] | Inserts a binarization module after layer norm layers in Transformers to quantize the token represen<br>tations with 1-bit vectors.                                                                                                                                                                                 |
|                                               |                                                                       | [270] | Prunes the redundant vectors of intermediate outputs of encoders progressively.                                                                                                                                                                                                                                     |
|                                               |                                                                       | [271] | Proposes ToMe and merges the representations of similar tokens in the outputs of MHAs.                                                                                                                                                                                                                              |
|                                               | Split inference<br>for LLMs with<br>token representation<br>reduction | [272] | Uses information bottleneck to enable edge devices to extract and upload informative features to edge<br>servers while keeping comparable inference accuracy in split inference.                                                                                                                                    |
|                                               |                                                                       | [273] | Combines the transmission rate maximization and the information bottleneck together in split inference<br>to improve the robustness of the received features and ensure satisfactory inference performance<br>without incurring too much communication overhead.                                                    |
|                                               |                                                                       | [274] | Removes image token representations irrelevant to the input texts and unimportant to the tasks for<br>ViTs and merges similar text and image token representations independently.                                                                                                                                   |
| Split                                         |                                                                       | [275] | Adopts deep joint source-channel coding to train encoders and decoders of Transformers by consid<br>ering the effects of physical channel noises and interference, making split inference for Transformers<br>robust.                                                                                               |
| inference                                     | Progressive<br>split inference                                        | [276] | Users schedule the offloading sequence of features and upload the features with higher importance<br>to edge servers first until edge servers determine that the uploaded features reach a target confidence<br>level for inference.                                                                                |
|                                               | Split inference<br>with early exit                                    | [138] | Demonstrates that the hidden states of the current token need to be forwarded to the subsequent layers<br>for KV cache computation in LLM inference using early exit, illustrating the necessity of designing<br>the uploading strategy of hidden states at exiting layers for split LLM inference with early exit. |
|                                               | Multi-hop/U-shaped<br>split inference<br>for LLMs                     | [277] | An LLM is partitioned into multiple sub-models to be placed on multiple edge devices/servers for<br>multi-hop split inference via inter-device (server) communication.                                                                                                                                              |
|                                               |                                                                       | [225] | Adopts the U-shaped split LLM inference, where only the body sub-LLM with the hidden layers in<br>decoder blocks is placed on the edge server, and only the outputs of intermediate Transformer blocks<br>are exchanged between edge devices and edge servers.                                                      |
| Collaborative                                 | Speculative                                                           | [136] | Speculative decoding enables an edge device to run a smaller on-device LLM while asking the edge<br>server to run a larger LLM to verify and correct the output tokens uploaded by the edge device for<br>reducing E2E latency.                                                                                     |
| inference                                     | decoding                                                              | [278] | Edge devices can decide whether to upload the generated results to edge servers with powerful LLMs<br>for verification based on calibrated confidence scores.                                                                                                                                                       |

comes crucial to accommodate large-sized models. Moreover, considering the mobility of users, service migration, which migrates the AI models from one place to another according to varying user locations, can also be studied. In [\[202\]](#page-38-28), the authors optimize the service placement and inference task offloading strategies in a cloud-edge collaborative inference framework, where the cloud and edge servers provide inference services with the cached LLMs. The framework aims to minimize the total inference cost of edge servers and the cloud within the GPU memory and computing capacity constraints. Here, the total inference cost of edge servers includes model loading/evicting costs, transmission costs of input prompts and inference results, edge computing costs, and accuracy costs. Another potential method for determining service placement strategies is jointly considering model requests and required computing resources. The queries of end users requesting the same LLM can be offloaded and forwarded to the same edge server [\[267\]](#page-40-11), thereby improving memory/storage efficiency since the LLM can remain in the memory of the edge server. The authors in [\[268\]](#page-40-12) design a serverless inference system for LLMs, where inference requests can be forwarded to an edge server with the lowest estimated startup time, including LLM loading and migration time. For example, the inference requests can be forwarded to an edge server with the request LLM already in its memory by eliminating additional latency for loading the LLM from the storage disk.

Although the aforementioned excellent works have been done on service placement for inference, these works have not exploited the shared parameters among models/tasks for edge inference. In fact, by exploiting this feature, multiple LLMs with substantially shared parameters can be loaded into the memory of a server for concurrent inference, thereby significantly improving inference throughput by serving more user requests simultaneously. This requires the design of parameter-sharing service placement schemes for LLM inference by jointly considering the memory, storage, computing, and spectrum constraints of edge networks.

*3) Resource management for centralized LLM inference:* To cater to more users with heterogeneous communicationcomputing resources and service requirements, integrated communication-computing resource allocation needs to be investigated. A key principle in centralized edge inference involves assigning users with delay-sensitive tasks with more spectrum bandwidth and computing resources to satisfy their QoS requirements. In [\[24\]](#page-34-20), Fang et al. propose an edge LLM inference scheme where users can offload LLM inference tasks to an edge server under the designed communication and computing resource allocation policies. By jointly allocating spectrum bandwidth and computing resources for inference tasks, the average E2E latency and inference accuracy can be optimized. In the context of LLM inference, radio resources can be jointly optimized with input token pruning and parameter-sharing service placement. On the one hand, wireless channels can be allocated more to users with highdimensional input tokens to upload in order to achieve high inference accuracy. The optimization problem can, therefore, be formulated as an accuracy maximization (or fairness) problem subject to computing-communication latency constraints by jointly optimizing users' token pruning ratios and radio resource allocation. On the other hand, when considering parameter-sharing service placement, a trade-off exists between communication latency and storage/memory efficiency. Specifically, although offloading multiple inference requests requiring the same LLM to a server enhances storage/memory efficiency, as demonstrated above, data transmissions may be subject to longer communication latency, adversely affecting QoS for users. This motivates us to study the parametersharing service placement/migration problem under latency constraints while considering channel allocation and data routing over wireless networks [\[281\]](#page-40-23), [\[282\]](#page-40-24).

Furthermore, one particular challenge in edge LLM inference lies in the autoregressive process of LLMs. This characteristic introduces two kinds of difficulty: 1) the task computing delay can hardly be predicted since the length of output tokens during the decoding process may not be known at the beginning, and 2) the memory usage is relevant to the length of output tokens according to KV cache, implying that we must reserve sufficient memory and computing resources for task scheduling. Based on the above observations, we can explore new mathematical modeling of edge computing to address resource optimization for edge LLM inference.

#### *B. Split Inference*

Split inference is a technique that offloads part of the computing workload from edge devices to edge servers by placing partitioned models on edge devices and edge servers, respectively [\[194\]](#page-38-20), [\[249\]](#page-39-33). The most widely adopted split inference paradigm is the bi-partition paradigm. Edge devices execute on-device inference with the user-side sub-model based on the raw data and upload intermediate features to edge servers for going through the remaining neural networks [\[273\]](#page-40-17). Split inference is particularly well-suited for LLM inference. On the one hand, compared with on-device LLM inference, split LLM inference offloads a major part of computation to the edge server, thus reducing the device-side workload, which is of paramount importance for computation-intensive LLM inference. On the other hand, given that LLM applications within edge networks, such as mobile health and autonomous driving, often involve highly sensitive personal data, split LLM inference effectively mitigates privacy issues as edge devices do not need to share private raw data with edge servers.

There are two prominent bi-partition methods for split LLM inference. First, the most resource-intensive modules of LLMs can be placed on the server to fully harness its computing power. In Transformers, the decoder module often requires more computing resources than the encoder module [\[136\]](#page-37-17). Therefore, for LLMs with the encoder-decoder architecture, such as BART [\[81\]](#page-35-42), the encoder module can often be cached on edge devices while the decoder module can be placed on edge servers. For decoder-only based LLMs, such as GPTseries models, the text and position embeddings as well as several bottom Transformer blocks can be placed on edge devices due to their relatively light workload, whereas the remaining Transformer blocks can be placed on edge servers. Second, the sub-model occupying large storage capacity can be placed on edge servers. For example, the FFNs account for approximately 2/3 of parameters in LLMs, while only a small portion of parameters in FFNs have a significant contribution to the final inference results [\[283\]](#page-40-25). Therefore, these parameters in FFNs, with smaller data sizes yet critical for inference, can be placed on edge devices to save storage/memory resources.

Split inference for LLMs encounters similar challenges to SL, as discussed in Section [VII-C,](#page-26-0) i.e., *communication costs for high-dimensional feature uploading*, *computing latency on edge devices/servers*, and *inference output leakage*. In what follows, we will introduce the techniques to address these challenges.

*1) Split inference for LLMs with token representation reduction:* Considering split inference, there are various ways to reduce the volume of token representations (latent representation) at the cut layer in LLMs for uploading to edge servers. First, the outputs of intermediate layers in Transformers can be compressed via quantization [\[156\]](#page-37-6), [\[284\]](#page-40-26), [\[285\]](#page-40-27), pruning [\[270\]](#page-40-14), [\[274\]](#page-40-18), and merging [\[286\]](#page-40-28), [\[287\]](#page-40-29). Uploading the compressed intermediate layer outputs can substantially reduce the communication overhead, thereby achieving low-latency split inference. 1) Quantization enables converting intermediate outputs of hidden layers in Transformers from high bitwidth to low bitwidth. In [\[269\]](#page-40-13), the authors insert a binarization module after the layer norm layer inside the Transformer encoder blocks to represent the token representations with 1 bit vectors instead of float-value vectors. 2) Pruning involves removing redundant vectors of token representations, which can apply to both images and text inputs based on a lightweight module [\[274\]](#page-40-18). For example, the authors in [\[270\]](#page-40-14) eliminate the redundant vectors of the intermediate encoder outputs, where the remaining intermediate encoder outputs of the last encoder only account for 2% of the original intermediate encoder outputs. With this method, edge devices can eliminate unimportant vectors of intermediate layer outputs before uploading them to edge servers in split LLM inference. As the size of remaining representation vectors decreases as the encoder goes deeper, which indicates that more encoder blocks remain on edge devices for local computing, it is worthwhile investigating the optimal cut layer selection involving the trade-off between communication efficiency and computing burden on edge devices. 3) Merging can compress the information of intermediate outputs by merging redundant token representations with similar semantic meanings. The authors in [\[271\]](#page-40-15) propose ToMe inserted into the Transformer blocks, which merges the representations of similar tokens in the outputs of MHAs. It is promising to employ this method in split LLM inference so that edge devices merge the representations in intermediate results with the lightweight merging module before uploading to edge servers.

Feature extraction can also be adopted here to map the original token representation into a new dimension, which is often a lower dimension, while maintaining informative features sufficient for accurate split inference [\[288\]](#page-40-30). A popular approach for this is the information bottleneck method, which reduces the size of intermediate features while keeping comparable inference accuracy in split inference [\[272\]](#page-40-16). The principle of information bottleneck lies in maximizing the mutual information between the inference results and the extracted features while minimizing the mutual information between the input data and the extracted features, which can be adopted in split LLM inference. This allows edge devices to extract informative features and upload the extracted features to edge servers. Furthermore, the authors in [\[273\]](#page-40-17) propose the robust information bottleneck by combining the transmission rate maximization and the information bottleneck together. By considering the robustness of information distortion in the received features, this principle can improve the robustness of the received features and ensure that edge servers receive sufficient informative features without incurring too much communication overhead. By using the feature extraction method, split LLM inference can alleviate the communication burden of transferring high-dimensional features by uploading informative features to edge servers.

For multimodal LLMs, one can exploit the correlation across different modalities when reducing the token representation of different modalities, thereby reducing the communication latency. A number of works have discussed how to reduce multimodal token representation in LLMs. In [\[274\]](#page-40-18), the authors propose a cross-modal representation reduction method for ViTs. The ViTs use image text cross-modal encoders to integrate input texts and images for vision language tasks, such as visual question answering. The proposed framework aims to reduce the computing overhead in crossmodal interaction. Specifically, it first removes image token representations irrelevant to the input texts and unimportant to the tasks. Then, it independently merges similar text and image token representations to reduce the computing overhead. By applying this method to split inference, edge devices can select and upload the informative representations to edge servers based on the correlation across multiple modalities. Additionally, the semantic information of the selected cross-modal features can be extracted using approaches such as crossattention mechanisms [\[289\]](#page-40-31) and the feature similarity-based semantic fusion [\[275\]](#page-40-19). Following the principle of deep joint source-channel coding (JSCC) [\[275\]](#page-40-19), [\[290\]](#page-40-32), the encoders and decoders can be trained by considering the effects of physical channel noises and interference, making split inference robust to such adverse channel effects. Since this field is still in its infancy, cross-modal JSCC for split LLMs can be a significant research direction for 6G edge intelligence.

- *2) Progressive split inference:* The progressive split inference mechanism [\[276\]](#page-40-20) can be adopted here to eliminate the unnecessary transmissions of intermediate token representations as long as a desired inference accuracy can be satisfied. In [\[276\]](#page-40-20), users can schedule the offloading sequence of features before forwarding them to edge servers. The features with higher importance, such as features with higher discriminant gain, are uploaded to edge servers first. The progressive feature offloading will terminate until edge servers determine that the uploaded features reach a target confidence level sufficient for edge servers to conduct inference above the accuracy threshold. By using this paradigm in split LLM inference, communication overhead between end users and edge servers can be reduced, thus saving communication resources.
- *3) Split inference with early exit:* Early exit techniques, as introduced in Section [IV-A,](#page-10-2) have been widely used in LLM inference for latency reduction. This approach can be adopted in split inference to reduce computing latency on edge devices/servers. In this context, when adding early exit modules into user-side sub-LLMs, the inference computation in later layers in the user(server)-side sub-LLMs can be skipped, implying that there is no need for uploading intermediate features for latency reduction. However, in LLM inference with the early exit approach, the hidden states of the corresponding token in the later layers are missing when the token outputs the results in the early exit layer. Therefore, in split LLM inference with the early exit technique, if the early exit layers reside at user-side sub-LLMs, the hidden states still need to be uploaded to edge servers for KV cache computation in later layers. As such, we need to design the uploading strategy of hidden states at exiting layers, e.g., opportunistically uploading the hidden states by considering channel conditions.
- *4) Other variants of split inference:* Although split inference enhances user privacy by remaining raw data on local devices, the bi-partition paradigm still allows edge servers to obtain the inference results, which can be privacy-sensitive. To address this problem, the U-shaped or Λ-shaped split LLM inference paradigm can be adopted. In [\[225\]](#page-39-16), an LLM is divided into three sub-models at the decoder blocks. The three sub-models are the head sub-LLM with the text input module, the body sub-LLM with the hidden layers in decoder blocks, and the tail sub-LLM with the text output module. Most decoder blocks, which demand more computing resources, can be placed on edge servers to fully leverage edge servers' computing resources. During the inference process, only the outputs of intermediate Transformer blocks in LLMs are exchanged between the end user and the edge server. These outputs are high-dimensional vectors, which are difficult for

edge servers to interpret, thereby effectively preventing edge servers from recovering either the raw data or inference results.

Moreover, split inference can be extended into multi-edge scenarios to unleash the power of distributed edge servers in wireless edge networks, resulting in multi-hop split inference. In these scenarios, an LLM can be partitioned into multiple sub-models to be placed on multiple edge devices/servers according to their computation abilities and the inter-device (server) communication conditions [\[277\]](#page-40-21). In this way, edge inference can be carried out sequentially with a mesh of edge servers by transmitting sub-model layer outputs to the next server [\[291\]](#page-40-33). Obviously, developing efficient algorithms to obtain the optimal splitting decisions for such a scenario becomes much more challenging.

#### *C. Collaborative Inference*

Split inference exchanges high-dimensional intermediate features, resulting in excessive communication overhead. To overcome this limitation, edge devices and edge servers can cooperate in some other modes with the smaller size of information exchange. For instance, speculative decoding [\[136\]](#page-37-17), which has been introduced in Section [IV-A,](#page-10-2) can be adopted in device-server collaborative inference for LLMs. Speculative decoding enables an edge device to run a smaller on-device LLM, called an approximation model, while asking the edge server to run a larger LLM to verify and correct the output tokens uploaded by the edge device. The main advantages of this approach are threefold: First, it empowers edge devices to generate preliminary results/decisions, which can be used for low-latency inference. Second, the parallel decoding process accelerates the inference process on the edge server. Since LLMs generate tokens with the autoregressive decoding technique, verifying a long sequence of tokens is much faster than decoding serially by the server-side LLM alone. Third, concerning communication overhead, the output token can be smaller than cut-layer intermediate features in many cases. To further save communication-computing resources, edge devices can decide whether to upload the tokens generated by on-device LLMs to edge servers for verification [\[278\]](#page-40-22) based on calibrated confidence scores, because highly confident outputs might not require an edge server to consume resources for verification.

#### *D. Lessons Learned*

Resource-efficient LLM inference techniques can be adapted to wireless edge environments, enabling effective cooperation between edge devices and edge servers. Thus, edge LLM inference must be jointly optimized with various LLM techniques, such as KV cache optimization, multimodal feature extraction, and autoregressive decoding, all of which significantly impact memory usage, communication overhead, and computing latency for LLM inference. For this reason, the design of edge LLM inference, such as model splitting, early exiting, and radio-computing resource allocation, naturally differs from conventional edge inference systems, creating a rich set of research problems in this area.

# IX. FURTHER RESEARCH OPPORTUNITIES

<span id="page-32-0"></span>MEI for LLMs remains a largely uncharted direction. This section will discuss the pressing concerns and potential solutions for MEI4LLM.

#### *A. Green Edge LLM*

Energy consumption is a primary public concern over LLMs. For instance, it is estimated that the energy consumption of training GPT-4 is equivalent to the consumption over 5 to 6 years of 1,000 average US households. Moreover, the energy costs of model inference can be even higher due to the frequent service requests from users worldwide. The estimated energy footprint of GPT-4 training and inference are detailed in Table [VII.](#page-32-1) These facts underscore the urgent need for the development of energy-efficient LLM training and inference techniques.

<span id="page-32-1"></span>TABLE VII: Approximate energy usage of GPT models.

| Name  | Total energy consumption<br>in training (MWh) | Daily energy consumption<br>of responses (MWh) |
|-------|-----------------------------------------------|------------------------------------------------|
| GPT-3 | 1,287 [292]                                   | 3 [293]                                        |
| GPT-4 | ≥50,000 [294]                                 | 5 [293]                                        |

By pushing the intelligence to the network edge, MEI4LLM can reduce the energy consumption of LLMs in three aspects. First, making model fine-tuning or inference services available at the network edge eliminates the need to transmit large volumes of data to cloud centers, thereby reducing the energy costs in data transportation. Second, integrated communication-computing design can be jointly optimized to enhance energy efficiency for training/inference. For instance, less data communication can be achieved by data compression or parameter freezing to reduce total transmit power as long as the required training/inference accuracy can be achieved. At last, edge LLM can utilize a small-scale LLM to obtain initial inference results and harness the power of the cloudbased large-scale LLM only when the inference confidence is low. This potentially reduces the energy consumption at cloud centers resulting from invoking large-scale LLMs for every user request.

Regarding the research problems, green edge LLM features the integrated design of wireless communications and computing, which must account for the overall transmission and computing energy. With this in mind, there are two main design goals, i.e., reducing energy consumption on edge devices or decreasing the overall energy consumption for green AI. The first goal is to benefit battery-constrained IoT and mobile devices, making AI services or training more accessible to customers. To achieve this goal, AI training/inference can be offloaded to edge servers as long as it saves energy for end users. For instance, when adopting PEFT and SL, the client-side models can be frozen as much as possible, even entirely frozen, to minimize energy consumption on edge devices. Although this approach might lead to slower learning convergence or increase the energy consumption on edge servers for achieving a target training/inference accuracy, this cost might be less concerning since edge servers are usually more powerful and connected to powerlines. The second one aims to decrease the overall system costs, particularly from the perspective of mobile operators. By minimizing the total energy consumption or maximizing the overall energy efficiency, network operators can improve AI-centric metrics with limited or lower energy costs. A meaningful metric can be "AI energy efficiency" [\[295\]](#page-40-37), i.e., the amount of intelligence (AI accuracy) achieved per energy cost. The optimization of AI by considering energy consumption can eliminate the case where the system uses a great amount of energy to merely achieve minor improvements, which is not justifiable from both operators' and society's viewpoints.

# *B. Secure Edge LLM*

Secure edge LLM is another important branch of research. Although federated learning and split learning, as alluded to earlier, can serve as privacy-enhancing training frameworks for LLMs to avoid direct access to personal data, there is still privacy risk as malicious servers may launch attacks to recover the raw data based on received models or intermediate features [\[296\]](#page-40-38), [\[297\]](#page-40-39). However, while LLM security has been extensively studied, edge LLM security has received much less attention. At the mobile edge, security components should be integrated to ensure secure and trustworthy LLMs.

Let us consider two aspects of secure edge LLM. The first aspect is defending against inference attacks to protect user privacy leakage from honest-but-curious edge servers or other edge devices. It has been demonstrated that prompting LLMs may reveal the private information of other users, such as credit card information, in the training process by only inserting a few benign-appearing sentences into the training dataset [\[298\]](#page-40-40). For FL and SL, similar problems may still exist. From the user perspective, one potential defense mechanism is to add noise to protect highly sensitive personal data, such as credit card information, based on the theory of differential privacy. The research question is where to insert noise and the amount of noise to insert.

The second aspect is defending against data poisoning or backdoor attacks by filtering out malicious users who aim to alter the training process, thereby maintaining training effectiveness. These attacks can lead to severe consequences for LLMs, i.e., outputting harmful health guidance if considering a healthcare LLM. Although such attacks have been studied for LLMs [\[299\]](#page-40-41), [\[300\]](#page-41-0), there is still a lack of research works on designing effective attack/defense mechanisms by considering distributed learning for LLMs, particularly FL and SL, over edge devices. Considering PEFT, the attacker may only be able to change a small proportion of the models, e.g., adaptors or prompts, to impact the training process rather than modifying the full set of parameters. This leads to new challenges/opportunities in designing the attack/defense schemes.

#### *C. Quality-aware Edge LLM Training*

Unlike cloud-based training, data quality at the network edge cannot be well controlled due to the lack of human annotations and monitoring. Unfortunately, training over massive and low-quality data/labels can be detrimental to model performance rather than beneficial. To effectively deploy LLMs at the network edge, it is essential to enable LLM training with automated quality control.

There are two key research directions for improving the quality of edge LLM training. First, improving the quality of training datasets can directly improve edge LLM training outcomes. On the one hand, edge servers can increase the diversity of training datasets by prioritizing varied data sources. Higher diversity in training datasets increases the likelihood of achieving better performance on corresponding tasks [\[301\]](#page-41-1). For example, in federated learning, edge servers can allocate additional training rounds to edge devices with more diverse data. Additionally, in cloud-edge-end FL, the cloud center can assign varying weights to edge LLMs during aggregation based on the regional diversity associated with different edge servers. On the other hand, to ensure high accuracy and reliability in LLM responses, data cleaning for training datasets can be adopted for edge LLM training. Considering LLMs' sensitivity to input data, data cleaning is essential to detect and remove errors, irrelevant information, duplicates, and inconsistencies before LLM training [\[302\]](#page-41-2). For example, in LLM-empowered AI chatbots, data cleaning can identify and remove useless information and grammatical errors in raw data inputs, thereby enhancing the quality of the training datasets and ensuring LLMs' performance [\[303\]](#page-41-3). However, since data cleaning introduces additional computational overhead to edge servers/devices, a trade-off exists between computational efficiency and the quality of training data.

Second, edge LLM training can harness a large volume of unlabeled data or data with noisy labels captured by edge devices. While the training quality of LLMs significantly depends on high-quality training datasets, acquiring enough high-quality labeled data for edge devices requires considerable effort and costs, particularly for the vast number of resource-limited IoT devices at the network edge. Therefore, edge servers must leverage the large volume of unlabeled data collected from distributed edge networks to train LLMs with unsupervised learning [\[304\]](#page-41-4) and semi-supervised learning. For LLMs, edge servers can fully leverage the relationships between limited labeled data and large volumes of unlabeled data across different modalities. For example, on-device LLM can continuously capture data from multiple modalities, automatically generating labels for data of specific modalities. By utilizing the inherent patterns and correlations in these data, edge servers can reconstruct the labels of unlabeled data, thereby enhancing the training quality and adaptability across various tasks.

# X. CONCLUSIONS

<span id="page-33-0"></span>In recent years, language models have experienced exponential growth in size, giving birth to numerous LLMs with billions of parameters. This trend urges us to think about how edge intelligence can accommodate these giant models. In this article, we advocated the paradigm shift from cloud computing to 6G MEI for LLM deployment. We highlighted killer applications to motivate this paradigm shift, arguing that cloud computing can hardly fulfill the latency, bandwidth, and privacy requirements. Meanwhile, we identified the key challenges that mainly arise from the resource limitations at the network edge. To address these challenges, we first proposed a 6G MEI architecture for LLMs and then elaborated on several methods to enable efficient edge caching and delivery, edge training, and edge inference for LLMs under the resource-constrained mobile edge. We hope this article can inspire more researchers in the wireless community to explore the deployment of LLMs at the mobile edge and further advance this emerging field.

# REFERENCES

- <span id="page-34-0"></span>[1] Google, "A large language model from Google research, designed for the medical domain," 2023. [Online]. Available: [https://sites.research](https://sites.research.google/med-palm/) [.google/med-palm/](https://sites.research.google/med-palm/)
- <span id="page-34-1"></span>[2] G. DeepMind, "RT-2: New model translates vision and language into action," 2023. [Online]. Available: [https://www.deepmind.com/blog/rt](https://www.deepmind.com/blog/rt-2-new-model-translates-vision-and-language-into-action) [-2-new-model-translates-vision-and-language-into-action](https://www.deepmind.com/blog/rt-2-new-model-translates-vision-and-language-into-action)
- <span id="page-34-2"></span>[3] Y. Li, H. Wen, W. Wang, X. Li, Y. Yuan, G. Liu, J. Liu, W. Xu, X. Wang, Y. Sun *et al.*, "Personal LLM agents: Insights and survey about the capability, efficiency and security," *arXiv preprint arXiv:2401.05459*, 2024.
- [4] M. A. Rahman, "A survey on security and privacy of multimodal LLMs-connected healthcare perspective," in *Proc. IEEE Glob. Commun. Conf. Workshops (GC Wkshps)*, Kuala Lumpur, Malaysia, Dec. 2023, pp. 1807–1812.
- <span id="page-34-3"></span>[5] J. Sun, C. Wu, S. Mumtaz, J. Tao, M. Cao, M. Wang, and V. Frascolla, "An efficient privacy-aware split learning framework for satellite communications," *IEEE J. Sel. Areas Commun.*, pp. 1–11, early access 2024.
- <span id="page-34-4"></span>[6] Google, "Google introduces Gemini, the most capable and flexible AI model we've ever built," 2023. [Online]. Available: [https://store.goog](https://store.google.com/intl/en/ideas/articles/pixel-feature-drop-december-2023/) [le.com/intl/en/ideas/articles/pixel-feature-drop-december-2023/](https://store.google.com/intl/en/ideas/articles/pixel-feature-drop-december-2023/)
- <span id="page-34-5"></span>[7] Qualcomm, "Qualcomm works with Meta to enable on-device AI applications using Llama 2," 2023. [Online]. Available: [https:](https://www.qualcomm.com/news/releases/2023/07/qualcomm-works-with-meta-to-enable-on-device-ai-applications-usi) [//www.qualcomm.com/news/releases/2023/07/qualcomm-works-with](https://www.qualcomm.com/news/releases/2023/07/qualcomm-works-with-meta-to-enable-on-device-ai-applications-usi) [-meta-to-enable-on-device-ai-applications-usi](https://www.qualcomm.com/news/releases/2023/07/qualcomm-works-with-meta-to-enable-on-device-ai-applications-usi)
- <span id="page-34-6"></span>[8] Z. Liu, C. Zhao, F. Iandola, C. Lai, Y. Tian, I. Fedorov, Y. Xiong, E. Chang, Y. Shi, R. Krishnamoorthi *et al.*, "MobileLLM: Optimizing sub-billion parameter language models for on-device use cases," in *Proc. Int. Conf. Mach. Learn. (ICML)*, Vienna, Austria, Jul. 2024, pp. 1–24.
- <span id="page-34-7"></span>[9] Apple, "Introducing Apple's on-device and server foundation models," 2024. [Online]. Available: [https://machinelearning.apple.com/research](https://machinelearning.apple.com/research/introducing-apple-foundation-models) [/introducing-apple-foundation-models](https://machinelearning.apple.com/research/introducing-apple-foundation-models)
- <span id="page-34-10"></span>[10] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang, J. Zhang, Z. Dong *et al.*, "A survey of large language models," *arXiv preprint arXiv:2303.18223*, 2023.
- <span id="page-34-17"></span>[11] M. Xu, W. Yin, D. Cai, R. Yi, D. Xu, Q. Wang, B. Wu, Y. Zhao, C. Yang, S. Wang *et al.*, "A survey of resource-efficient LLM and multimodal foundation models," *arXiv preprint arXiv:2401.08092*, 2024.
- <span id="page-34-11"></span>[12] Z. Wan, X. Wang, C. Liu, S. Alam, Y. Zheng, Z. Qu, S. Yan, Y. Zhu, Q. Zhang, M. Chowdhury *et al.*, "Efficient large language models: A survey," *Trans. Mach. Learn. Res.*, 2024.
- <span id="page-34-18"></span>[13] M. Xu, H. Du, D. Niyato, J. Kang, Z. Xiong, S. Mao, Z. Han, A. Jamalipour, D. I. Kim, X. Shen, V. C. M. Leung, and H. V. Poor, "Unleashing the power of edge-cloud generative AI in mobile networks: A survey of AIGC services," *IEEE Commun. Surveys Tuts.*, vol. 26, no. 2, pp. 1127–1170, 2nd Quart. 2024.
- <span id="page-34-12"></span>[14] C. Kachris, "A survey on hardware accelerators for large language models," *arXiv preprint arXiv:2401.09890*, 2024.
- <span id="page-34-19"></span>[15] G. Bai, Z. Chai, C. Ling, S. Wang, J. Lu, N. Zhang, T. Shi, Z. Yu, M. Zhu, Y. Zhang *et al.*, "Beyond efficiency: A systematic survey of resource-efficient large language models," *arXiv preprint arXiv:2401.00625*, 2024.
- <span id="page-34-13"></span>[16] Z. Yuan, Y. Shang, Y. Zhou, Z. Dong, C. Xue, B. Wu, Z. Li, Q. Gu, Y. J. Lee, Y. Yan *et al.*, "LLM inference unveiled: Survey and roofline model insights," *arXiv preprint arXiv:2402.16363*, 2024.

- <span id="page-34-25"></span>[17] Y. Liu, H. He, T. Han, X. Zhang, M. Liu, J. Tian, Y. Zhang, J. Wang, X. Gao, T. Zhong *et al.*, "Understanding LLMs: A comprehensive overview from training to inference," *arXiv preprint arXiv:2401.02038*, 2024.
- <span id="page-34-14"></span>[18] Z. Han, C. Gao, J. Liu, S. Q. Zhang *et al.*, "Parameter-efficient finetuning for large models: A comprehensive survey," *arXiv preprint arXiv:2403.14608*, 2024.
- <span id="page-34-8"></span>[19] Y. Zhou, X. Lin, X. Zhang, M. Wang, G. Jiang, H. Lu, Y. Wu, K. Zhang, Z. Yang, K. Wang *et al.*, "On the opportunities of green computing: A survey," *arXiv preprint arXiv:2311.00447*, 2023.
- [20] A. A. Chien, L. Lin, H. Nguyen, V. Rao, T. Sharma, and R. Wijayawardana, "Reducing the carbon impact of generative AI inference (today and in 2035)," in *Proc. 2nd Workshop Sustain. Comput. Syst.*, Boston, MA, USA, Jul. 2023, pp. 1–7.
- <span id="page-34-9"></span>[21] Y.-C. Wang, J. Xue, C. Wei, and C.-C. J. Kuo, "An overview on generative AI at scale with edge-cloud computing," *IEEE Open J. Commun. Soc.*, pp. 2952–2971, Oct. 2023.
- <span id="page-34-15"></span>[22] X. Zhu, J. Li, Y. Liu, C. Ma, and W. Wang, "A survey on model compression for large language models," *arXiv preprint arXiv:2308.07633*, 2023.
- <span id="page-34-16"></span>[23] X. Miao, G. Oliaro, Z. Zhang, X. Cheng, H. Jin, T. Chen, and Z. Jia, "Towards efficient generative large language model serving: A survey from algorithms to systems," *arXiv preprint arXiv:2312.15234*, 2023.
- <span id="page-34-20"></span>[24] J. Fang, Y. He, F. R. Yu, J. Li, and V. C. Leung, "Large language models (LLMs) inference offloading and resource allocation in cloudedge networks: An active inference approach," in *Proc. IEEE Veh. Technol. Conf. (VTC)*, Hong Kong, Oct. 2023, pp. 1–5.
- <span id="page-34-21"></span>[25] M. Xu, D. Niyato, J. Kang, Z. Xiong, S. Mao, Z. Han, D. I. Kim, and K. B. Letaief, "When large language model agents meet 6G networks: Perception, grounding, and alignment," *IEEE Wireless Commun.*, pp. 1–9, early access 2024.
- <span id="page-34-22"></span>[26] Y. Shen, J. Shao, X. Zhang, Z. Lin, H. Pan, D. Li, J. Zhang, and K. B. Letaief, "Large language models empowered autonomous edge AI for connected intelligence," *IEEE Commun. Mag.*, vol. 62, no. 10, pp. 140–146, Oct. 2024.
- <span id="page-34-24"></span>[27] Y. Huang, H. Du, X. Zhang, D. Niyato, J. Kang, Z. Xiong, S. Wang, and T. Huang, "Large language models for networking: Applications, enabling techniques, and challenges," *IEEE Netw.*, pp. 1–8, early access 2024.
- <span id="page-34-23"></span>[28] H. Zhou, C. Hu, Y. Yuan, Y. Cui, Y. Jin, C. Chen, H. Wu, D. Yuan, L. Jiang, D. Wu *et al.*, "Large language model (LLM) for telecommunications: A comprehensive survey on principles, key techniques, and opportunities," *IEEE Commun. Surveys Tuts.*, pp. 1–54, early access 2024.
- <span id="page-34-28"></span>[29] L. Skorin-Kapov and M. Matijasevic, "Analysis of QoS requirements for e-health services and mapping to evolved packet system QoS classes," *International journal of telemedicine and applications*, vol. 2010, no. 1, p. 628086, Oct. 2010.
- <span id="page-34-29"></span>[30] 3GPP, "3rd generation partnership project; Technical specification group services and system aspects; Study on traffic characteristics and performance requirements for AI/ML model transfer in 5GS; (Release 18)," 3rd Generation Partnership Project (3GPP), Technical Specification (TS) 22.874, Dec. 2021, version 18.2.0.
- <span id="page-34-30"></span>[31] Nvidia, "Creating voice-based virtual assistants using NVIDIA riva and rasa," 2021. [Online]. Available: [https://developer.nvidia.com/blo](https://developer.nvidia.com/blog/creating-voice-based-virtual-assistants-using-nvidia-riva-and-rasa/) [g/creating-voice-based-virtual-assistants-using-nvidia-riva-and-rasa/](https://developer.nvidia.com/blog/creating-voice-based-virtual-assistants-using-nvidia-riva-and-rasa/)
- <span id="page-34-31"></span>[32] H. N. Qureshi, M. Manalastas, A. Ijaz, A. Imran, Y. Liu, and M. O. Al Kalaa, "Communication requirements in 5G-enabled healthcare applications: Review and considerations," in *Proc. Healthcare*, vol. 10, no. 2, Feb. 2022, p. 293.
- <span id="page-34-32"></span>[33] 3GPP, "3rd generation partnership project; Technical specification group services and system aspects; Service requirements for the 5G system; Stage 1 (Release 19)," 3rd Generation Partnership Project (3GPP), Technical Specification (TS) 22.261, Dec. 2024, version 19.6.0.
- <span id="page-34-26"></span>[34] K. Singhal, T. Tu, J. Gottweis, R. Sayres, E. Wulczyn, L. Hou, K. Clark, S. Pfohl, H. Cole-Lewis, D. Neal *et al.*, "Towards expertlevel medical question answering with large language models," *arXiv preprint arXiv:2305.09617*, 2023.
- <span id="page-34-27"></span>[35] J. L. Prieto, "New fitbit study explores metabolic health," 2024. [Online]. Available: [https://blog.google/products/fitbit/new-quest-fitbi](https://blog.google/products/fitbit/new-quest-fitbit-study-metabolic-health/) [t-study-metabolic-health/](https://blog.google/products/fitbit/new-quest-fitbit-study-metabolic-health/)
- <span id="page-34-33"></span>[36] K. He, R. Mao, Q. Lin, Y. Ruan, X. Lan, M. Feng, and E. Cambria, "A survey of large language models for healthcare: From data, technology, and applications to accountability and ethics," *arXiv preprint arXiv:2310.05694*, 2023.
- <span id="page-34-34"></span>[37] X. Yu, S. Park, D. Kim, E. Kim, J. Kim, W. Kim, Y. An, and S. Xiong, "A practical wearable fall detection system based on tiny convolutional

- neural networks," *Biomedical Signal Processing and Control*, vol. 86, p. 105325, Aug. 2023.
- <span id="page-35-0"></span>[38] GDPR, "Art. 9 GDPR: Processing of special categories of personal data," 2015. [Online]. Available:<https://gdpr-info.eu/art-9-gdpr/>
- <span id="page-35-1"></span>[39] M. Satyanarayanan, P. Bahl, R. Caceres, and N. Davies, "The case for VM-based cloudlets in mobile computing," *IEEE Pervasive Comput.*, vol. 8, no. 4, pp. 14–23, Oct.-Dec. 2009.
- <span id="page-35-2"></span>[40] J. Spataro, "Introducing Microsoft 365 Copilot – your copilot for work," 2023. [Online]. Available: [https://blogs.microsoft.com/blog/2](https://blogs.microsoft.com/blog/2023/03/16/introducing-microsoft-365-copilot-your-copilot-for-work/) [023/03/16/introducing-microsoft-365-copilot-your-copilot-for-work/](https://blogs.microsoft.com/blog/2023/03/16/introducing-microsoft-365-copilot-your-copilot-for-work/)
- <span id="page-35-3"></span>[41] S. Hu, Z. Fang, Z. Fang, X. Chen, and Y. Fang, "AgentsCoDriver: Large language model empowered collaborative driving with lifelong learning," *arXiv preprint arXiv:2404.06345*, 2024.
- <span id="page-35-4"></span>[42] Ghost, "Ghost autonomy announces investment from OpenAI startup fund to bring multi-modal LLMs to autonomous driving," 2023. [Online]. Available: [https://www.ghostautonomy.com/blog/ghost-auton](https://www.ghostautonomy.com/blog/ghost-autonomy-announces-investment-from-openai-startup-fund-to-bring-multi-modal-llms-to-autonomous-driving) [omy-announces-investment-from-openai-startup-fund-to-bring-multi](https://www.ghostautonomy.com/blog/ghost-autonomy-announces-investment-from-openai-startup-fund-to-bring-multi-modal-llms-to-autonomous-driving) [-modal-llms-to-autonomous-driving](https://www.ghostautonomy.com/blog/ghost-autonomy-announces-investment-from-openai-startup-fund-to-bring-multi-modal-llms-to-autonomous-driving)
- <span id="page-35-5"></span>[43] T. Feed, "Chinese automaker geely to release auto industry's first large language model," 2023. [Online]. Available: [https:](https://technode.com/2023/08/01/chinese-automaker-geely-to-release-auto-industrys-first-large-language-model/) [//technode.com/2023/08/01/chinese-automaker-geely-to-release-auto](https://technode.com/2023/08/01/chinese-automaker-geely-to-release-auto-industrys-first-large-language-model/)[industrys-first-large-language-model/](https://technode.com/2023/08/01/chinese-automaker-geely-to-release-auto-industrys-first-large-language-model/)
- <span id="page-35-6"></span>[44] Z. Fang, S. Hu, H. An, Y. Zhang, J. Wang, H. Cao, X. Chen, and Y. Fang, "PACP: Priority-aware collaborative perception for connected and autonomous vehicles," *IEEE Trans. Mobile Comput.*, pp. 1–15, early access 2024.
- <span id="page-35-7"></span>[45] X. Chen, Y. Deng, H. Ding, G. Qu, H. Zhang, P. Li, and Y. Fang, "Vehicle as a service (VaaS): Leverage vehicles to build service networks and capabilities for smart cities," *IEEE Commun. Surveys Tuts.*, vol. 26, no. 3, pp. 2048–2081, 3rd Quart. 2024.
- <span id="page-35-8"></span>[46] S. Hu, Z. Fang, Y. Deng, X. Chen, and Y. Fang, "Collaborative perception for connected and autonomous driving: Challenges, possible solutions and opportunities," *arXiv preprint arXiv:2401.01544*, 2024.
- <span id="page-35-9"></span>[47] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, "Attention is all you need," in *Proc. Adv. Neural Inform. Process. Syst. (NeurIPS)*, Long Beach, CA, USA, Dec. 2017, pp. 5998–6008.
- <span id="page-35-10"></span>[48] G. Soyalp, A. Alar, K. Ozkanli, and B. Yildiz, "Improving text classification with transformer," in *Proc. IEEE Int. Conf. Comput. Sci. Eng. (UBMK)*, Sep. 2021, pp. 707–712.
- <span id="page-35-11"></span>[49] M. Ott, S. Edunov, D. Grangier, and M. Auli, "Scaling neural machine translation," in *Proc. ACL Conf. Mach. Transl. (WMT)*, Brussels, Belgium, Oct. 2018, pp. 1–9.
- <span id="page-35-12"></span>[50] Y. Guan, Z. Li, Z. Lin, Y. Zhu, J. Leng, and M. Guo, "Block-skim: Efficient question answering for transformer," in *Proc. AAAI Conf. Artif. Intell. (AAAI)*, vol. 36, no. 10, Vancouver, BC, Canada, Feb. 2022, pp. 10 710–10 719.
- <span id="page-35-13"></span>[51] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, "BERT: Pretraining of deep bidirectional transformers for language understanding," in *Proc. Conf. North Amer. Chapter Assoc. Comput. Linguist.: Human Lang. Technol., Vol. 1*, Minneapolis, MN, USA, Jun. 2019, pp. 4171– 4186.
- <span id="page-35-14"></span>[52] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly *et al.*, "An image is worth 16x16 words: Transformers for image recognition at scale," in *Proc. Int. Conf. Learn. Represent. (ICLR)*, May 2021, pp. 1–21.
- <span id="page-35-15"></span>[53] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko, "End-to-end object detection with transformers," in *Proc. ECVA Eur. Conf. Comput. Vis. (ECCV)*, Aug. 2020, pp. 213– 229.
- <span id="page-35-16"></span>[54] L. Ye, M. Rochan, Z. Liu, and Y. Wang, "Cross-modal self-attention network for referring image segmentation," in *Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR)*, Long Beach, CA, USA, Jun. 2019, pp. 10 502–10 511.
- <span id="page-35-17"></span>[55] Y. Qiu, H. Chen, X. Dong, Z. Lin, I. Y. Liao, M. Tistarelli, and Z. Jin, "IFViT: Interpretable fixed-length representation for fingerprint matching via vision transformer," *arXiv preprint arXiv:2404.08237*, 2024.
- <span id="page-35-18"></span>[56] P. Gage, "A new algorithm for data compression," *C Users Journal*, vol. 12, pp. 23–38, Feb. 1994.
- <span id="page-35-19"></span>[57] R. Sennrich, B. Haddow, and A. Birch, "Neural machine translation of rare words with subword units," in *Proc. Annu. Meet. Assoc. Comput. Linguistics*, Aug. 2016, pp. 1715–1725.
- <span id="page-35-20"></span>[58] M. Schuster and K. Nakajima, "Japanese and korean voice search," in *Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP)*, Kyoto, Japan, Mar. 2012, pp. 5149–5152.

- <span id="page-35-21"></span>[59] S. Chaudhari, V. Mithal, G. Polatkan, and R. Ramanath, "An attentive survey of attention models," *ACM Trans. Intell. Syst. Technol.*, vol. 12, no. 5, pp. 1–32, Oct. 2021.
- <span id="page-35-22"></span>[60] A. de Santana Correia and E. L. Colombini, "Attention, please! A survey of neural attention models in deep learning," *Artif. Intell. Rev.*, vol. 55, no. 8, pp. 6037–6124, Mar. 2022.
- <span id="page-35-23"></span>[61] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan, "Show and tell: A neural image caption generator," in *Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit.*, Boston, MA, USA, Jun. 2015, pp. 3156–3164.
- <span id="page-35-24"></span>[62] I. Goodfellow, Y. Bengio, and A. Courville, *Deep learning*. Cambridge, MA, USA: MIT press, 2016.
- [63] Y. LeCun, Y. Bengio, and G. Hinton, "Deep learning," *Nature*, vol. 521, no. 7553, pp. 436–444, May 2015.
- <span id="page-35-25"></span>[64] A. Graves, *Long Short-Term Memory*. Berlin, Heidelberg: Springer Berlin Heidelberg, 2012, pp. 37–45.
- <span id="page-35-26"></span>[65] S. Minaee, T. Mikolov, N. Nikzad, M. Chenaghlu, R. Socher, X. Amatriain, and J. Gao, "Large language models: A survey," *arXiv preprint arXiv:2402.06196*, 2024.
- <span id="page-35-27"></span>[66] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat *et al.*, "GPT-4 technical report," *arXiv preprint arXiv:2303.08774*, 2023.
- <span id="page-35-28"></span>[67] R. Dale, "GPT-3: What's it good for?" *Nat. Lang. Eng.*, vol. 27, no. 1, pp. 113–118, Jan. 2021.
- <span id="page-35-29"></span>[68] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and H. Jegou, "Training data-efficient image transformers & distillation ´ through attention," in *Proc. Int. Conf. Mach. Learn.*, Jul. 2021, pp. 10 347–10 357.
- <span id="page-35-30"></span>[69] Z. Lin, X. Hu, Y. Zhang, Z. Chen, Z. Fang, X. Chen, A. Li, P. Vepakomma, and Y. Gao, "SplitLoRA: A split parameter-efficient fine-tuning framework for large language models," *arXiv preprint arXiv:2407.00952*, 2024.
- <span id="page-35-31"></span>[70] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut, "ALBERT: A lite BERT for self-supervised learning of language representations," in *Proc. Int. Conf. Learn. Represent.*, Apr. 2020.
- <span id="page-35-32"></span>[71] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu, "Exploring the limits of transfer learning with a unified text-to-text transformer," *J. Mach. Learn. Res.*, vol. 21, no. 140, pp. 1–67, Jun. 2020.
- <span id="page-35-33"></span>[72] K. Sanderson, "GPT-4 is here: What scientists think," *Nature*, vol. 615, no. 7954, p. 773, Mar. 2023.
- <span id="page-35-36"></span>[73] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, "Improving language understanding by generative pre-training," 2018. [Online]. Available: [https://cdn.openai.com/research-covers/language-unsuperv](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf) ised/language [understanding](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf) paper.pdf
- <span id="page-35-34"></span>[74] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov, "RoBERTa: A robustly optimized bert pretraining approach," *arXiv preprint arXiv:1907.11692*, 2019.
- <span id="page-35-35"></span>[75] Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. R. Salakhutdinov, and Q. V. Le, "XLNet: Generalized autoregressive pretraining for language understanding," in *Proc. Adv. Neural Inform. Process. Syst. (NeurIPS)*, vol. 32, Red Hook, NY, USA, Dec. 2019, pp. 5753–5763.
- <span id="page-35-37"></span>[76] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever *et al.*, "Language models are unsupervised multitask learners," *OpenAI blog*, vol. 1, no. 8, pp. 1–24, 2019.
- <span id="page-35-38"></span>[77] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell *et al.*, "Language models are few-shot learners," in *Proc. Adv. Neural Inform. Process. Syst.*, vol. 33, Dec. 2020, pp. 1877–1901.
- <span id="page-35-39"></span>[78] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann *et al.*, "PaLM: Scaling language modeling with pathways," *J. Mach. Learn. Res.*, vol. 24, no. 1, pp. 11 324–11 436, Mar. 2024.
- <span id="page-35-40"></span>[79] G. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth *et al.*, "Gemini: A family of highly capable multimodal models," *arXiv preprint arXiv:2312.11805*, 2023.
- <span id="page-35-41"></span>[80] H. Touvron, L. Martin, K. Stone, and et al, "Llama 2: Open foundation and fine-tuned chat models," *arXiv preprint arXiv:2307.09288*, 2023.
- <span id="page-35-42"></span>[81] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov, and L. Zettlemoyer, "BART: Denoising sequence-tosequence pre-training for natural language generation, translation, and comprehension," in *Proc. Annu. Meeting Assoc. Comput. Linguistics*, Jul. 2020, pp. 7871–7880.
- <span id="page-35-43"></span>[82] L. Xue, N. Constant, A. Roberts, M. Kale, R. Al-Rfou, A. Siddhant, A. Barua, and C. Raffel, "mT5: A massively multilingual pre-trained text-to-text transformer," in *Proc. Conf. North Amer. Chapter Assoc. Comput. Linguistics: Human Lang. Technol.*, Jun. 2021, pp. 483–498.

- <span id="page-36-0"></span>[83] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdinov, and A. Mohamed, "HuBERT: Self-supervised speech representation learning by masked prediction of hidden units," *IEEE/ACM Trans. Audio, Speech and Lang. Proc.*, vol. 29, p. 3451–3460, Oct. 2021.
- <span id="page-36-1"></span>[84] Claude.ai, "How does Claude 3 AI work?" 2024. [Online]. Available: <https://claude3.pro/how-does-claude-3-ai-work/>
- <span id="page-36-2"></span>[85] Y. Chang, X. Wang, J. Wang, Y. Wu, L. Yang, K. Zhu, H. Chen, X. Yi, C. Wang, Y. Wang *et al.*, "A survey on evaluation of large language models," *ACM Trans. Intell. Syst. Technol.*, vol. 15, no. 3, pp. 1–45, Mar. 2024.
- <span id="page-36-3"></span>[86] S. Xu, C. K. Thomas, O. Hashash, N. Muralidhar, W. Saad, and N. Ramakrishnan, "Large multi-modal models (LMMs) as universal foundation models for AI-native wireless systems," *IEEE Netw.*, vol. 38, no. 5, pp. 10–20, Sep. 2024.
- <span id="page-36-4"></span>[87] W. Yang, Z. Fang, T. Zhang, S. Wu, and C. Lu, "Modal-aware bias constrained contrastive learning for multimodal recommendation," in *Proc. ACM Int. Conf. Multimedia*, Oct. 2023, pp. 6369––6378.
- <span id="page-36-5"></span>[88] J. Wu, W. Gan, Z. Chen, S. Wan, and S. Y. Philip, "Multimodal large language models: A survey," in *Proc. 2023 IEEE Int. Conf. Big Data (BigData)*, Sorrento, Italy, Dec. 2023, pp. 2247–2256.
- <span id="page-36-6"></span>[89] C. Cui, Y. Ma, X. Cao, W. Ye, Y. Zhou, K. Liang, J. Chen, J. Lu, Z. Yang, K.-D. Liao *et al.*, "A survey on multimodal large language models for autonomous driving," in *Proc. IEEE/CVF Winter Conf. Appl. Comput. Vis.*, Waikoloa, HI, USA, Jan. 2024, pp. 958–979.
- <span id="page-36-7"></span>[90] Y. Huo, M. Zhang, G. Liu, H. Lu, Y. Gao, G. Yang, J. Wen, H. Zhang, B. Xu, W. Zheng *et al.*, "WenLan: Bridging vision and language by large-scale multi-modal pre-training," *arXiv preprint arXiv:2103.06561*, 2021.
- <span id="page-36-8"></span>[91] D. Qi, L. Su, J. Song, E. Cui, T. Bharti, and A. Sacheti, "ImageBERT: Cross-modal pre-training with large-scale weak-supervised image-text data," *arXiv preprint arXiv:2001.07966*, 2020.
- <span id="page-36-9"></span>[92] B. Peng, C. Li, P. He, M. Galley, and J. Gao, "Instruction tuning with GPT-4," *arXiv preprint arXiv:2304.03277*, 2023.
- <span id="page-36-10"></span>[93] S. Zhang, L. Dong, X. Li, S. Zhang, X. Sun, S. Wang, J. Li, R. Hu, T. Zhang, F. Wu *et al.*, "Instruction tuning for large language models: A survey," *arXiv preprint arXiv:2308.10792*, 2023.
- <span id="page-36-11"></span>[94] Eclipse Foundation, "Xtext language engineering for everyone," 2023. [Online]. Available:<https://eclipse.org/Xtext>
- <span id="page-36-12"></span>[95] P. Zhou, L. Wang, Z. Liu, Y. Hao, P. Hui, S. Tarkoma, and J. Kangasharju, "A survey on generative AI and LLM for video generation, understanding, and streaming," *arXiv preprint arXiv:2404.16038*, 2024.
- <span id="page-36-13"></span>[96] C. Jeong, "A study on the implementation of generative AI services using an enterprise data-based llm application architecture," *arXiv preprint arXiv:2309.01105*, 2023.
- <span id="page-36-14"></span>[97] Dirox, "Generative AI vs interactive AI: Understanding the differences," 2023. [Online]. Available: [https://dirox.com/post/g](https://dirox.com/post/generative-ai-vs-interactive-ai-understanding-the-differences) [enerative-ai-vs-interactive-ai-understanding-the-differences](https://dirox.com/post/generative-ai-vs-interactive-ai-understanding-the-differences)
- <span id="page-36-15"></span>[98] H. Du, R. Zhang, D. Niyato, J. Kang, Z. Xiong, and D. I. Kim, "Reinforcement learning with large language models (LLMs) interaction for network services," in *Proc. Int. Conf. Comput. Netw. Commun. (ICNC)*, Feb. 2024, pp. 799–803.
- <span id="page-36-16"></span>[99] H. Du, R. Zhang, D. Niyato, J. Kang, Z. Xiong, S. Cui, X. Shen, and D. I. Kim, "User-centric interactive AI for distributed diffusion model-based AI-generated content," *arXiv preprint arXiv:2311.11094*, 2023.
- <span id="page-36-17"></span>[100] R. Zhang, H. Du, Y. Liu, D. Niyato, J. Kang, Z. Xiong, A. Jamalipour, and D. I. Kim, "Interactive generative AI agents for satellite networks through a mixture of experts transmission," *arXiv preprint arXiv:2404.09134*, 2024.
- <span id="page-36-18"></span>[101] R. Zhang, H. Du, Y. Liu, D. Niyato, J. Kang, S. Sun, X. Shen, and H. V. Poor, "Interactive AI with retrieval-augmented generation for next generation networking," *IEEE Netw.*, pp. 1–11, early access 2024.
- <span id="page-36-19"></span>[102] R. Merritt, "What is retrieval-augmented generation, aka RAG?" 2023. [Online]. Available: [https://blogs.nvidia.com/blog/what-is-retrieval-au](https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/) [gmented-generation/](https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/)
- <span id="page-36-20"></span>[103] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. Kuttler, M. Lewis, W.-t. Yih, T. Rockt ¨ aschel ¨ *et al.*, "Retrievalaugmented generation for knowledge-intensive NLP tasks," in *Proc. Adv. Neural Inform. Process. Syst. (NeurIPS)*, vol. 33, Dec. 2020, pp. 9459–9474.
- <span id="page-36-21"></span>[104] Google Cloud, "What is retrieval-augmented generation (RAG)?" 2023. [Online]. Available: [https://cloud.google.com/use-cases/retrieval](https://cloud.google.com/use-cases/retrieval-augmented-generation?hl=en) [-augmented-generation?hl=en](https://cloud.google.com/use-cases/retrieval-augmented-generation?hl=en)
- <span id="page-36-22"></span>[105] IBM, "What is retrieval-augmented generation?" 2023. [Online]. Available: [https://research.ibm.com/blog/retrieval-augmented-generati](https://research.ibm.com/blog/retrieval-augmented-generation-RAG) [on-RAG](https://research.ibm.com/blog/retrieval-augmented-generation-RAG)

- <span id="page-36-23"></span>[106] C. Wu, K. He, J. Chen, Z. Zhao, and R. Du, "Liveness is not enough: Enhancing fingerprint authentication with behavioral biometrics to defeat puppet attacks," in *Proc. USENIX Secur. Symp.*, Aug. 2020, pp. 2219–2236.
- [107] H. Yuan, Z. Chen, Z. Lin, J. Peng, Z. Fang, Y. Zhong, Z. Song, X. Wang, and Y. Gao, "Graph learning for multi-satellite based spectrum sensing," in *Proc. IEEE Int. Conf. Commun. Technol. (ICCT)*, WuXi, China, Oct. 2023, pp. 1112–1116.
- <span id="page-36-24"></span>[108] C. Wu, K. He, J. Chen, Z. Zhao, and R. Du, "Toward robust detection of puppet attacks via characterizing fingertip-touch behaviors," *IEEE Trans. Dependable Secure. Comput.*, vol. 19, no. 6, pp. 4002–4018, Nov.-Dec. 2021.
- <span id="page-36-25"></span>[109] W. Wang, J. Xie, C. Hu, H. Zou, J. Fan, W. Tong, Y. Wen, S. Wu, H. Deng, Z. Li *et al.*, "DriveMLM: Aligning multi-modal large language models with behavioral planning states for autonomous driving," *arXiv preprint arXiv:2312.09245*, 2023.
- <span id="page-36-26"></span>[110] Y. Zhu, Y. Liu, F. Stahlberg, S. Kumar, Y.-h. Chen, L. Luo, L. Shu, R. Liu, J. Chen, and L. Meng, "Towards an on-device agent for text rewriting," in *Proc. North Am. Chapter Assoc. Comput. Linguist. (NAACL)*, Mexico City, Mexico, Jun. 2024, pp. 2535–2552.
- <span id="page-36-27"></span>[111] J. Wang, C. Jiang, H. Zhang, Y. Ren, K.-C. Chen, and L. Hanzo, "Thirty years of machine learning: The road to pareto-optimal wireless networks," *IEEE Commun. Surveys Tuts.*, vol. 22, no. 3, pp. 1472–1514, 3rd Quart. 2020.
- [112] C. Wu, K. He, J. Chen, R. Du, and Y. Xiang, "CaIAuth: Context-aware implicit authentication when the screen is awake," *IEEE Internet Things J.*, vol. 7, no. 12, pp. 11 420–11 430, Dec. 2020.
- [113] H. Yuan, Z. Chen, Z. Lin, J. Peng, Z. Fang, Y. Zhong, Z. Song, and Y. Gao, "SATSense: Multi-satellite collaborative framework for spectrum sensing," *arXiv preprint arXiv:2405.15542*, 2024.
- <span id="page-36-28"></span>[114] J. Peng, Z. Chen, Z. Lin, H. Yuan, Z. Fang, L. Bao, Z. Song, Y. Li, J. Ren, and Y. Gao, "Sums: Sniffing unknown multiband signals under low sampling rates," *arXiv preprint arXiv:2405.15705*, 2024.
- <span id="page-36-29"></span>[115] Z. Lin, S. Bi, and Y.-J. A. Zhang, "Optimizing AI service placement and resource allocation in mobile edge intelligence systems," *IEEE Trans. Wireless Commun.*, vol. 20, no. 11, pp. 7257–7271, Nov. 2021.
- <span id="page-36-30"></span>[116] W. Yang, Z. Q. Liew, W. Y. B. Lim, Z. Xiong, D. Niyato, X. Chi, X. Cao, and K. B. Letaief, "Semantic communication meets edge intelligence," *IEEE Wireless Commun.*, vol. 29, no. 5, pp. 28–35, Dec. 2022.
- <span id="page-36-31"></span>[117] S. Deng, H. Zhao, W. Fang, J. Yin, S. Dustdar, and A. Y. Zomaya, "Edge intelligence: The confluence of edge computing and artificial intelligence," *IEEE Internet Things J.*, vol. 7, no. 8, pp. 7457–7469, Aug. 2020.
- <span id="page-36-33"></span>[118] D. Xu, T. Li, Y. Li, X. Su, S. Tarkoma, T. Jiang, J. Crowcroft, and P. Hui, "Edge intelligence: Empowering intelligence to the edge of network," *Proc. IEEE*, vol. 109, no. 11, pp. 1778–1837, Nov. 2021.
- <span id="page-36-32"></span>[119] X. Zhang, Y. Wang, S. Lu, L. Liu, W. Shi *et al.*, "OpenEI: An open framework for edge intelligence," in *Proc. IEEE Int. Conf. Distrib. Comput. Syst. (ICDCS)*, Dallas, TX, USA, Jul. 2019, pp. 1840–1851.
- <span id="page-36-34"></span>[120] Z. Zhou, X. Chen, E. Li, L. Zeng, K. Luo, and J. Zhang, "Edge intelligence: Paving the last mile of artificial intelligence with edge computing," *Proc. IEEE*, vol. 107, no. 8, pp. 1738–1762, Aug. 2019.
- <span id="page-36-35"></span>[121] HuaweiTech, "ITU-R WP5D completed the recommendation framework for IMT-2030 (global 6G vision)," 2023. [Online]. Available: [https://www.huawei.com/en/huaweitech/future-technologies](https://www.huawei.com/en/huaweitech/future-technologies/itu-r-wp5d-completed-recommendation-framework-imt-2030) [/itu-r-wp5d-completed-recommendation-framework-imt-2030](https://www.huawei.com/en/huaweitech/future-technologies/itu-r-wp5d-completed-recommendation-framework-imt-2030)
- <span id="page-36-36"></span>[122] ITU, "Architectural framework for machine learning in future networks including IMT-2020," International Telecommunication Union (ITU), ITU-T Recommendation ITU-T Y.3172, Jun. 2019.
- <span id="page-36-37"></span>[123] Q. Chen, Z. Guo, W. Meng, S. Han, C. Li, and T. Q. Quek, "A survey on resource management in joint communication and computingembedded SAGIN," *IEEE Commun. Surveys Tuts.*, pp. 1–44, early access 2024.
- <span id="page-36-38"></span>[124] G. Ananthanarayanan, P. Bahl, P. Bod´ık, K. Chintalapudi, M. Philipose, L. Ravindranath, and S. Sinha, "Real-time video analytics: The killer app for edge computing," *Computer*, vol. 50, no. 10, pp. 58–67, Oct. 2017.
- <span id="page-36-39"></span>[125] J. Cao, L. Xu, R. Abdallah, and W. Shi, "EdgeOS H: A home operating system for internet of everything," in *Proc. IEEE Int. Conf. Distrib. Comput. Syst. (ICDCS)*, Atlanta, GA, USA, Jun. 2017, pp. 1756–1764.
- <span id="page-36-40"></span>[126] L. Li, K. Ota, and M. Dong, "Deep learning for smart industry: Efficient manufacture inspection system with fog computing," *IEEE Trans. Ind. Informat.*, vol. 14, no. 10, pp. 4665–4673, Oct. 2018.
- <span id="page-36-41"></span>[127] X. Ye, "Calflops: A FLOPs and params calculate tool for neural networks," 2023. [Online]. Available: [https://github.com/MrYxJ/calcu](https://github.com/MrYxJ/calculate-flops.pytorch) [late-flops.pytorch](https://github.com/MrYxJ/calculate-flops.pytorch)

- <span id="page-37-0"></span>[128] S. Malladi, T. Gao, E. Nichani, A. Damian, J. D. Lee, D. Chen, and S. Arora, "Fine-tuning language models with just forward passes," in *Proc. Adv. Neural Inform. Process. Syst. (NeurIPS)*, New Orleans, LA, USA, Dec. 2023.
- <span id="page-37-1"></span>[129] D. Xu, W. Yin, X. Jin, Y. Zhang, S. Wei, M. Xu, and X. Liu, "LLMCad: Fast and scalable on-device large language model inference," *arXiv preprint arXiv:2309.04255*, 2023.
- <span id="page-37-5"></span>[130] W. Cheng, W. Zhang, H. Shen, Y. Cai, X. He, and K. Lv, "Optimize weight rounding via signed gradient descent for the quantization of LLMs," *arXiv preprint arXiv:2309.05516*, 2023.
- <span id="page-37-2"></span>[131] J. Lin, J. Tang, H. Tang, S. Yang, W.-M. Chen, W.-C. Wang, G. Xiao, X. Dang, C. Gan, and S. Han, "AWQ: Activation-aware weight quantization for LLM compression and acceleration," in *Proc. Mach. Learn. Syst.*, vol. 6, Santa Clara, CA, USA, May 2024, pp. 87–100.
- <span id="page-37-9"></span>[132] X. Ma, G. Fang, and X. Wang, "LLM-Pruner: On the structural pruning of large language models," in *Proc. Adv. Neural Inform. Process. Syst. (NeurIPS)*, New Orleans, LA, USA, Dec. 2023, pp. 21 702–21 720.
- <span id="page-37-11"></span>[133] E. Frantar and D. Alistarh, "SparseGPT: Massive language models can be accurately pruned in one-shot," in *Proc. Int. Conf. Mach. Learn. (ICML)*, Honolulu, HI, USA, Jul. 2023, pp. 10 323–10 337.
- <span id="page-37-15"></span>[134] Y. Gu, L. Dong, F. Wei, and M. Huang, "MiniLLM: Knowledge distillation of large language models," in *Proc. Int. Conf. Learn. Represent. (ICLR)*, Vienna Austria, May 2024, pp. 1–24.
- <span id="page-37-16"></span>[135] M. Wu, A. Waheed, C. Zhang, M. Abdul-Mageed, and A. F. Aji, "LaMini-LM: A diverse herd of distilled models from large-scale instructions," in *Proc. Conf. Eur. Chapter Assoc. Comput. Linguist. (EACL)*, St. Julian's, Malta, Mar. 2024, pp. 944–964.
- <span id="page-37-17"></span>[136] Y. Leviathan, M. Kalman, and Y. Matias, "Fast inference from transformers via speculative decoding," in *Proc. Int. Conf. Mach. Learn. (ICML)*, Honolulu, HI, USA, Jul. 2023, pp. 19 274–19 286.
- <span id="page-37-18"></span>[137] L. Soldaini and A. Moschitti, "The cascade transformer: An application for efficient answer sentence selection," in *Proc. Annu. Meet. Assoc. Comput. Linguist. (ACL)*, Jul. 2020, pp. 5697–5708.
- <span id="page-37-19"></span>[138] T. Schuster, A. Fisch, J. Gupta, M. Dehghani, D. Bahri, V. Tran, Y. Tay, and D. Metzler, "Confident adaptive language modeling," in *Proc. Adv. Neural Inform. Process. Syst. (NeurIPS)*, New Orleans, LA, USA, Nov. 2022, pp. 17 456–17 472.
- <span id="page-37-20"></span>[139] R. Yi, L. Guo, S. Wei, A. Zhou, S. Wang, and M. Xu, "EdgeMoE: Fast on-device inference of moe-based large language models," *arXiv preprint arXiv:2308.14352*, 2023.
- <span id="page-37-21"></span>[140] Z. Liu, J. Wang, T. Dao, T. Zhou, B. Yuan, Z. Song, A. Shrivastava, C. Zhang, Y. Tian, C. Re *et al.*, "Deja Vu: Contextual sparsity for efficient LLMs at inference time," in *Proc. Int. Conf. Mach. Learn. (ICML)*, Honolulu, HI, USA, Jul. 2023, pp. 22 137–22 176.
- <span id="page-37-22"></span>[141] T. Cai, Y. Li, Z. Geng, H. Peng, J. D. Lee, D. Chen, and T. Dao, "Medusa: Simple LLM inference acceleration framework with multiple decoding heads," in *Proc. Int. Conf. Mach. Learn. (ICML)*, Vienna, Austria, 2024, pp. 1–27.
- <span id="page-37-23"></span>[142] Y. Fu, P. Bailis, I. Stoica, and H. Zhang, "Break the sequential dependency of LLM inference using lookahead decoding," in *Proc. Int. Conf. Mach. Learn. (ICML)*, Vienna, Austria, Jul. 2024, pp. 1–20.
- <span id="page-37-24"></span>[143] H. Lee, G. Park, Y. Lee, J. Kim, W. Jeong, M. Jeon, and S. J. Hwang, "HiP attention: Sparse sub-quadratic attention with hierarchical attention pruning," *arXiv preprint arXiv:2406.09827*, 2024.
- <span id="page-37-25"></span>[144] Z. Liu, J. Yuan, H. Jin, S. Zhong, Z. Xu, V. Braverman, B. Chen, and X. Hu, "KIVI: A tuning-free asymmetric 2bit quantization for KV cache," in *Proc. Int. Conf. Mach. Learn. (ICML)*, Vienna, Austria, Jul. 2024, pp. 32 332–32 344.
- <span id="page-37-26"></span>[145] A. Liu, J. Liu, Z. Pan, Y. He, G. Haffari, and B. Zhuang, "MiniCache: KV cache compression in depth dimension for large language models," *arXiv preprint arXiv:2405.14366*, 2024.
- <span id="page-37-27"></span>[146] Z. Liu, A. Desai, F. Liao, W. Wang, V. Xie, Z. Xu, A. Kyrillidis, and A. Shrivastava, "Scissorhands: Exploiting the persistence of importance hypothesis for LLM KV cache compression at test time," in *Proc. Adv. Neural Inform. Process. Syst. (NeurIPS)*, vol. 36. New Orleans, LA, USA: Curran Associates, Inc., Dec. 2023, pp. 52 342–52 364.
- <span id="page-37-28"></span>[147] Y. Zhang, B. Gao, T. Liu, K. Lu, W. Xiong, Y. Dong, B. Chang, J. Hu, W. Xiao *et al.*, "PyramidKV: Dynamic KV cache compression based on pyramidal information funneling," *arXiv preprint arXiv:2406.02069*, 2024.
- <span id="page-37-29"></span>[148] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe, A. Gesmundo, M. Attariyan, and S. Gelly, "Parameter-efficient transfer learning for NLP," in *Proc. Int. Conf. Mach. Learn. (ICML)*, Long Beach, CA, USA, Jun. 2019, pp. 2790–2799.
- <span id="page-37-30"></span>[149] B. Lester, R. Al-Rfou, and N. Constant, "The power of scale for parameter-efficient prompt tuning," in *Proc. Conf. Empir. Methods*

- *Nat. Lang. Process. (EMNLP)*, Online and Punta Cana, Dominican Republic, Nov. 2021, pp. 3045–3059.
- <span id="page-37-31"></span>[150] X. L. Li and P. Liang, "Prefix-tuning: Optimizing continuous prompts for generation," in *Proc. 59th Annu. Meet. Assoc. Comput. Linguist. and 11th Int. Joint Conf. Nat. Lang. Process.*, Aug. 2021, pp. 4582– 4597.
- <span id="page-37-32"></span>[151] Z. Fu, H. Yang, A. M.-C. So, W. Lam, L. Bing, and N. Collier, "On the effectiveness of parameter-efficient fine-tuning," in *Proc. AAAI Conf. Artif. Intell. (AAAI)*, Washington, DC, USA, Feb. 2023, pp. 12 799– 12 807.
- <span id="page-37-33"></span>[152] D. Guo, A. M. Rush, and Y. Kim, "Parameter-efficient transfer learning with diff pruning," in *Proc. 59th Annu. Meet. Assoc. Comput. Linguist. and 11th Int. Joint Conf. Nat. Lang. Process*, Aug. 2021, pp. 4884– 4896.
- <span id="page-37-10"></span>[153] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen, "LoRA: Low-rank adaptation of large language models," in *Proc. Int. Conf. Learn. Represent. (ICLR)*, Apr. 2022, pp. 1–13.
- <span id="page-37-3"></span>[154] Z. Liu, B. Oguz, C. Zhao, E. Chang, P. Stock, Y. Mehdad, Y. Shi, R. Krishnamoorthi, and V. Chandra, "LLM-QAT: Data-free quantization aware training for large language models," *arXiv preprint arXiv:2305.17888*, 2023.
- <span id="page-37-4"></span>[155] W. Shao, M. Chen, Z. Zhang, P. Xu, L. Zhao, Z. Li, K. Zhang, P. Gao, Y. Qiao, and P. Luo, "OmniQuant: Omnidirectionally calibrated quantization for large language models," in *Proc. Int. Conf. Learn. Represent. (ICLR)*, Vienna Austria, May 2024, pp. 1–25.
- <span id="page-37-6"></span>[156] G. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, and S. Han, "SmoothQuant: Accurate and efficient post-training quantization for large language models," in *Proc. Int. Conf. Mach. Learn. (ICML)*, Honolulu, HI, USA, Jul. 2023, pp. 38 087–38 099.
- <span id="page-37-7"></span>[157] G. Fang, X. Ma, M. Song, M. B. Mi, and X. Wang, "DepGraph: Towards any structural pruning," in *Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR)*, Vancouver, BC, Canada, Jun. 2023, pp. 16 091–16 101.
- <span id="page-37-8"></span>[158] X. Dong, S. Chen, and S. Pan, "Learning to prune deep neural networks via layer-wise optimal brain surgeon," in *Proc. Adv. Neural Inform. Process. Syst. (NeurIPS)*, Long Beach, CA, USA, Dec. 2017.
- <span id="page-37-12"></span>[159] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V. Lin *et al.*, "OPT: Open pre-trained transformer language models," *arXiv preprint arXiv:2205.01068*, 2022.
- <span id="page-37-13"></span>[160] G. Hinton, O. Vinyals, and J. Dean, "Distilling the knowledge in a neural network," *arXiv preprint arXiv:1503.02531*, 2015.
- <span id="page-37-14"></span>[161] X. Xu, M. Li, C. Tao, T. Shen, R. Cheng, J. Li, C. Xu, D. Tao, and T. Zhou, "A survey on knowledge distillation of large language models," *arXiv preprint arXiv:2402.13116*, 2024.
- <span id="page-37-34"></span>[162] G. Gerganov, "llama.cpp," 2024, accessed: 2024-05-10. [Online]. Available:<https://github.com/ggerganov/llama.cpp>
- <span id="page-37-35"></span>[163] J. Xin, R. Tang, J. Lee, Y. Yu, and J. Lin, "DeeBERT: Dynamic early exiting for accelerating BERT inference," in *Proc. Annu. Meet. Assoc. Comput. Linguist. (ACL)*, Jul. 2020, pp. 2246–2251.
- <span id="page-37-36"></span>[164] Y. Matsubara, M. Levorato, and F. Restuccia, "Split computing and early exiting for deep learning applications: Survey and research challenges," *ACM Comput. Surv.*, vol. 55, no. 5, pp. 1–30, Dec. 2022.
- <span id="page-37-37"></span>[165] Y. Chen, X. Pan, Y. Li, B. Ding, and J. Zhou, "EE-LLM: Large-scale training and inference of early-exit large language models with 3D parallelism," in *Proc. Int. Conf. Mach. Learn. (ICML)*, Vienna, Austria, jul 2024, pp. 1–27.
- <span id="page-37-38"></span>[166] X. Li, Y. Shao, T. Sun, H. Yan, X. Qiu, and X. Huang, "Accelerating BERT inference for sequence labeling via early-exit," in *Proc. 59th Annu. Meet. Assoc. Comput. Linguist. and 11th Int. Joint Conf. Nat. Lang. Process.*, Aug. 2021, pp. 189–199.
- <span id="page-37-39"></span>[167] W. Fedus, B. Zoph, and N. Shazeer, "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity," *J. Mach. Learn. Res.*, vol. 23, no. 120, p. 5232–5270, Jan. 2022.
- [168] D. Lepikhin, H. Lee, Y. Xu, D. Chen, O. Firat, Y. Huang, M. Krikun, N. Shazeer, and Z. Chen, "GShard: Scaling giant models with conditional computation and automatic sharding," in *Proc. Int. Conf. Learn. Represent. (ICLR)*, may 2021, pp. 1–23.
- <span id="page-37-40"></span>[169] W. Fedus, J. Dean, and B. Zoph, "A review of sparse expert models in deep learning," *arXiv preprint arXiv:2209.01667*, 2022.
- <span id="page-37-41"></span>[170] Y. Song, Z. Mi, H. Xie, and H. Chen, "PowerInfer: Fast large language model serving with a consumer-grade GPU," *arXiv preprint arXiv:2312.12456*, 2023.
- <span id="page-37-42"></span>[171] B. Xiao, C. Shi, X. Nie, F. Yang, X. Deng, L. Su, W. Chen, and B. Cui, "Clover: Regressive lightweight speculative decoding with sequential knowledge," *arXiv preprint arXiv:2405.00263*, 2024.
- <span id="page-37-43"></span>[172] Y. Deng, Z. Song, and C. Yang, "Attention is naturally sparse with gaussian distributed input," *arXiv preprint arXiv:2404.02690*, 2024.

- <span id="page-38-0"></span>[173] P. Zeng, Z. Ning, J. Zhao, W. Cui, M. Xu, L. Guo, X. Chen, and Y. Shan, "The CAP principle for LLM serving: A survey of long-context large language model serving," *arXiv preprint arXiv:2405.11299*, 2024.
- [174] L. Ribar, I. Chelombiev, L. Hudlass-Galley, C. Blake, C. Luschi, and D. Orr, "SparQ attention: Bandwidth-efficient LLM inference," in *Proc. Int. Conf. Mach. Learn. (ICML)*, Vienna, Austria, jul 2024, pp. 1–26.
- <span id="page-38-1"></span>[175] L. Song, Y. Chen, S. Yang, X. Ding, Y. Ge, Y.-C. Chen, and Y. Shan, "Low-rank approximation for sparse attention in multi-modal llms," in *Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR)*, Seattle, WA, USA, Jun. 2024, pp. 13 763–13 773.
- <span id="page-38-2"></span>[176] Y. Gui, X. Yan, P. Yin, H. Yang, and J. Cheng, "SPT: Fine-tuning transformer-based language models efficiently with sparsification," *arXiv preprint arXiv:2312.10365*, 2023.
- <span id="page-38-3"></span>[177] Z. Zhang, Y. Sheng, T. Zhou, T. Chen, L. Zheng, R. Cai, Z. Song, Y. Tian, C. Re, C. Barrett, Z. Wang, and B. Chen, "H2O: Heavy-hitter oracle for efficient generative inference of large language models," in *Proc. Adv. Neural Inform. Process. Syst. (NeurIPS)*, New Orleans, LA, USA, Dec. 2023.
- <span id="page-38-4"></span>[178] G. Xiao, Y. Tian, B. Chen, S. Han, and M. Lewis, "Efficient streaming language models with attention sinks," in *Proc. Int. Conf. Learn. Represent. (ICLR)*, Vienna Austria, May 2024, pp. 1–21.
- <span id="page-38-5"></span>[179] WWDC24, "Platforms state of the union (ASL)," 2024. [Online]. Available: [https://developer.apple.com/videos/play/wwdc2024/102/?t](https://developer.apple.com/videos/play/wwdc2024/102/?time=279) [ime=279](https://developer.apple.com/videos/play/wwdc2024/102/?time=279)
- <span id="page-38-6"></span>[180] F. Zhang, L. Li, J. Chen, Z. Jiang, B. Wang, and Y. Qian, "IncreLoRA: Incremental parameter allocation method for parameter-efficient finetuning," *arXiv preprint arXiv:2308.12043*, 2023.
- <span id="page-38-7"></span>[181] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer, "QLoRA: Efficient finetuning of quantized LLMs," in *Proc. Adv. Neural Inform. Process. Syst. (NeurIPS)*, New Orleans, LA, USA, Dec. 2023, pp. 10 088–10 115.
- <span id="page-38-8"></span>[182] A. G. Baydin, B. A. Pearlmutter, D. Syme, F. Wood, and P. Torr, "Gradients without backpropagation," *arXiv preprint arXiv:2202.08587*, 2022.
- <span id="page-38-9"></span>[183] Huawei, *6G: The Next Horizon: From Connected People and Things to Connected Intelligence*. Cambridge, U.K.: Cambridge Univ. Press, 2021.
- <span id="page-38-10"></span>[184] C. Wu, J. Chen, S. Zhu, W. Feng, K. He, R. Du, and Y. Xiang, "WAFBOOSTER: Automatic boosting of waf security against mutated malicious payloads," *IEEE Trans. Depend. Sec. Comput.*, pp. 1–13, early access 2024.
- <span id="page-38-11"></span>[185] Q. Li, C. Wu, J. Chen, Z. Zhang, K. He, R. Du, X. Wang, Q. Zhao, and Y. Liu, "Privacy-preserving universal adversarial defense for black-box models," *arXiv preprint arXiv:2408.10647*, 2024.
- <span id="page-38-12"></span>[186] G. Qu, Z. Lin, F. Liu, X. Chen, and K. Huang, "TrimCaching: Parameter-sharing AI model caching in wireless edge networks," in *Proc. IEEE Int. Conf. Distrib. Comput. Syst. (ICDCS)*, Jersey City, NJ, USA, Jul. 2024, pp. 36–46.
- <span id="page-38-13"></span>[187] G. Xylomenos, C. N. Ververidis, V. A. Siris, N. Fotiou, C. Tsilopoulos, X. Vasilakos, K. V. Katsaros, and G. C. Polyzos, "A survey of information-centric networking research," *IEEE Commun. Surveys Tuts.*, vol. 16, no. 2, pp. 1024–1049, 2nd Quart. 2014.
- <span id="page-38-14"></span>[188] A. Seetharam, "On caching and routing in information-centric networks," *IEEE Commun. Mag.*, vol. 56, no. 3, pp. 204–209, Mar. 2018.
- <span id="page-38-15"></span>[189] H. Xiong, J. Bian, Y. Li, X. Li, M. Du, S. Wang, D. Yin, and S. Helal, "When search engine services meet large language models: Visions and challenges," *IEEE Trans. Serv. Comput.*, pp. 1–23, early access 2024.
- <span id="page-38-16"></span>[190] P. Ghimire, K. Kim, and M. Acharya, "Opportunities and challenges of generative AI in construction industry: Focusing on adoption of textbased models," *Buildings*, vol. 14, no. 1, p. 220, Jan. 2024.
- <span id="page-38-17"></span>[191] J. Cosentino, A. Belyaeva, X. Liu, N. A. Furlotte, Z. Yang, C. Lee, E. Schenck, Y. Patel, J. Cui, L. D. Schneider *et al.*, "Towards a personal health large language model," *arXiv preprint arXiv:2406.06474*, 2024.
- <span id="page-38-18"></span>[192] X. Liu, Y. Deng, and T. Mahmoodi, "Wireless distributed learning: A new hybrid split and federated learning approach," *IEEE Trans. Wireless Commun.*, vol. 22, no. 4, pp. 2650–2665, Apr. 2023.
- <span id="page-38-19"></span>[193] N. H. Tran, W. Bao, A. Zomaya, M. N. H. Nguyen, and C. S. Hong, "Federated learning over wireless networks: Optimization model design and analysis," in *Proc. IEEE Int. Conf. Comput. Commun. (INFOCOM)*, Paris, France, Apr. 2019, pp. 1387–1395.
- <span id="page-38-20"></span>[194] Z. Lin, G. Qu, X. Chen, and K. Huang, "Split learning in 6G edge networks," *IEEE Wireless Commun.*, vol. 31, no. 4, pp. 170–176, Aug. 2024.
- <span id="page-38-21"></span>[195] W. Fan, Y. Ding, L. Ning, S. Wang, H. Li, D. Yin, T.-S. Chua, and Q. Li, "A survey on RAG meeting LLMs: Towards retrieval-augmented

- large language models," in *Proc. ACM SIGKDD Conf. Knowl. Discov. Data Min.*, Barcelona Spain, Aug. 2024, pp. 6491–6501.
- <span id="page-38-22"></span>[196] J. Chen, H. Lin, X. Han, and L. Sun, "Benchmarking large language models in retrieval-augmented generation," in *Proc. AAAI Conf. Artif. Intell. (AAAI)*, vol. 38, no. 16, Vancouver, BC, Canada, Feb. 2024, pp. 17 754–17 762.
- <span id="page-38-24"></span>[197] G. Qu, Z. Lin, Q. Chen, J. Li, F. Liu, X. Chen, and K. Huang, "Trim-Caching: Parameter-sharing edge caching for AI model downloading," *arXiv preprint arXiv:2404.14204*, 2024.
- <span id="page-38-26"></span>[198] H. Wu, Q. Zeng, and K. Huang, "Efficient multiuser AI downloading via reusable knowledge broadcasting," *IEEE Trans. Wireless Commun.*, vol. 23, no. 8, pp. 10 459–10 472, Aug. 2024.
- <span id="page-38-25"></span>[199] K. Huang, H. Wu, Z. Liu, and X. Qi, "In-situ model downloading to realize versatile edge AI in 6G mobile networks," *IEEE Wireless Commun.*, vol. 30, no. 3, pp. 96–102, Jun. 2023.
- <span id="page-38-23"></span>[200] N. Hudson, H. Khamfroush, and D. E. Lucani, "QoS-aware placement of deep learning services on the edge with multiple service implementations," in *Proc. Int. Conf. on Comput. Commun. and Netw. (ICCCN)*, Athens, Greece, Jul. 2021, pp. 1–8.
- <span id="page-38-27"></span>[201] M. Xu, D. Niyato, H. Zhang, J. Kang, Z. Xiong, S. Mao, and Z. Han, "Joint foundation model caching and inference of generative AI services for edge intelligence," in *Proc. IEEE Glob. Commun. Conf. (GLOBECOM)*, Kuala Lumpur, Malaysia, Dec. 2023, pp. 3548–3553.
- <span id="page-38-28"></span>[202] M. Xu, D. Niyato, H. Zhang, J. Kang, Z. Xiong, S. Mao, and Z. Han, "Cached model-as-a-resource: Provisioning large language model agents for edge intelligence in space-air-ground integrated networks," *arXiv preprint arXiv:2403.05826*, 2024.
- <span id="page-38-29"></span>[203] K. Poularakis and L. Tassiulas, "Exploiting user mobility for wireless content delivery," in *Proc. IEEE Int. Symp. Inf. Theory (ISIT)*, Istanbul, Turkey, Jul. 2013, pp. 1017–1021.
- <span id="page-38-30"></span>[204] C. Zhong, M. C. Gursoy, and S. Velipasalar, "Deep reinforcement learning-based edge caching in wireless networks," *IEEE Trans. on Cogn. Commun. Netw.*, vol. 6, no. 1, pp. 48–61, Mar. 2020.
- <span id="page-38-31"></span>[205] J. Gu, W. Wang, A. Huang, H. Shan, and Z. Zhang, "Distributed cache replacement for caching-enable base stations in cellular networks," in *Proc. IEEE Int. Conf. Commun. (ICC)*, Sydney, Australia, Jun. 2014, pp. 2648–2653.
- <span id="page-38-32"></span>[206] E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh, "GPTQ: Accurate post-training quantization for generative pre-trained transformers," *arXiv preprint arXiv:2210.17323*, 2022.
- <span id="page-38-33"></span>[207] S. Kim, C. Hooper, A. Gholami, Z. Dong, X. Li, S. Shen, M. W. Mahoney, and K. Keutzer, "SqueezeLLM: Dense-and-sparse quantization," in *Proc. Int. Conf. Mach. Learn. (ICML)*, Vienna, Austria, jul 2024, pp. 1–23.
- <span id="page-38-34"></span>[208] T. Dettmers, R. Svirschevski, V. Egiazarian, D. Kuznedelev, E. Frantar, S. Ashkboos, A. Borzunov, T. Hoefler, and D. Alistarh, "SpQR: A sparse-quantized representation for near-lossless LLM weight compression," in *Proc. Int. Conf. Learn. Represent. (ICLR)*, Vienna Austria, May 2024, pp. 1–29.
- <span id="page-38-35"></span>[209] R. Tandon and O. Simeone, "Cloud-aided wireless networks with edge caching: Fundamental latency trade-offs in fog radio access networks," in *Proc. IEEE Int. Symp. Inf. Theory (ISIT)*, Barcelona, Spain, Jul. 2016, pp. 2029–2033.
- <span id="page-38-36"></span>[210] Y. Xu, H. Lee, D. Chen, H. Choi, B. Hechtman, and S. Wang, "Automatic cross-replica sharding of weight update in data-parallel training," *arXiv preprint arXiv:2004.13336*, 2020.
- <span id="page-38-37"></span>[211] Y. Huang, Y. Cheng, A. Bapna, O. Firat, D. Chen, M. Chen, H. Lee, J. Ngiam, Q. V. Le, Y. Wu *et al.*, "GPipe: Efficient training of giant neural networks using pipeline parallelism," in *Proc. Adv. Neural Inform. Process. Syst. (NeurIPS)*, Vancouver, BC, Canada, Dec. 2019, pp. 103–112.
- <span id="page-38-38"></span>[212] M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catanzaro, "Megatron-LM: Training multi-billion parameter language models using model parallelism," *arXiv preprint arXiv:1909.08053*, 2019.
- <span id="page-38-39"></span>[213] D. Narayanan, M. Shoeybi, J. Casper, P. LeGresley, M. Patwary, V. Korthikanti, D. Vainbrand, P. Kashinkunti, J. Bernauer, B. Catanzaro *et al.*, "Efficient large-scale language model training on GPU clusters using megatron-LM," in *Proc. Int. Conf. High Perform. Comput., Netw., Storage Anal.*, New York, NY, USA, Nov. 2021, pp. 1–15.
- <span id="page-38-40"></span>[214] S. Rajbhandari, J. Rasley, O. Ruwase, and Y. He, "ZeRO: Memory optimizations toward training trillion parameter models," in *Proc. Int. Conf. High Perform. Comput., Netw., Storage Anal.*, Atlanta, GA, USA, Nov. 2020, pp. 1–16.
- <span id="page-38-41"></span>[215] J. Ren, S. Rajbhandari, R. Y. Aminabadi, O. Ruwase, S. Yang, M. Zhang, D. Li, and Y. He, "ZeRO-Offload: Democratizing billionscale model training," in *Proc. USENIX Annu. Tech. Conf. (USENIX ATC)*, Jul. 2021, pp. 551–564.

- <span id="page-39-7"></span>[216] Y. Liu, Z. Zeng, W. Tang, and F. Chen, "Data-importance aware radio resource allocation: Wireless communication helps machine learning," *IEEE Commun. Lett.*, vol. 24, no. 9, pp. 1981–1985, May 2020.
- <span id="page-39-8"></span>[217] D. Liu, G. Zhu, J. Zhang, and K. Huang, "Data-importance aware user scheduling for communication-efficient edge machine learning," *IEEE Trans. on Cogn. Commun. Netw.*, vol. 7, no. 1, pp. 265–278, Mar. 2021.
- <span id="page-39-9"></span>[218] J. Jiang, X. Liu, and C. Fan, "Low-parameter federated learning with large language models," in *International Conference on Web Information Systems and Applications*, Yinchuan, China, Aug. 2024, pp. 319–330.
- <span id="page-39-10"></span>[219] W. Kuang, B. Qian, Z. Li, D. Chen, D. Gao, X. Pan, Y. Xie, Y. Li, B. Ding, and J. Zhou, "FederatedScope-LLM: A comprehensive package for fine-tuning large language models in federated learning," in *Proc. ACM SIGKDD Conf. Knowl. Discovery Data Mining (KDD)*, Barcelona, Spain, Aug. 2024, p. 5260–5271.
- <span id="page-39-11"></span>[220] S. Chen, G. Long, T. Shen, and J. Jiang, "Prompt federated learning for weather forecasting: Toward foundation models on meteorological data," in *Proc. Int. Joint Conf. Artif. Intell. (IJCAI)*, Aug. 2023, pp. 3532 – 3540.
- <span id="page-39-12"></span>[221] T. Che, J. Liu, Y. Zhou, J. Ren, J. Zhou, V. S. Sheng, H. Dai, and D. Dou, "Federated learning of large language models with parameterefficient prompt tuning and adaptive optimization," in *Proc. Conf. Empir. Methods Nat. Lang. Process. (EMNLP)*, Singapore, Dec. 2023, pp. 7871–7888.
- <span id="page-39-13"></span>[222] F.-E. Yang, C.-Y. Wang, and Y.-C. F. Wang, "Efficient model personalization in federated learning via client-specific prompt generation," in *Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR)*, Vancouver, BC, Canada, Jun. 2023, pp. 19 159–19 168.
- <span id="page-39-14"></span>[223] H. Nam, J. Park, and S.-L. Kim, "Active wireless split learning via online cloud-local server Delta-knowledge distillation," in *Proc. IEEE Int. Conf. Commun. Workshops*, Rome, Italy, May 2023, pp. 825–830.
- <span id="page-39-15"></span>[224] F. Zheng, C. Chen, L. Lyu, and B. Yao, "Reducing communication for split learning by randomized top-k sparsification," in *Proc. Int. Joint Conf. Artif. Intell. (IJCAI)*, Macao, China, Aug., 2023, pp. 4665—- 4673.
- <span id="page-39-16"></span>[225] S. Ohta and T. Nishio, "λ-split: A privacy-preserving split computing framework for cloud-powered generative AI," *arXiv preprint arXiv:2310.14651*, 2023.
- <span id="page-39-17"></span>[226] G. Wang, J. Liu, C. Li, J. Ma, Y. Zhang, X. Wei, K. Zhang, M. Chong, R. Zhang, Y. Liu, and S. Zhang, "Cloud-device collaborative learning for multimodal large language models," in *Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR)*, Seattle, WA, USA, Jun. 2024, pp. 12 646–12 655.
- <span id="page-39-18"></span>[227] Y. Tian, Z. Zhang, Y. Yang, Z. Chen, Z. Yang, R. Jin, T. Q. S. Quek, and K.-K. Wong, "An edge-cloud collaboration framework for generative AI service provision with synergetic big cloud model and small edge models," *arXiv preprint arXiv:2401.01666*, 2024.
- <span id="page-39-19"></span>[228] L. Liu, J. Zhang, S. Song, and K. B. Letaief, "Client-edge-cloud hierarchical federated learning," in *Proc. IEEE Int. Conf. Commun. (ICC)*, Dublin, Ireland, Jun. 2020, pp. 1–6.
- <span id="page-39-20"></span>[229] L. Liu, J. Zhang, S. Song, and K. B. Letaief, "Hierarchical federated learning with quantization: Convergence analysis and system design," *IEEE Trans. Wireless Commun.*, vol. 22, no. 1, pp. 2–18, Jan. 2023.
- <span id="page-39-0"></span>[230] A. M. Devices, "Fine-tune Llama 2 with LoRA: Customizing a large language model for question-answering," 2024. [Online]. Available: [https://rocm.blogs.amd.com/artificial-intelligence/llama2-lora/REA](https://rocm.blogs.amd.com/artificial-intelligence/llama2-lora/README.html) [DME.html](https://rocm.blogs.amd.com/artificial-intelligence/llama2-lora/README.html)
- <span id="page-39-1"></span>[231] Z. Jiang, H. Lin, Y. Zhong, Q. Huang, Y. Chen, Z. Zhang, Y. Peng, X. Li, C. Xie, S. Nong *et al.*, "MegaScale: Scaling large language model training to more than 10,000 GPUs," in *Proc. USENIX Symp. Netw. Syst. Des. Implement. (NSDI)*, Santa Clara, CA, USA, Apr. 2024, pp. 745–760.
- <span id="page-39-2"></span>[232] J. Zhou, Y. Chen, Z. Hong, W. Chen, Y. Yu, T. Zhang, H. Wang, C. Zhang, and Z. Zheng, "Training and serving system of foundation models: A comprehensive survey," *IEEE Open J. Comput. Soc.*, vol. 5, pp. 107–119, Mar. 2024.
- <span id="page-39-4"></span>[233] J. Novikova, O. Dusek, and V. Rieser, "The E2E dataset: New chal- ˇ lenges for end-to-end generation," in *Proc. Annu. SIGdial Meeting Disc. Dialogue*, Saarbrucken, Germany, Aug. 2017, pp. 201–206. ¨
- <span id="page-39-3"></span>[234] S. Rajbhandari, O. Ruwase, J. Rasley, S. Smith, and Y. He, "ZeRO-Infinity: Breaking the GPU memory wall for extreme scale deep learning," in *Proc. Int. Conf. High Perform. Comput., Netw., Storage Anal.*, St. Louis, MO, USA, Nov. 2021, pp. 1–14.
- <span id="page-39-5"></span>[235] C. Hu, W. Bao, D. Wang, and F. Liu, "Dynamic adaptive DNN surgery for inference acceleration on the edge," in *Proc. IEEE Int. Conf. Comput. Commun. (INFOCOM)*, Paris, France, Apr. 2019, pp. 1423– 1431.

- <span id="page-39-6"></span>[236] S. Wang, X. Zhang, H. Uchiyama, and H. Matsuda, "HiveMind: Towards cellular native machine learning model splitting," *IEEE J. Sel. Areas Commun.*, vol. 40, no. 2, pp. 626–640, Feb. 2021.
- <span id="page-39-21"></span>[237] X. Chen, G. Zhu, Y. Deng, and Y. Fang, "Federated learning over multihop wireless networks with in-network aggregation," *IEEE Trans. Wireless Commun.*, vol. 21, no. 6, pp. 4622–4634, Jun. 2022.
- <span id="page-39-22"></span>[238] Z. Fang, Z. Lin, Z. Chen, X. Chen, Y. Gao, and Y. Fang, "Automated federated pipeline for parameter-efficient fine-tuning of large language models," *arXiv preprint arXiv:2404.06448*, 2024.
- <span id="page-39-23"></span>[239] Z. Lin, Z. Chen, Z. Fang, X. Chen, X. Wang, and Y. Gao, "FedSN: A general federated learning framework over LEO satellite networks," *IEEE Trans. Mobile Comput.*, pp. 1–15, early access 2024.
- <span id="page-39-24"></span>[240] Y. Zhang, H. Chen, Z. Lin, Z. Chen, and J. Zhao, "FedAC: A adaptive clustered federated learning framework for heterogeneous data," *arXiv preprint arXiv:2403.16460*, 2024.
- <span id="page-39-25"></span>[241] H. Woisetschlager, A. Isenko, S. Wang, R. Mayer, and H.-A. Jacobsen, ¨ "Federated fine-tuning of LLMs on the very edge: The good, the bad, the ugly," in *Proc. Workshop Data Manag. End-to-End Mach. Learn.*, Santiago, AA, Chile, Jun. 2024, p. 39–50.
- <span id="page-39-26"></span>[242] J. Zhang, S. Vahidian, M. Kuo, C. Li, R. Zhang, T. Yu, Y. Zhou, G. Wang, and Y. Chen, "Towards building the federated GPT: Federated instruction tuning," in *Proc. IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP)*, Seoul, Korea, Republic of, Apr. 2024, pp. 6915– 6919.
- <span id="page-39-28"></span>[243] Z. Zhang, Y. Yang, Y. Dai, L. Qu, and Z. Xu, "When federated learning meets pre-trained language models' parameter-efficient tuning methods," *arXiv preprint arXiv:2212.10025*, 2023.
- <span id="page-39-29"></span>[244] T. Fan, Y. Kang, G. Ma, W. Chen, W. Wei, L. Fan, and Q. Yang, "FATE-LLM: A industrial grade federated learning framework for large language models," *arXiv preprint arXiv:2310.10049*, 2023.
- <span id="page-39-27"></span>[245] S. Babakniya, A. R. Elkordy, Y. H. Ezzeldin, Q. Liu, K.-B. Song, M. El-Khamy, and S. Avestimehr, "SLoRA: Federated parameter efficient fine-tuning of language models," in *Proc. Adv. Neural Inform. Process. Syst. (NeurIPS)*, New Orleans, LA, USA, Dec. 2023, pp. 1–13.
- <span id="page-39-30"></span>[246] X. Liu, K. Ji, Y. Fu, W. L. Tam, Z. Du, Z. Yang, and J. Tang, "P-Tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks," in *Proc. 60th Annu. Meet. Assoc. Comput. Linguist. (ACL)*, Dublin, Ireland, May 2022, pp. 61–68.
- <span id="page-39-31"></span>[247] C. Chen, H. Xu, W. Wang, B. Li, B. Li, L. Chen, and G. Zhang, "Communication-efficient federated learning with adaptive parameter freezing," in *Proc. IEEE Int. Conf. Distrib. Comput. Syst. (ICDCS)*, Jul. 2021, pp. 1–11.
- <span id="page-39-32"></span>[248] Q. Chen, W. Meng, T. Q. S. Quek, and S. Chen, "Multi-tier hybrid offloading for computation-aware IoT applications in civil aircraftaugmented SAGIN," *IEEE J. Sel. Areas Commun.*, vol. 41, no. 2, pp. 399–417, Dec. 2023.
- <span id="page-39-33"></span>[249] Z. Lin, G. Qu, Q. Chen, X. Chen, Z. Chen, and K. Huang, "Pushing large language models to the 6G edge: Vision, challenges, and opportunities," *arXiv preprint arXiv:2309.16739*, 2023.
- <span id="page-39-34"></span>[250] O. Gupta and R. Raskar, "Distributed learning of deep neural network over multiple agents," *J. Netw. Comput. Appl.*, vol. 116, pp. 1–8, Aug. 2018.
- <span id="page-39-35"></span>[251] P. Vepakomma, O. Gupta, T. Swedish, and R. Raskar, "Split learning for health: Distributed deep learning without sharing raw patient data," *arXiv preprint arXiv:1812.00564*, 2018.
- <span id="page-39-36"></span>[252] Y. J. Ha, M. Yoo, S. Park, S. Jung, and J. Kim, "Secure aerial surveillance using split learning," in *Proc. Int. Conf. Ubiquitous Future Netw. (ICUFN)*, Jeju Island, Korea, Republic of, Aug. 2021, pp. 434– 437.
- <span id="page-39-37"></span>[253] Z. Lin, G. Zhu, Y. Deng, X. Chen, Y. Gao, K. Huang, and Y. Fang, "Efficient parallel split learning over resource-constrained wireless edge networks," *IEEE Trans. Mobile Comput.*, vol. 23, no. 10, pp. 9224–9239, Oct. 2024.
- <span id="page-39-38"></span>[254] M. Kim, A. DeRieux, and W. Saad, "A bargaining game for personalized, energy efficient split learning over wireless networks," in *Proc. IEEE Wireless Commun. Netw. Conf. (WCNC)*, Glasgow, United Kingdom, Mar. 2023, pp. 1–6.
- <span id="page-39-39"></span>[255] C. Thapa, P. C. M. Arachchige, S. Camtepe, and L. Sun, "SplitFed: When federated learning meets split learning," in *Proc. AAAI Conf. Artif. Intell. (AAAI)*, Vancouver, BC, Canada, Feb. 2022, pp. 8485– 8493.
- <span id="page-39-40"></span>[256] Z. Lin, G. Qu, W. Wei, X. Chen, and K. K. Leung, "AdaptSFL: Adaptive split federated learning in resource-constrained edge networks," *arXiv preprint arXiv:2403.13101*, 2024.
- <span id="page-39-41"></span>[257] C. Thapa, M. A. P. Chamikara, and S. A. Camtepe, "Advancements of federated learning towards privacy preservation: From federated

- learning to split learning," *Federated Learning Systems: Towards Next-Generation AI*, pp. 79–109, Jun. 2021.
- <span id="page-40-0"></span>[258] Z. Yang, Y. Chen, H. Huangfu, M. Ran, H. Wang, X. Li, and Y. Zhang, "Robust split federated learning for U-shaped medical image networks," *arXiv preprint arXiv:2212.06378*, 2022.
- <span id="page-40-1"></span>[259] S. Lyu, Z. Lin, G. Qu, X. Chen, X. Huang, and P. Li, "Optimal resource allocation for U-shaped parallel split learning," in *Proc. IEEE Glob. Commun. Conf. Workshops (GC Wkshps)*, Kuala Lumpur, Malaysia, Dec. 2023, pp. 197–202.
- <span id="page-40-2"></span>[260] G. Zhu, Y. Deng, X. Chen, H. Zhang, Y. Fang, and T. F. Wong, "ESFL: Efficient split federated learning over resource-constrained heterogeneous wireless devices," *IEEE Internet Things J.*, vol. 11, no. 16, pp. 27 153–27 166, Aug. 2024.
- <span id="page-40-3"></span>[261] Y. Chen, R. Li, Z. Zhao, C. Peng, J. Wu, E. Hossain, and H. Zhang, "NetGPT: A native-AI network architecture beyond provisioning personalized generative services," *arXiv preprint arXiv:2307.06148*, 2024.
- <span id="page-40-4"></span>[262] R. Gozalo-Brizuela and E. C. Garrido-Merchan, "ChatGPT is not all you need. A state of the art review of large generative AI models," *arXiv preprint arXiv:2301.04655*, 2023.
- <span id="page-40-5"></span>[263] C. Gao and S. Q. Zhang, "DLoRA: Distributed parameter-efficient fine-tuning solution for large language model," *arXiv preprint arXiv:2404.05182*, 2024.
- <span id="page-40-6"></span>[264] S. Yun, Z. A. Bhuiyan, M. T. A. H. Sadi, and S. Su, "Privacy-preserving federated learning through clustered sampling on fine-tuning distributed non-iid large language models," in *Proc. IEEE Int. Conf. Parallel Distrib. Process. Appl., Big Data Cloud Comput., Sustain. Comput. Commun., Social Comput. Netw. (ISPA/BDCloud/SocialCom/SustainCom)*, Wuhan, China, Dec. 2023, pp. 531–538.
- <span id="page-40-7"></span>[265] Y. Liang, C. Ge, Z. Tong, Y. Song, J. Wang, and P. Xie, "Not all patches are what you need: Expediting vision transformers via token reorganizations," in *Proc. Int. Conf. Learn. Represent. (ICLR)*, Apr. 2022, pp. 1–21.
- <span id="page-40-10"></span>[266] H. Jiang, Q. Wu, C.-Y. Lin, Y. Yang, and L. Qiu, "LLMLingua: Compressing prompts for accelerated inference of large language models," in *Proc. Conf. Empir. Methods Nat. Lang. Process. (EMNLP)*, Singapore, Dec. 2023, pp. 13 358–13 376.
- <span id="page-40-11"></span>[267] D. Ding, A. Mallick, C. Wang, R. Sim, S. Mukherjee, V. Ruhle, L. V. Lakshmanan, and A. H. Awadallah, "Hybrid LLM: Cost-efficient and quality-aware query routing," in *Proc. Int. Conf. Learn. Represent. (ICLR)*, Vienna Austria, May 2024, pp. 1–19.
- <span id="page-40-12"></span>[268] Y. Fu, L. Xue, Y. Huang, A.-O. Brabete, D. Ustiugov, Y. Patel, and L. Mai, "ServerlessLLM: Locality-enhanced serverless inference for large language models," *arXiv preprint arXiv:2401.14351*, 2024.
- <span id="page-40-13"></span>[269] Q. Cao, S. Min, Y. Wang, and H. Hajishirzi, "BTR: Binary token representations for efficient retrieval augmented language models," in *Proc. Int. Conf. Learn. Represent. (ICLR)*, Vienna Austria, May 2024, pp. 1–18.
- <span id="page-40-14"></span>[270] S. Goyal, A. R. Choudhury, S. Raje, V. Chakaravarthy, Y. Sabharwal, and A. Verma, "PoWER-BERT: Accelerating BERT inference via progressive word-vector elimination," in *Proc. Int. Conf. Mach. Learn. (ICML)*, Jul. 2020, pp. 3690–3699.
- <span id="page-40-15"></span>[271] D. Bolya, C.-Y. Fu, X. Dai, P. Zhang, C. Feichtenhofer, and J. Hoffman, "Token merging: Your ViT but faster," in *Proc. Int. Conf. Learn. Represent. (ICLR)*, Kigali, Rwanda, May 2023, pp. 1–20.
- <span id="page-40-16"></span>[272] J. Shao, Y. Mao, and J. Zhang, "Learning task-oriented communication for edge inference: An information bottleneck approach," *IEEE J. Sel. Areas Commun.*, vol. 40, no. 1, pp. 197–211, Jan. 2021.
- <span id="page-40-17"></span>[273] Y. Shi, Y. Zhou, D. Wen, Y. Wu, C. Jiang, and K. B. Letaief, "Taskoriented communications for 6G: Vision, principles, and technologies," *IEEE Wireless Commun.*, vol. 30, no. 3, pp. 78–85, Jun. 2023.
- <span id="page-40-18"></span>[274] Q. Cao, B. Paranjape, and H. Hajishirzi, "PuMer: Pruning and merging tokens for efficient vision language models," in *Proc. Annu. Meet. Assoc. Comput. Linguist. (ACL)*, Toronto, Canada, Jul. 2023, pp. 12 890–12 903.
- <span id="page-40-19"></span>[275] M. Chen, M. Liu, W. Wang, H. Dou, and L. Wang, "Cross-modal semantic communications in 6G," in *Proc. IEEE/CIC Int. Conf. Commun. China (ICCC)*, Dalian, China, Aug. 2023, pp. 1–6.
- <span id="page-40-20"></span>[276] Q. Lan, Q. Zeng, P. Popovski, D. Gund ¨ uz, and K. Huang, "Progressive ¨ feature transmission for split classification at the wireless edge," *IEEE Trans. Wireless Commun.*, vol. 22, no. 6, pp. 3837–3852, Jun. 2023.
- <span id="page-40-21"></span>[277] R. Ma, J. Wang, Q. Qi, X. Yang, H. Sun, Z. Zhuang, and J. Liao, "Poster: PipeLLM: Pipeline LLM inference on heterogeneous devices with sequence slicing," in *Proc. ACM SIGCOMM*, New York, NY, USA, Sep. 2023, pp. 1126–1128.
- <span id="page-40-22"></span>[278] Y. Wang, K. Chen, H. Tan, and K. Guo, "Tabi: An efficient multilevel inference system for large language models," in *Proc. Eur. Conf. Comput. Syst (EuroSys)*, Rome, Italy, May 2023, pp. 233–248.

- <span id="page-40-8"></span>[279] X. Huang, L. L. Zhang, K.-T. Cheng, and M. Yang, "Fewer is more: Boosting LLM reasoning with reinforced context pruning," *arXiv preprint arXiv:2312.08901*, 2023.
- <span id="page-40-9"></span>[280] Y. Li, B. Dong, C. Lin, and F. Guerin, "Compressing context to enhance inference efficiency of large language models," in *Proc. Conf. Empir. Methods Nat. Lang. Process. (EMNLP)*, Singapore, Dec. 2023, pp. 6342–6353.
- <span id="page-40-23"></span>[281] H. Ding, Y. Guo, X. Li, and Y. Fang, "Beef up the edge: Spectrumaware placement of edge computing services for the internet of things," *IEEE Trans. Mobile Comput.*, vol. 18, no. 12, pp. 2783–2795, Dec. 2019.
- <span id="page-40-24"></span>[282] X. Chen, G. Zhu, H. Ding, L. Zhang, H. Zhang, and Y. Fang, "Endto-end service auction: A general double auction mechanism for edge computing services," *IEEE/ACM Trans. Netw.*, vol. 30, no. 6, pp. 2616– 2629, Dec. 2022.
- <span id="page-40-25"></span>[283] Z. Liu, Q. Song, Q. C. Xiao, S. K. Selvaraj, R. Mazumder, A. Gupta, and X. Hu, "FFSplit: Split feed-forward network for optimizing accuracy-efficiency trade-off in language model inference," *arXiv preprint arXiv:2401.04044*, 2024.
- <span id="page-40-26"></span>[284] J. Liu, R. Gong, X. Wei, Z. Dong, J. Cai, and B. Zhuang, "QLLM: Accurate and efficient low-bitwidth quantization for large language models," in *Proc. Int. Conf. Learn. Represent. (ICLR)*, Vienna Austria, May 2024, pp. 1–23.
- <span id="page-40-27"></span>[285] G. Yang, D. Lo, R. Mullins, and Y. Zhao, "Dynamic stashing quantization for efficient transformer training," in *Proc. Findings Assoc. Comput. Linguist.: EMNLP 2023*, Singapore, Dec. 2023, pp. 7329– 7336.
- <span id="page-40-28"></span>[286] Y. Jin, K. Xu, L. Chen, C. Liao, J. Tan, B. Chen, C. Lei, A. Liu, C. Song, X. Lei *et al.*, "Unified language-vision pretraining in LLM with dynamic discrete visual tokenization," in *Proc. Int. Conf. Learn. Represent. (ICLR)*, Vienna Austria, May 2024, pp. 1–21.
- <span id="page-40-29"></span>[287] P. Jin, R. Takanobu, C. Zhang, X. Cao, and L. Yuan, "Chat-UniVi: Unified visual representation empowers large language models with image and video understanding," in *Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR)*, Seattle, WA, USA, Jun. 2024, pp. 13 700–13 710.
- <span id="page-40-30"></span>[288] J. Huang, D. Li, C. Huang, X. Qin, and W. Zhang, "Joint task and dataoriented semantic communications: A deep separate source-channel coding scheme," *IEEE Internet Things J.*, vol. 11, no. 2, pp. 2255– 2272, Jan. 2024.
- <span id="page-40-31"></span>[289] P. Wang, J. Li, C. Liu, X. Fan, M. Ma, and Y. Wang, "Distributed semantic communications for multimodal audio-visual parsing tasks," *IEEE Trans. Green Commun. Netw.*, pp. 1–10, early access, 2024.
- <span id="page-40-32"></span>[290] J. Huang, K. Yuan, C. Huang, and K. Huang, "D<sup>2</sup> -JSCC: Digital deep joint source-channel coding for semantic communications," *arXiv preprint arXiv:2403.07338*, 2024.
- <span id="page-40-33"></span>[291] A. Borzunov, M. Ryabinin, A. Chumachenko, D. Baranchuk, T. Dettmers, Y. Belkada, P. Samygin, and C. Raffel, "Distributed inference and fine-tuning of large language models over the internet," in *Proc. Adv. Neural Inform. Process. Syst. (NeurIPS)*, New Orleans, LA, USA, Dec. 2023, pp. 12 312–12 331.
- <span id="page-40-34"></span>[292] D. Patterson, J. Gonzalez, Q. Le, C. Liang, L.-M. Munguia, D. Rothchild, D. So, M. Texier, and J. Dean, "Carbon emissions and large neural network training," *arXiv preprint arXiv:2104.10350*, 2021.
- <span id="page-40-35"></span>[293] M. Aibin, "Energy consumption of ChatGPT responses," 2024. [Online]. Available: [https://www.baeldung.com/cs/chatgpt-large-lang](https://www.baeldung.com/cs/chatgpt-large-language-models-power-consumption) [uage-models-power-consumption](https://www.baeldung.com/cs/chatgpt-large-language-models-power-consumption)
- <span id="page-40-36"></span>[294] Y. Li, M. Mughees, Y. Chen, and Y. R. Li, "The unseen AI disruptions for power grids: LLM-induced transients," *arXiv preprint arXiv:2409.11416*, 2024.
- <span id="page-40-37"></span>[295] Y. Mao, X. Yu, K. Huang, Y.-J. A. Zhang, and J. Zhang, "Green edge AI: A contemporary survey," *Proc. IEEE*, pp. 1–32, early access 2024.
- <span id="page-40-38"></span>[296] C. Wu, J. Chen, Q. Fang, K. He, Z. Zhao, H. Ren, G. Xu, Y. Liu, and Y. Xiang, "Rethinking membership inference attacks against transfer learning," *IEEE Trans. Inf. Forensics Secur.*, vol. 19, pp. 6441–6454, Jun. 2024.
- <span id="page-40-39"></span>[297] X. Yang, J. Chen, K. He, H. Bai, C. Wu, and R. Du, "Efficient privacypreserving inference outsourcing for convolutional neural networks," *IEEE Trans. Inf. Forensics Secur.*, vol. 18, pp. 4815–4829, Jun. 2023.
- <span id="page-40-40"></span>[298] A. Panda, C. A. Choquette-Choo, Z. Zhang, Y. Yang, and P. Mittal, "Teach LLMs to phish: Stealing private information from language models," in *Proc. Int. Conf. Learn. Represent. (ICLR)*, Vienna Austria, May 2024, pp. 1–25.
- <span id="page-40-41"></span>[299] A. Wan, E. Wallace, S. Shen, and D. Klein, "Poisoning language models during instruction tuning," in *Proc. Int. Conf. Mach. Learn. (ICML)*, Honolulu, HI, USA, Jul. 2023, pp. 35 413–35 425.

- <span id="page-41-0"></span>[300] N. Kandpal, M. Jagielski, F. Tramer, and N. Carlini, "Backdoor attacks ` for in-context learning with language models," in *Proc. Int. Conf. Mach. Learn. (ICML)*, Honolulu, HI, USA, Jul. 2023, pp. 1–11.
- <span id="page-41-1"></span>[301] A. Lee, B. Miranda, S. Sundar, and S. Koyejo, "Beyond scale: the diversity coefficient as a data quality metric demonstrates LLMs are pre-trained on formally diverse data," in *Proc. Int. Conf. Mach. Learn. (ICML)*, Honolulu, HI, USA, Jul. 2023, pp. 1–18.
- <span id="page-41-2"></span>[302] M. Hofer, D. Obraczka, A. Saeedi, H. Kopcke, and E. Rahm, "Con- ¨ struction of knowledge graphs: Current state and challenges," *Information*, vol. 15, no. 8, p. 509, Aug. 2024.
- <span id="page-41-3"></span>[303] Wonderchat, "How to train ChatGPT on your own data," 2024. [Online]. Available: [https://wonderchat.io/blog/how-to-train-chatgpt](https://wonderchat.io/blog/how-to-train-chatgpt-on-your-own-data)[on-your-own-data](https://wonderchat.io/blog/how-to-train-chatgpt-on-your-own-data)
- <span id="page-41-4"></span>[304] J. Zhang, H. Gao, P. Zhang, B. Feng, W. Deng, and Y. Hou, "LA-UCL: LLM-augmented unsupervised contrastive learning framework for fewshot text classification," in *Proc. Joint Int. Conf. Comput. Linguistics, Lang. Resources Eval. (LREC-COLING 2024)*, Torino, Italia, May 2024, pp. 10 198–10 207.