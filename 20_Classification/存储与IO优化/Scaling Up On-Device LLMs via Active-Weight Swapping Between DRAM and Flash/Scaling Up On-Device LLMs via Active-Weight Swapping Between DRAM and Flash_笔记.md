---
title: "ActiveFlow: Scaling Up On-Device LLMs via Active-Weight Swapping Between DRAM and Flash"
authors: "Fucheng Jia, Huiqiang Jiang, Yunxin Liu, Zewen Wu, Qianxi Zhang, Ju Ren, Ting Cao, Shiqi Jiang, Yuqing Yang, Deyu Zhang"
year: "2026"
created: "2026-01-20"
tags: [paper-notes, auto-generated]
status: "generated"
---

# ActiveFlow: Scaling Up On-Device LLMs via Active-Weight Swapping Between DRAM and Flash


## 背景、动机与关键观察

根据论文内容，整理背景、动机与关键观察部分的详细笔记如下：

### 背景、动机与关键观察

#### 移动端大模型部署的内存瓶颈

随着大语言模型（LLMs）逐渐成为移动设备和 PC 系统的核心组件（例如 Apple 的 3B 基础模型、微软的 Phi Silica 等），将更大规模的模型部署在边缘端面临着严峻的挑战。其中最核心的限制来自于 **DRAM（动态随机存取存储器）容量**。

1.  **DRAM 容量限制与增长停滞**：受限于功耗和物理面积，移动设备的 DRAM 容量增长缓慢。即使是最新一代的旗舰手机（如 iPhone 16），其内存往往也停留在 8 GB 左右，难以容纳参数量日益增长的 LLM（通常需要数十 GB 显存）。
2.  **操作系统的内存管理机制**：移动设备的可用 DRAM 并不是全部供给 LLM 使用的。它由操作系统、后台活跃应用和当前前台应用共享。当系统可用内存过低时，移动操作系统（如 iOS 或 Android）会强制终止后台应用。因此，LLM 推理必须具备 **自适应 DRAM 使用（Adaptive DRAM usage）** 的能力，即根据当前剩余内存动态调整资源占用。
3.  **传统 DRAM-Flash 交换的局限性**：
    *   为了突破物理内存限制，传统的深度神经网络（如 CNN 和 BERT）通常采用 **DRAM-Flash 交换** 技术，类似于操作系统的虚拟内存机制。
    *   **I/O 瓶颈**：这种方法在 LLM 的 **自回归解码（Autoregressive Decoding）** 阶段失效。传统的 DNN 是计算密集型的，计算时间足以掩盖数据加载时间；而 LLM 解码是内存密集型的（Memory-bound），每次生成一个 token 都需要加载全部权重。
    *   Flash 存储的带宽（通常为 GB/s 级别）远低于 DRAM 带宽（通常为 10s-100s GB/s 级别），两者存在显著差距（约 5 倍）。如果在解码阶段频繁进行全量权重的 DRAM-Flash 交换，会导致推理速度极慢。


<!-- Figure 1 未找到匹配图片 -->


**图表分析 (Figure 1)**：
该图展示了 LLaMA-3-8B 模型的困惑度（Perplexity，越低越好）与内存成本（Cost）的关系。
*   **Pareto Frontier**：ActiveFlow（本论文方法）位于性能-成本的帕累托前沿上。
*   **对比基线**：相比于现有的量化（Q）、剪枝（P）和上下文稀疏（SP）方法，ActiveFlow 在相同的内存预算下能获得更低的困惑度，或在相同的模型质量下占用更少的内存。这证明了实现自适应内存管理的必要性和有效性。

#### 上下文稀疏性与现有方法的不足

为了解决上述内存瓶颈，论文利用了 LLM 的一个独特属性：**上下文稀疏性（Contextual Sparsity）**。

1.  **活跃权重（Active Weights）**：虽然模型整体参数量巨大，但在生成每一个 token 时，实际上只有一小部分权重参与了有效的计算。这部分被激活的权重被称为“活跃权重”。
2.  **稀疏性上限分析**：
    *   论文通过 Llama-2-70B 模型进行了上限分析。计算权重的重要性得分 $S_{ij} = |W_{ij}| \cdot |X_j|$ （其中 $W_{ij}$ 是权重元素，$X_j$ 是输入激活元素）。
    *   结果显示，在解码过程中，为了生成与全量模型相同的 token，仅需要激活 **< 15%** 的权重。这为减少内存占用提供了巨大的理论空间。


<!-- Figure 2 未找到匹配图片 -->


**图表分析 (Figure 2)**：
该图展示了 LLaMa-2-70B 模型在解码期间的上限稀疏度。
*   **横轴**：解码的 Token ID。
*   **纵轴**：活跃权重的百分比。
*   **结论**：绝大多数 token 只需要不到 5% 的权重即可准确生成，最大需求量也不超过 15%。这意味着如果我们能准确预测并仅加载这部分权重，就能极大降低 DRAM 占用。

3.  **现有方法的不足**：
    *   **ReLU-based 方法的局限**：现有的稀疏推理工作（如 Deja Vu, PowerInfer）严重依赖 **ReLU** 激活函数产生的精确零值。然而，现代主流 LLM（如 LLaMA, Mixtral）大多使用 **SwiGLU** 或 **SiLU** 等非 ReLU 激活函数，因为它们具有更好的精度表现。
    *   **额外预测器的开销**：为了预测哪些神经元为零，现有方法通常需要训练额外的预测器（Predictors）。这些预测器本身就会占用 GB 级别的内存，且引入了额外的计算延迟，不适合资源受限的移动端。
    *   **Top-K 激活稀疏性的优势**：论文采用了基于幅度的 **Top-K 稀疏性**（Magnitude-based Sparsity）。
        *   **原理**：仅保留激活值幅度最大的前 K 个元素对应的通道。
        *   **优点**：不依赖特定模型架构（如 ReLU），无需额外的预测器，且已被证明与模型量化兼容。


<!-- Figure 3 未找到匹配图片 -->


**图表分析 (Figure 3)**：
该图对比了 ReLU-based 稀疏性和 Top-K 激活稀疏性。
*   **(a) ReLU-based Sparsity**：依赖 ReLU 将负值变为 0，且通常需要一个额外的 Predictor 模块来提前预测零值，增加了系统复杂度和内存开销。
*   **(b) Top-K Sparsity**：直接选取输入向量 $x$ 中绝对值最大的元素，只计算对应的权重行/列。这种方法更加通用，且不需要额外的模型组件。

#### 两个关键观察：跨层相似性与时间局部性

为了实现高效的活跃权重 DRAM-Flash 交换，论文基于对 LLM 行为的深入分析，提出了两个关键观察。

**观察一：跨层输入激活具有高度相似性**

传统的 DNN 计算是层与层顺序依赖的，导致无法在计算当前层时预加载下一层的权重（因为下一层的输入还未计算出来）。然而，论文发现 LLM 中存在 **跨层相似性（Cross-layer Similarity）**。

1.  **现象**：在 Transformer 结构中，Attention 块和 MLP 块的输入激活在相邻层之间表现出极高的余弦相似度（Cosine Similarity，> 95%）。
2.  **原因分析**：这主要是由于广泛使用的 **残差连接（Residual Connection）** 结构。
    *   Transformer 层的输出公式大致为：$Output = x + F(x)$。
    *   其中，$x$ 是残差输入，$F(x)$ 是当前块的计算结果。
    *   由于 LayerNorm 层的存在（会将数值缩放）以及权重本身的幅度较小，导致 $F(x)$ 的数值远小于残差 $x$。
    *   因此，下一层的输入 $x_{next} \approx x + F(x)$ 主要由 $x$ 主导，导致 $x_{next}$ 与 $x$ 非常相似。
3.  **意义**：这一发现是论文提出 **跨层活跃权重预加载（Cross-layer active weight preloading）** 的基础。我们可以利用当前层（甚至前几层）的激活值，来高精度地预测后续层的活跃权重，从而实现计算与 I/O 的重叠。


<!-- Figure 4 未找到匹配图片 -->


<!-- Figure 5 未找到匹配图片 -->


**图表分析 (Figure 4 & Figure 5)**：
*   **Figure 5** 展示了简化的 Transformer 层结构，强调了残差连接将输入直接传递到输出的路径。
*   **Figure 4(a)** 展示了 LLaMA-2-7B 模型中 Attention 输入的跨层余弦相似度（红色）和 Top-K 稀疏度的重合精度（蓝色）。可以看到从第 3 层开始，相似度极高（>95%），Top-K 精度也超过 80%。
*   **Figure 4(b)** 解释了原因：LayerNorm 之后的激活值（蓝色）和权重（绿色）幅度较小，而 LayerNorm 之前的激活值（红色，即残差部分）幅度较大，主导了下一层的输入分布。

**观察二：解码过程中存在热点活跃权重（Hot Active Weights）**

论文还研究了活跃权重的时间局部性，即某些权重是否在解码过程中被频繁使用。

1.  **任务级 vs. 上下文级频率**：
    *   **任务级（Task level）**：在整个数据集的所有上下文中，权重被选中的频率。
    *   **上下文级（Context level）**：在针对**特定输入上下文**的解码过程中，权重被选中的频率。
2.  **结论**：上下文级别的热点权重选择概率极高（> 0.7）。这意味着对于同一个用户的同一段对话，活跃权重具有很强的时间局部性。
3.  **意义**：这启发了 **动态权重缓存（Dynamic Weight Caching）** 策略。与其每次都从 Flash 读取这些高频权重，不如将它们缓存在 DRAM 中，以提高缓存命中率并减少 I/O 开销。


<!-- Figure 6 未找到匹配图片 -->


**图表分析 (Figure 6)**：
该图展示了 Llama-2-7B 模型在 50% 上下文稀疏度下，Attention Q/K/V 算子中活跃权重的选择概率。
*   **橙色点（Context level）**：代表在特定上下文解码时的选择概率，明显高于蓝色点。
*   **蓝色点（Task level）**：代表在整个任务中的平均选择概率。
*   **结论**：利用上下文级别的信息进行缓存管理（Context-aware caching）比通用的缓存策略更有效，因为同一段对话中“热”的权重更加集中和稳定。

## 核心技术一：跨层活跃权重预加载与流水线

### 核心技术一：跨层活跃权重预加载与流水线

ActiveFlow 框架的核心在于解决移动设备上 DRAM 容量受限与 Flash I/O 带宽瓶颈之间的矛盾。为了实现高效的推理，论文提出了一套结合了预测、存储布局优化和流水线并行的综合方案。

### 跨层活跃权重预加载 (Cross-layer Active Weight Preloading)

传统的稀疏推理方法通常面临 I/O 效率低下的问题。由于活跃权重在内存中是不连续的，直接从 Flash 读取会导致大量的小块随机读取（Small Chunk Read），极大地降低了 I/O 吞吐量。为了解决这一问题，ActiveFlow 提出了跨层活跃权重预加载技术。

#### 1. 核心原理与预测机制
该技术基于论文在前文中提到的关键观察：由于残差连接（Residual Connection）的存在，相邻层之间的输入激活表现出极高的相似性（余弦相似度 > 80%）。这意味着当前层的激活值可以有效地预测后续几层的活跃权重。

- **层组（Layer Group）**：ActiveFlow 将连续的 $N$ 层定义为一个“层组”。
- **预加载策略**：在计算当前层时，系统利用当前层的激活值（Activation），一次性预测并加载下一个层组（Next Layer Group）中所有算子（包括 Attention 和 FFN）所需的活跃权重。
- **重叠执行**：这种预测机制允许数据的加载与当前的计算并行进行，从而隐藏 I/O 延迟。

#### 2. I/O 效率分析
从 Flash 读取数据的吞吐量与读取块的大小密切相关。


<!-- Figure 7 未找到匹配图片 -->


**

<!-- Figure 7 未找到匹配图片 -->

 分析**：
该图展示了不同设备（OnePlus 12, Pixel 6, Infinix Zero）上 Flash 读取吞吐量与 I/O Chunk Size 的关系。
- **观察**：当读取块大小（Chunk Size）较小（如 4KB，对应单个通道的权重）时，吞吐量仅为几百 MB/s，远低于设备的峰值带宽。只有当 Chunk Size 超过 64KB 时，才能接近 Flash 的峰值带宽（GB/s 级别）。
- **结论**：如果按需逐个加载活跃通道，I/O 将成为严重的瓶颈。ActiveFlow 通过预加载层组，增大了单次 I/O 的请求大小，显著提升了吞吐量。


<!-- Figure 8 未找到匹配图片 -->


**

<!-- Figure 8 未找到匹配图片 -->

 分析**：
该图直观展示了跨层预加载的流程。
- **左侧**：当前的激活（Current Activation）用于计算当前层。
- **右侧**：同时，该激活被用于索引并预加载未来 $N$ 层（Layer Group）的活跃权重。
- **关键点**：由于跨层相似性不是 100%，预加载可能会遗漏少量权重。对于这部分未命中的权重（约占总活跃权重的 5%），系统会在实际需要时触发**按需加载（On-demand loading）**。

---

### Flash 数据布局重排 (Flash Data Layout Reordering)

为了配合跨层预加载策略，使得物理存储上的数据布局能够支持大块顺序读取，ActiveFlow 对存储在 Flash 中的模型权重进行了重排。

#### 1. 打破张量边界
标准的 LLM 存储格式通常按层、按张量（Tensor）顺序存储。这种布局下，跨层的特定通道（Channel）在物理地址上相距甚远，无法通过一次 I/O 操作读取。

#### 2. 通道优先的重排策略
ActiveFlow 打破了张量和层的边界，在层组（Layer Group）内部重新组织数据。

- **重排顺序**：按照 **通道 ID (Channel ID) $\rightarrow$ 层 ID (Layer ID) $\rightarrow$ 算子类型 (Operator Type)** 的优先级进行排序。
- **效果**：对于同一个通道 ID，其在层组内所有层、所有算子对应的权重被连续存储在了一起。


<!-- Figure 9 未找到匹配图片 -->


**

<!-- Figure 9 未找到匹配图片 -->

 分析**：
该图对比了传统的权重布局与 ActiveFlow 的重排布局。
- **左侧（Reorder 前）**：权重按 $W_Q, W_K, W_V...$ 顺序存储。如果需要加载 Channel 0，需要在 $W_Q$ 处读一次，跳跃到 $W_K$ 处再读一次，导致多次小块 I/O。
- **右侧（Reorder 后）**：权重按 Channel 0 的所有相关数据（跨层、跨算子）聚合存储。
- **优势**：当系统预测 Channel 0 为活跃通道时，可以发起一次大的 I/O 请求，将该层组内 Channel 0 的所有权重一次性读入。这使得最小读取块大小（MinReadChunkSize）成倍增加，从而充分利用 Flash 带宽。

---

### 计算-加载重叠流水线 (Computing-Loading Overlap Pipeline)

ActiveFlow 设计了一个精密的流水线来协调计算与 I/O 资源，最大化重叠率。

#### 1. 四阶段流水线设计
流水线包含以下四个主要操作：
1.  **Computing (C)**：执行当前层组的矩阵计算。
2.  **Top-K (T)**：对激活值进行 Top-K 操作，提取活跃权重的索引（Mask）。
3.  **On-demand loading (L)**：加载当前层组中被预加载遗漏的权重（Cache miss 且 Preload miss）。
4.  **Preloading (PL)**：根据当前激活，预加载下一个层组的活跃权重。


<!-- Figure 10 未找到匹配图片 -->


**

<!-- Figure 10 未找到匹配图片 -->

 分析**：
该图展示了流水线的时间时序（Time Sequence）。
- **并行性**：在计算当前层组 $N$ 的 $Attention$ 和 $FFN$ 模块（$C_{QKV}^N, C_O^N...$）的同时，系统并行地执行下一个层组 $N+1$ 的预加载任务（$PL_{QKV}^{N+1}, PL_O^{N+1}$）。
- **按需加载**：在计算开始前，会有一个较短的按需加载阶段（$L_Q^N, L_K^N...$）来补充遗漏的权重，这部分开销较小。
- **目标**：理想状态下，预加载的时间 $T_{preload}$ 应完全被计算时间 $T_{comp}$ 覆盖，从而隐藏 I/O 延迟。


<!-- Figure 11 未找到匹配图片 -->


**

<!-- Figure 11 未找到匹配图片 -->

 分析**：
该图展示了 ActiveFlow 的系统架构与数据流。
- **Flash**：存储重排后的完整模型（按层组划分）。
- **DRAM**：分为动态权重缓存（Dynamic Cache）、预加载缓冲区和计算线程。
- **流程**：加载线程（Load Thread）负责将数据从 Flash 搬运到 DRAM，计算线程（Compute Threads）从 DRAM 读取活跃权重进行计算。两者通过同步机制协同工作。

#### 2. 系统成本模型与优化
为了在给定的内存预算 $M_{max}$ 下最小化解码延迟 $T_{decode}$，论文定义了详细的成本模型。

**目标函数**：
$$Minimize \quad T_{decode} = T_{load} + T_{overlap} + T_{comp}$$

其中：
- $T_{load}$：首个层组的加载时间。
- $T_{comp}$：层组的计算时间，由内存带宽决定：
  $$T_{comp} = \frac{M_{cl}}{BW_{mem}}$$
- $T_{overlap}$：重叠阶段的时间，包含按需加载和（预加载与计算的）最大值：
  $$T_{overlap} = T_{onload} + \max(T_{preload}, T_{comp})$$
- $T_{onload}$：按需加载时间，受限于小块读取的带宽 $BW_{flash}^{small}$：
  $$T_{onload} = \frac{S_l \cdot (1 - sp) \cdot (1 - hr) \cdot (1 - si)}{BW_{flash}^{small}}$$
  （其中 $sp$ 为稀疏度，$hr$ 为缓存命中率，$si$ 为跨层相似度）

**内存约束**：
$$M = M_{cl} + M_{cache} + M_{kv} \le M_{max}$$

**优化策略**：
ActiveFlow 通过贪心算法搜索最优参数配置。由于计算时间 $T_{comp}$ 与层组大小成正比，而预加载效率随层组增大而提高（Chunk 变大），系统会寻找一个平衡点（例如 $N=4$），使得 $T_{preload} \approx T_{comp}$，从而实现最大的流水线重叠效率。

## 核心技术二：动态缓存与自蒸馏优化

### 核心技术二：动态缓存与自蒸馏优化

本部分笔记主要聚焦于 ActiveFlow 框架如何通过动态资源管理和模型蒸馏技术，在受限的移动端内存中实现高效推理并保持模型精度。

### 弹性执行与动态权重缓存

为了在不同的硬件设备和内存预算下实现最优的推理性能，ActiveFlow 引入了一套弹性执行机制，能够根据当前系统状态动态调整配置参数。

#### 1. 弹性参数优化问题
ActiveFlow 将推理过程建模为一个受约束的优化问题。其目标是在满足严格内存限制（$M_{max}$）的前提下，最小化解码延迟（$T_{decode}$）。

需要优化的系统参数包括：
*   **稀疏度 ($sp$)**：决定了模型的活跃权重比例。
*   **跨层组大小 ($N$)**：决定了预加载的层数（Layer Group）。
*   **缓存大小 ($M_{cache}$)**：决定了 DRAM 中用于缓存热点权重的空间。

优化目标函数定义如下：

$$Minimize \quad T_{decode} = T_{load} + T_{overlap} + T_{comp} \tag{1}$$

约束条件为：

$$M \le M_{max} \tag{2}$$

其中：
*   $T_{load}$ 为首个跨层组的加载时间。
*   $T_{overlap}$ 为计算与加载重叠的时间。
*   $T_{comp}$ 为纯计算时间。
*   总内存消耗 $M$ 由三部分组成：跨层组内存 $M_{cl}$ + 权重缓存 $M_{cache}$ + KV Cache 内存 $M_{kv}$。

**贪婪搜索策略**：
为了求解上述优化问题，论文采用了贪婪策略：
1.  首先根据内存预算确定 **LLM 稀疏度 ($sp$)**，以确保在内存允许范围内获得最高精度。公式为：$sp = 1 - (M_{max}/S_m)$。
2.  其次，递归地增加 **跨层组大小 ($N$)** 以最小化解码时间。增加 $N$ 可以提高 Flash 读取的大块数据传输效率（带宽利用率高），从而减少预加载时间 $T_{preload}$。当 $T_{preload} \le T_{comp}$（完全掩盖加载延迟）或收益递减时停止增加。

#### 2. 基于 LFU 的动态缓存驱逐策略
基于论文中关于 "Contextual Hot Active Weights"（上下文热点活跃权重）的观察，即在解码过程中，某些权重通道会被频繁选中。ActiveFlow 设计了动态缓存机制来最大化缓存命中率。


<!-- Figure 12 未找到匹配图片 -->


**图片分析 (Figure 12)**：
该图展示了 LLM 解码过程中的动态权重缓存机制。
*   **左侧 (Initial)**：展示了权重的初始状态，假设有 8 个通道（Channel 0-7），DRAM 缓存容量仅能容纳 4 个通道（Channel 0, 2, 3, 5）。
*   **中间 (Token 0)**：
    *   **激活需求**：生成 Token 0 时，激活了通道 0, 1, 4, 6。
    *   **命中情况**：通道 0 在缓存中（命中），通道 1, 4, 6 需要从 Flash 加载（未命中）。此时命中率为 25%。
    *   **统计更新**：系统记录各通道的激活频率（Frequency）。
*   **右侧 (Token 1)**：
    *   **驱逐策略**：当缓存已满且需要加载新通道时，系统比较激活频率。例如，通道 1 的频率最低（假设），被新加载的通道 7 替换。
    *   **效果**：生成 Token 1 时，激活了通道 0, 4, 6, 7。由于之前的加载和缓存更新，此时通道 0, 4, 6 均在缓存中，仅通道 7 可能需要处理（如果之前未预取）。命中率显著提升至 75%。

**核心机制**：
*   **LFU (Least Frequently Used)**：系统维护每个权重通道的激活频率计数器。
*   **动态替换**：当新激活的通道不在缓存中时，如果其计数高于缓存中频率最低的通道，则执行驱逐替换。
*   **效果**：通过利用时间局部性，大幅减少了从 Flash 读取的数据量，缓解了 I/O 瓶颈。

---

### Top-K 稀疏感知自蒸馏 (Sparsity-aware Self-distillation)

尽管 Top-K 激活稀疏性优于基于 ReLU 的稀疏性，但在高稀疏度下（即保留极少权重），模型仍会面临信息丢失导致的精度下降问题。传统的微调（Fine-tuning）难以恢复这种因动态稀疏性导致的细微分布变化。

ActiveFlow 提出了一种 **稀疏感知的自蒸馏** 方法，作为量化和微调流程的扩展。

#### 1. 教师-学生网络架构与 KL 散度
该方法利用原始的密集模型（Dense Model）作为“教师”，稀疏模型作为“学生”。


<!-- Figure 13 未找到匹配图片 -->


**图片分析 (Figure 13)**：
该图展示了自蒸馏过程中的前向传播（Forward）和反向传播（Backward）流程。
*   **(a) Forward (前向传播)**：
    *   **Teacher Model**：输入 $x$，经过完整的权重计算，输出概率分布 $y_T$。
    *   **Student Model**：输入同样的 $x$，但在计算权重前，先通过 **Top-K Mask** 模块，仅保留幅度最大的 K 个激活值对应的权重进行计算，输出 $y_S$。
*   **(b) Backward (反向传播)**：
    *   **SD Loss (KLD)**：计算教师输出 $y_T$ 和学生输出 $y_S$ 之间的 KL 散度损失。
    *   **Gradient STE**：在反向传播经过 Top-K Mask 层时，使用直通估计器（Straight Through Estimator），允许梯度穿过零值区域回传给权重。

**KL 散度损失公式**：
为了使稀疏模型的输出分布尽可能逼近密集模型，使用了 Kullback-Leibler 散度作为损失函数：

$$\mathcal{D}_{KL}(P_T \parallel P_S) = \sum_{i} P_T(i) \log \frac{P_T(i)}{P_S(i)} \tag{10}$$

这种软标签（Soft Label）监督比硬标签能包含更丰富的类别相关性信息，有助于补偿稀疏化带来的信息损失。

#### 2. 梯度直通估计器 (STE)
在训练稀疏模型时，Mask 操作会将大量元素置为 0，导致反向传播时梯度消失（0 的导数为 0）。为了解决这个问题，ActiveFlow 采用了 **STE (Straight-Through Estimator)** 技术。

*   **前向传播**：应用 Mask 操作。
    $$y = \text{Mask}(x) \tag{11}$$
*   **反向传播**：将 Mask 操作视为恒等映射（Identity Function），直接传递梯度。
    $$\frac{\partial y}{\partial x} = I \tag{12}$$

这意味着即使某些权重在前向传播中未被激活（被 Mask 掉），它们在反向传播中仍然能接收到更新信号，从而加速收敛并保持权重的分布特征。

#### 3. "一次蒸馏，全尺度适应" (One-distill-all-scale)
这是该方法的一个显著特性。通常，不同的稀疏度水平需要分别训练不同的模型。但 ActiveFlow 发现：

*   **单一训练**：只需在一个固定的稀疏度水平（如 50%）下进行一次蒸馏。
*   **全尺度泛化**：蒸馏后的模型不仅在该稀疏度下表现良好，还能直接适应其他稀疏度水平（如 40% 或 60%），且无需重新微调。
*   **实验数据**：论文实验显示（Table 3），即使训练和推理的稀疏度差异达到 $\pm 15\%$，PPL（困惑度）误差仍保持在 1% 以内。

这一特性极大地降低了部署成本，使得同一个模型文件可以根据移动设备的实时内存状况，无缝切换不同的稀疏度模式。

### 实验与对比

在实验部分，论文使用了 **C4 数据集** 的子集（约 10B tokens）进行自蒸馏训练。
*   **硬件**：4张 A100 (80G) GPU。
*   **耗时**：对于一个 LLM，仅需约 10 小时。
*   **对比结果**：
    *   与直接应用 Top-K 稀疏性（如 TEAL）相比，ActiveFlow 在相同稀疏度下显著降低了 PPL。
    *   与 SOTA 的剪枝方法（如 CPSP, RIA）相比，ActiveFlow 处于 **Pareto Frontier**（帕累托前沿），即在相同的内存开销下能获得更好的模型质量，或在相同的质量下占用更少的内存。

## 系统实现与实验评估

### 系统实现与实验评估

本部分详细介绍了 ActiveFlow 的工程实现细节，并通过广泛的实验验证了该框架在不同移动设备上的端到端性能、技术有效性以及能效表现。

### 1. 系统实现细节 (System Implementation)

ActiveFlow 是基于 **llama.cpp** 框架构建的，针对移动端资源受限的特点进行了深度改造。

#### 1.1 核心架构与后端选择
*   **基础框架**：基于 llama.cpp 的 CPU 后端开发。
*   **设计考量**：
    *   虽然 NPU 在计算上可能更强，但在移动设备上，解码阶段（Decoding）主要受限于 **内存带宽（Memory Bandwidth Bound）**。
    *   移动设备通常采用统一内存架构（Unified Memory），CPU 在处理复杂的显存管理和数据加载逻辑上更具灵活性。
    *   先前的研究（如 T-MAC）也证明了 CPU 在低位宽 LLM 部署上的优越性。

#### 1.2 高效的 Flash I/O 管理
为了解决移动设备 Flash 存储随机读取性能差的问题，系统实现了以下优化：
*   **数据格式改造**：修改了 GGUF 模型格式，将模型权重按照 **跨层组（Cross-layer-group）** 的方式重新组织存储，而非传统的逐层存储。
*   **异步 I/O (io_uring)**：利用 Linux 的 `io_uring` 机制实现低开销的异步读取。
    *   使用 `io_uring_prep_read` 提交读取请求，使用 `io_uring_submit` 异步执行，并通过 `io_uring_wait_cqe` 同步结果。
*   **内存布局优化**：在读取稀疏的活跃权重时，将其加载到紧凑的密集缓冲区（Dense Buffer）中，并进行转置（Transpose）操作，以便于后续的量化计算。

#### 1.3 流水线并行与同步机制
为了实现计算与加载的完美重叠（Overlap），ActiveFlow 设计了精细的线程模型：
*   **线程分工**：
    *   **加载线程（Load Thread）**：绑定到 CPU 的小核（Little Core），负责从 Flash 读取数据。
    *   **计算线程（Compute Thread）**：绑定到 CPU 的大核（Big Core），负责执行矩阵乘法等计算。
*   **同步机制**：使用 **原子信号量（Atomic Semaphores）** 进行线程间通信。
    *   使用 `atomic_load_explicit` 和 `atomic_store_explicit` 来管理请求信号和完成信号。
    *   同步粒度为 **跨层组（Cross-layer-group）**，确保在计算当前组时，下一组的数据正在被预加载。

#### 1.4 动态缓存与自蒸馏实现
*   **动态缓存**：基于哈希表（Hash Table）管理权重张量。采用 **LFU（Least Frequently Used）** 策略，动态跟踪激活频率并驱逐最少使用的通道。
*   **自蒸馏模块**：基于 BitDistiller 框架开发。
    *   在前向传播中插入激活稀疏模块，根据阈值生成 Top-K 掩码。
    *   在反向传播中引入 **梯度直通估计器（Gradient STE）**，确保稀疏化后的梯度能够正常回传。

---

### 2. 端到端性能评估 (End-to-End Performance)

#### 2.1 实验设置

为了全面评估系统性能，实验选取了三种不同性能档次的移动设备和多种主流 LLM。

| 设备代号 | 型号 | 处理器 (CPU) | 内存 (RAM) | 存储 (Flash) | 最大读取带宽 |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **Device 1** | OnePlus 12 | X4+A720+A520 | 16 GB | UFS 4.0 | 5.8 GB/s |
| **Device 2** | Pixel 6 | X1+A76+A55 | 8 GB | UFS 3.1 | 4.2 GB/s |
| **Device 3** | Infinix ZERO 30 | A76+A55 | 8 GB | UFS 2.2 | 3.6 GB/s |

*   **模型**：LLaMA-2/3 (7B, 8B, 70B), Mixtral-8x7B。所有模型均采用 **4-bit 量化 (Q4_0)**。
*   **基线**：llama.cpp（全内存加载）、ProSparse（ReLU 稀疏）、TEAL（Top-K 稀疏）。

#### 2.2 解码速度与内存占用


<!-- Figure 14 未找到匹配图片 -->


**图片分析：**
图 14 展示了 ActiveFlow 与基线方法在不同设备和内存限制下的解码速度对比。
*   **低内存下的高性能**：在 Device 2 和 Device 3 上，ActiveFlow 在减少 **40%** 内存占用的情况下，达到了与全内存加载相当的性能。
*   **极端压缩下的加速**：当内存占用减少 **75%** 时，ActiveFlow 在 Device 2 和 Device 3 上分别实现了 $1.9 \times$ 和 $1.5 \times$ 的加速。这得益于计算-加载流水线有效地掩盖了 I/O 延迟。
*   **Mixtral-8x7B 的突破**：
    *   在 Device 2 (Pixel 6) 上，ActiveFlow 成功在仅 **2.9 GB** 内存占用的情况下运行了原本需要 >24 GB 显存的 Mixtral-8x7B 模型，实现了 **1.8 tokens/s** 的生成速度。这是首个在移动端成功部署原始 Mixtral 4-bit 模型（非 ReLU 改造版）的案例。

#### 2.3 困惑度 (PPL) 与 Pareto 前沿


<!-- Figure 15 未找到匹配图片 -->


**图片分析：**
图 15 展示了实际运行时内存占用与模型困惑度（PPL）的关系，即性能-成本的 Pareto 前沿。
*   **Pareto 最优**：ActiveFlow（Ours，蓝色/绿色/黄色线）始终位于曲线的左下角，意味着在相同的内存占用下，ActiveFlow 具有更低的 PPL（更高的模型质量）；或在相同的 PPL 下，ActiveFlow 占用的内存更少。
*   **对比 TEAL**：与同样采用 Top-K 稀疏的 TEAL 相比，ActiveFlow 在相同稀疏度下不仅内存占用更低，而且通过自蒸馏技术显著降低了 PPL 损失。

<Figure 14b> (引用自 End-to-End 图表中的 PPL 部分)
*   **下游任务准确率**：在 5-shot MMLU, GSM8K 等任务中，ActiveFlow 的自蒸馏版本比 TEAL 平均提升了 **2.64% 到 10.21%** 的准确率。

---

### 3. 消融实验与技术分析 (Ablation Studies)

本节深入分析了各个核心技术组件对系统整体性能的贡献。

#### 3.1 跨层预加载与流水线的贡献


<!-- Figure 16 未找到匹配图片 -->


**图片分析：**
图 16 分解了各项技术对 LLaMA-2-7B 解码速度的提升（在 60% 稀疏度下）。
1.  **基线 (Base)**：串行计算和读取。
2.  **+CrossLayer (N=1)**：当跨层组大小为 1 时，即仅预加载下一层，性能提升约 **10%**。此时受限于较小的 I/O 块大小，Flash 带宽未被跑满。
3.  **+CrossLayer (N=4)**：将跨层组大小增加到 4 层，性能提升达到 **120%**。这证明了通过聚合多层活跃权重来增大 I/O 块大小（Block Size）对于提升 Flash 读取效率至关重要。
4.  **+DynamicCache**：加入动态缓存后，最终实现了 **2倍 到 3倍** 的速度提升。

#### 3.2 跨层加载的延迟与相似度分析


<!-- Figure 17 未找到匹配图片 -->


**图片分析：**
*   **图 (a) 相似度影响**：当层间激活的余弦相似度（Cosine Similarity）大于 **0.4** 时，按需加载（On-demand load）的延迟显著降低，这意味着大部分活跃权重已被预加载覆盖。由于大多数层的相似度在 **0.8** 以上，这验证了跨层预加载的有效性。
*   **图 (b) 组大小影响**：随着跨层组大小（Layer#）从 0 增加到 4，预加载效率显著提升，总延迟大幅下降。虽然内存开销略有增加（红色柱状图），但换取了 $4.1 \times$ 的速度提升。

#### 3.3 缓存策略的有效性


<!-- Figure 18 未找到匹配图片 -->


**图片分析：**
*   **上下文级 vs. 任务级**：图 18 展示了 ActiveFlow 采用的 **上下文级（Context Level）** 缓存策略比传统的 **任务级（Task Level）** 统计具有更高的缓存命中率。在 BoolQ 任务中，上下文级缓存的命中率比任务级高出 **13%**。
*   **适应性**：上下文级策略能根据当前生成的 token 动态调整，平均命中率提升约 **12%**。


<!-- Figure 19 未找到匹配图片 -->


**图片分析：**
*   图 19 展示了不同缓存大小对 Flash 读取量的影响。
*   **I/O 缩减**：当缓存大小设为 **50%** 时，只有约 **18%** 的权重需要从 Flash 中读取。这意味着相比全量加载，内存访问量减少了 $5.2 \times$。
*   这证明了只需较小的 DRAM 缓存即可捕获大部分的“热”权重。

#### 3.4 功耗与能效分析


<!-- Figure 20 未找到匹配图片 -->


**图片分析：**
*   **功耗 (Power)**：相比基线 llama.cpp，ActiveFlow 的平均功耗降低了 **27.34%**。这主要是因为计算与加载的重叠减少了 CPU 的空转等待时间。
*   **能耗 (Energy Consumption)**：随着内存成本的降低（即稀疏度提高），每 token 的能耗显著下降。在 1.3GB 内存占用下，能耗减少了 **53%**。这表明 ActiveFlow 不仅能跑更大的模型，而且更省电。

#### 3.5 自蒸馏与 STE 的消融实验
实验表明（见论文 Table 5），移除梯度直通估计器（STE）或自蒸馏步骤均会导致 PPL 显著上升。
*   **One-distill-all-scale 特性**：ActiveFlow 的自蒸馏具有很强的泛化性。仅需在某一固定稀疏度（如 50%）下进行一次蒸馏，模型即可在其他稀疏度（如 60%, 70%）下保持良好的精度，无需为每个稀疏度单独训练。