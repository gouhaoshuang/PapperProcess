---
title: "ActiveFlow: LLM inference framework for adaptive DRAM usage"
authors: "Fucheng Jia, Huiqiang Jiang, Yunxin Liu, Zewen Wu, Qianxi Zhang, Ju Ren, Ting Cao, Shiqi Jiang, Yuqing Yang, Deyu Zhang"
created: "2026-01-20"
tags: [paper-notes, auto-generated]
status: "generated"
---

# ActiveFlow: LLM inference framework for adaptive DRAM usage

> **论文信息**
> - **作者**: Fucheng Jia, Huiqiang Jiang, Yunxin Liu, Zewen Wu, Qianxi Zhang, Ju Ren, Ting Cao, Shiqi Jiang, Yuqing Yang, Deyu Zhang
> - **核心贡献**: ActiveFlow是一个LLM推理框架，通过活跃权重DRAM-Flash交换、跨层活跃权重预加载和稀疏感知自蒸馏等技术，实现了自适应DRAM使用，从而扩展了移动设备上可部署的LLM模型大小，并在性能和成本之间实现了帕累托最优。


## Introduction

好的，我将根据您提供的大纲和要求，生成详细的论文笔记。


### Introduction

#### Background and Motivation

*   移动设备上部署大型语言模型（LLMs）面临 DRAM 容量限制：
    *   移动设备上的 DRAM 容量受限于功耗和面积，难以扩展，即使设备升级也难以增加 DRAM 容量。例如，iPhone 15 和 iPhone 16 都配备 8 GB DRAM。
    *   DRAM 的可用容量还受到同时运行的其他应用程序和操作系统进程的影响。在可用 DRAM 不足时，移动操作系统可能会终止应用程序，除非该应用程序可以减少内存使用。
*   现有 DRAM-Flash 交换技术不适用于 LLMs，因为 LLMs 的自回归解码阶段受限于内存访问：
    *   传统的 DRAM-Flash 交换技术依赖于传统 DNN 的计算密集型特性，使得当前算子的计算可以与下一个算子的数据加载重叠。
    *   虽然 LLM 的预填充阶段存在这种重叠，但自回归解码阶段的耗时更长，并且受限于内存访问，因此无法直接应用现有技术。
    *   LLM 推理需要尽量减少 Flash 数据加载，以缓解内存和 Flash 带宽之间的巨大差异（在移动电话上约为 5 倍）。
*   LLMs 的上下文稀疏性（contextual sparsity）为自适应内存管理提供了新的机会：
    *   LLM 的一个独特特性是上下文稀疏性，即虽然模型本身很大，但在每个 token 生成过程中，只有一小部分权重被激活使用，这些权重被称为“活跃权重”（active weights）。
    *   <图片 2> 展示了 LLaMa-2-70B 模型在解码过程中的上限稀疏性分析，结果表明，在每次推理迭代中，只需要激活少于 15% 的权重就可以生成相同的 token。

#### Challenges of Active Weight Swapping

*   如何准确识别活跃权重（active weights），因为上下文稀疏性是动态变化的：
    *   上下文稀疏性高度动态，在不同的 token、层和块之间变化。
    *   错误识别活跃权重可能会降低模型准确性。
*   如何尽早预测活跃权重，以便计算和加载重叠，并实现高效的大 I/O 传输：
    *   为了实现计算和加载的重叠，需要尽早预测活跃权重。
    *   高效的大 I/O 传输对于性能至关重要。
*   现有技术无法实现 LLMs 的自适应内存使用，因为它们依赖于 ReLU 或需要额外的预测器：
    *   一些方法（如 Deja Vu、PowerInfer 和 LLM in a flash）使用基于 ReLU 的模型来生成零激活，并引入额外的预测器（增加 GB 级别的内存开销）来预测这些零激活。
    *   但是，生产环境中使用的现代 LLM（如 LLaMA）很少使用基于 ReLU 的架构，因为其准确性较差（见 <图片 1> 和 Fig.14b）。
    *   其他一些工作通过持续的预训练将可用模型转换为基于 ReLU 或 ReLU 变体的模型，例如 PowerInfer-2、TurboSparse、ProSparse 和 Q-Sparse。这些工作需要在数千亿个 token 上进行训练，并消耗大量的硬件资源。
    *   还有一些工作（如 InfiniGen、NSA 和 SeerAttention）侧重于 KV 缓存稀疏性，而不是权重稀疏性。这些方法有利于长上下文场景（>32K），但在边缘设备上并不常见。
    *   TEAL 提出了一种无需训练、基于幅度的稀疏性方法（见 <图片 3>），其中只计算高于阈值的激活。但是，活跃权重无法被预测，只能在输入激活准备好之后才能被识别。此外，该方法是经验性的，并且没有机制来补偿由于潜在的活跃权重错误识别而造成的准确性损失。

#### ActiveFlow's Approach and Contributions

*   ActiveFlow 利用基于幅度的激活稀疏性（magnitude-based activation sparsity），适用于现代 LLMs：
    *   类似于 TEAL，ActiveFlow 利用基于幅度的、模型架构独立的激活稀疏性，以确保框架适用于现代 LLM。
*   ActiveFlow 包含跨层活跃权重预加载（cross-layer active weight preloading），稀疏感知自蒸馏（sparsity-aware self-distillation）和 DRAM-flash 活跃权重交换流水线（DRAM-flash active weight swapping pipeline）：
    *   **跨层活跃权重预加载**：利用当前层的激活来预先识别接下来 n 层的活跃权重，从而实现计算和加载的重叠。这是基于观察到由于广泛使用的残差连接，跨层的激活幅度分布具有显著的相似性（<图片 4>a 显示相似度 > 80%）。对于预加载遗漏的活跃权重，ActiveFlow 会在实际激活准备好时按需加载。
    *   **稀疏感知自蒸馏**：为了补偿近似，ActiveFlow 提出稀疏感知自蒸馏，以调整活跃权重，使其更接近密集模型的输出。这种蒸馏提高了稀疏率和模型准确性。
    *   **DRAM-flash 活跃权重交换流水线**：该流水线重新组织数据布局以进行跨层预加载，并将活跃权重的加载与当前层的计算重叠。它还集成了上下文热活跃权重缓存策略。该流水线根据可用内存来协调缓存、预加载的活跃权重和计算所涉及的权重之间的空间分配。
*   ActiveFlow 实现了用户无感知的自适应 DRAM 使用，并在性能和成本之间实现了帕累托最优：
    *   ActiveFlow 可以在不同的移动电话上进行评估（OnePlus 12、Pixel 6 和 Infinix Zero）。
    *   结果（<图片 1>，更多细节见第 7 节）表明，ActiveFlow 在现有效率优化方法（包括最先进的量化、剪枝和上下文稀疏性）中实现了推理性能-成本的帕累托最优，证明了其使用价值。
    *   特别地，在相同的模型质量和速度下，与 llama.cpp 相比，ActiveFlow 将 LLaMA 7B 的 DRAM 使用量减少了高达 40%。在相同的稀疏率下，与 TEAL 相比，ActiveFlow 可以将内存减少 2 倍。
    *   ActiveFlow 首次成功地将原始 Mixtural-8x7B 4bit 模型（未引入 ReLU）部署在 Pixel 6 手机上，以 2.9 GB 的内存成本实现了 1.8 tokens/s 的速度。
*   论文贡献总结：
    *   提出了 ActiveFlow，这是第一个 LLM 推理系统，通过活跃权重交换实现用户无感知的自适应 DRAM 使用。
    *   提出了跨层活跃权重预加载，以允许计算/加载重叠和大 I/O 传输。
    *   提出了稀疏感知自蒸馏，以补偿稀疏性引入的近似。
    *   实现了端到端的 ActiveFlow。结果表明，它在现有优化方法中实现了推理质量-成本的帕累托最优。


## Motivation and Background

好的，我已经理解了你的要求，现在开始根据提供的大纲和论文内容生成 Markdown 格式的笔记。


### Motivation and Background

#### Upper Bound Analysis of Contextual Sparsity in LLMs

*   **Contextual Sparsity（上下文稀疏性）**: LLMs 的一个显著特点是上下文稀疏性，这意味着在模型的众多权重中，只有一小部分权重在特定上下文中是活跃的。这些活跃权重足以生成与完整模型相似或相同的输出。换句话说，尽管 LLM 模型很大，但并非所有权重在每次推理时都被同等使用，只有与当前上下文相关的权重才会被激活。

*   **实验结果**: 论文通过实验分析了 Llama-2-70B 模型在解码过程中活跃权重的比例。实验通过逐步移除不重要的权重（每次移除 1%），并观察模型生成 token 的质量。权重的重要性通过公式 `$S_{ij} = |W_{ij}| \cdot |X_j|`$ 计算，其中 `$W_{ij}$` 是权重矩阵的元素，`$X_j$` 是输入激活向量的元素。实验结果表明，大多数 token 的生成只需要少于 15% 的权重，这说明 LLM 具有很高的上下文稀疏性，为优化推理过程提供了可能。具体可以参考 `<图片 2>`，该图展示了解码过程中 LLaMa-2-70B 模型的活跃权重上界分析。

*   **ReLU-based Sparsity vs. Top-K Sparsity**: 论文对比了两种实现稀疏性的方法：基于 ReLU 的稀疏性和基于幅度的激活稀疏性（Top-K sparsity）。
    *   **ReLU-based Sparsity**: 依赖于 ReLU 激活函数产生大量的零激活，从而减少计算量。但这种方法通常需要额外的预测器来预测这些零激活的位置，增加了额外的内存开销和计算负担。
    *   **Top-K Sparsity**:  基于激活值的幅度大小来选择活跃权重，即只保留幅度最大的前 K 个激活值对应的权重。这种方法不需要额外的预测器，且适用于所有线性变换算子，而不仅仅是 FFN 块。此外，Top-K sparsity 也更容易与模型量化等技术结合使用。

    论文指出，现代 LLM（如 LLaMA）很少使用 ReLU 架构，因此 Top-K sparsity 具有更广泛的适用性。`<图片 3>` 对比了 ReLU-based sparsity 和 Top-K activation sparsity。

#### Similarities in Cross-Layer Activations

*   **跨层相似性**: 论文的一个关键观察是，LLM 中 attention 块和 MLP 块的输入激活在不同层之间表现出高度的相似性。这意味着某一层的激活模式可以用来预测其他层的激活模式，从而为跨层优化提供可能。

*   **残差连接的作用**: 论文指出，残差连接（residual connection）是导致跨层相似性的主要原因。残差连接将前一层的输入直接添加到当前层的输出中，使得输入激活成为前一层输出和原始输入的叠加。由于原始输入在各层之间保持不变，因此激活模式也趋于相似。`<图片 5>` 展示了一个简化的 Transformer 层结构，其中可以清晰地看到残差连接。

*   **LayerNorm 和权重幅度的影响**:  LayerNorm 层可以减少激活幅度，而权重幅度通常小于激活幅度，这导致残差值大于 block 的输出激活值 `$F(X)$`。

*   **实验数据**: 论文通过实验验证了跨层相似性。`<图片 4>` 展示了 LLaMA-2-7B 模型中 attention 块输入激活的 cosine 相似度和 Top-K 稀疏精度。实验结果表明，从第三层开始，attention 块的 Q、K、V 以及 FFN 门控和 up 算子的输入激活具有超过 95% 的相似性，Top-K 稀疏精度也超过 80%。

*   **跨层预加载**:  基于跨层输入相似性的观察，论文提出了跨层预加载（cross-layer preloading）的思想，即使用当前层的激活来预测后续层的活跃权重，从而实现计算和数据加载的重叠，提高推理效率。

#### Contextual Hot Active Weights During Decoding

*   **热活跃权重（Hot Active Weights）**: 论文观察到，在 LLM 解码过程中，存在一些权重被频繁选择使用，这些权重被称为热活跃权重。这些权重在多次推理迭代中都会被激活，因此具有较高的时间局部性。

*   **时间局部性**:  热活跃权重的存在表明，可以通过缓存这些权重来提高缓存命中率，减少从 Flash 存储加载数据的次数，从而提高推理速度。

*   **任务级别 vs. 上下文级别**:  论文分析了两种不同级别的权重选择频率：
    *   **任务级别（Task Level）**:  统计在整个数据集（WikiText-2）上，权重通道在解码过程中被选择的频率。
    *   **上下文级别（Context Level）**:  统计在给定输入上下文的解码过程中，权重通道被选择的频率。

*   **实验结果**: 实验结果表明，上下文级别的热权重选择概率高于任务级别。`<图片 6>` 展示了 Llama-2-7B 模型中 attention Q/K/V 算子的活跃权重选择概率。结果显示，上下文级别的热权重选择概率超过 0.7，而任务级别超过 0.5。这表明，通过实现上下文缓存管理策略，可以提高缓存命中率，降低加载成本。

*   **上下文缓存管理策略**:  基于以上观察，论文提出了一种上下文缓存管理策略，即根据上下文动态调整缓存内容，优先缓存当前上下文中频繁使用的权重，从而提高缓存命中率，减少 Flash 存储的访问次数。


## Cross-layer Active Weight Preloading

好的，我已经理解了你的要求，现在开始根据提供的大纲和论文内容生成详细的笔记。

### Cross-layer Active Weight Preloading

#### Challenges and Existing Solutions

实现自适应 DRAM 使用的关键挑战在于两个方面：

1.  **权重加载与计算的重叠**: 是否能够使权重从 Flash 加载到 DRAM 的过程与模型的计算过程并行进行，从而隐藏 Flash 存储的读取延迟。
2.  **I/O 传输效率**: 如何确保数据传输能够充分利用 Flash 存储的带宽，避免因小块 I/O 导致的带宽浪费。

现有的解决方案，如 PowerInfer 等，仅部分地缓解了这些问题。它们通过在同一计算块内聚集协同活跃的权重通道，并尝试重叠每个聚集的簇的加载和计算过程。然而，这种方法的局限性在于：

*   **聚集效果**: 聚集的权重通道可能并不总是连续存储在 Flash 中，导致读取效率不高。
*   **适用性**: 这种方法可能只适用于某些特定类型的模型或层。

#### Cross-layer Active Weight Preloading Technique

为了克服上述挑战，本文提出了一种新的技术：**跨层活跃权重预加载 (Cross-layer Active Weight Preloading)**。该技术基于一个关键的观察：LLM 中不同层之间的激活值具有显著的相似性。

具体来说，该技术的工作方式如下：

1.  **利用跨层激活相似性**: 由于 LLM 中广泛使用的残差连接，当前层的激活值与后续层的激活值之间存在高度相关性。这意味着，可以使用当前层的激活值来预测后续层的活跃权重。
2.  **预加载机制**: 在计算当前层的同时，将接下来 N 层（称为“层组”）的活跃权重预先加载到 DRAM 中。这里的 N 是一个可配置的参数，可以根据可用的 DRAM 容量和计算延迟进行调整。
3.  **按需加载**: 尽管跨层激活相似性很高，但并非 100%。因此，预加载可能无法完全覆盖所有需要的活跃权重。对于预加载遗漏的权重，系统会在实际需要时进行按需加载。

**优点**:

*   **计算与加载重叠**: 预加载允许计算与数据加载并行进行，从而隐藏 Flash 存储的读取延迟。
*   **大块 I/O 传输**: 通过预加载多个层的权重，可以实现更大的 I/O 传输块，从而更有效地利用 Flash 存储的带宽。

参考 <图片 8>，该图展示了 Cross-layer active weight pre-loading 的过程。在计算当前层时，基于当前激活值，将接下来 N 层的活跃权重预加载到 DRAM 中。对于预加载遗漏的活跃权重，在实际激活值准备好后，进行按需加载。

#### Data Layout for Cross-layer Preloading

为了充分利用跨层预加载的优势，需要对 Flash 存储中的权重布局进行重新组织。传统的权重布局通常是按层和张量组织的，这对于按通道加载活跃权重来说效率较低。

因此，本文提出了一种新的数据布局方案：

1.  **打破层和张量边界**: 不再将权重按层和张量分开存储，而是将它们组织成连续的块，每个块包含多个层和张量的权重。
2.  **按通道、层和算子类型排序**: 在每个块内部，权重按照通道 ID、层 ID 和算子类型的顺序进行排序。例如，首先是所有层的第一个通道的 $W_q$ 权重，然后是所有层的第一个通道的 $W_k$ 权重，以此类推。

**优点**:

*   **提高加载效率**: 这种布局方式允许系统以更大的块读取权重，从而更有效地利用 Flash 存储的带宽。
*   **减少 I/O 操作**: 通过将相关的权重存储在一起，可以减少 I/O 操作的次数。

参考 <图片 9>，该图展示了在一个 4 层组中，权重重新排序的过程。传统的 LLM 权重布局是按层顺序排列每个权重张量。而本文提出的方法，根据通道 ID、层 ID 和算子类型的顺序，重新排列预加载层组内的权重通道。通过多层权重重新排序，增加了最小加载块大小，提高了加载效率。

通过这种数据布局，可以显著提高跨层预加载的效率，从而实现更快的 LLM 推理速度。

## Active Weight Swapping Pipeline

好的，我已经理解了你的要求，现在开始根据提供的大纲和相关信息生成详细的笔记内容。

### Active Weight Swapping Pipeline

#### Computing-Loading Overlapping Execution Pipeline

该部分介绍了一种用于 LLM 推理的计算-加载重叠执行流水线，旨在通过并发执行计算和数据加载来优化性能。该流水线由以下四个主要操作组成：

1.  **计算 (C)**：执行 LLM 推理所需的计算操作，例如矩阵乘法、激活函数等。
2.  **Top-K (T)**：从激活值中提取 Top-K 掩码，以确定激活的权重通道的索引。这一步骤是基于上下文稀疏性的思想，只保留最重要的权重进行计算，从而减少计算量和内存访问。
3.  **按需加载 (L)**：根据 Top-K 掩码，从 Flash 存储中加载当前层组 (Layer Group) 中激活的权重。由于 Flash 存储的访问速度相对较慢，因此需要尽量减少加载的数据量。
4.  **预加载 (PL)**：在当前层组进行计算的同时，预先从 Flash 存储中加载下一个层组的活跃权重。预加载的目的是为了隐藏 Flash 存储的访问延迟，使得计算和数据加载能够并行执行。

整个模型存储在 Flash 存储中，并采用跨层组布局。当前活跃的权重以及预加载和缓存的权重存储在 DRAM 中。计算和加载操作并发执行，以充分利用计算资源和内存带宽。

`<图片 10>` 展示了 LLM 推理的计算-加载重叠执行流水线在一个 attention block 中的具体流程。

`<图片 11>` 展示了 ActiveFlow 的权重布局和数据流。整个模型以跨层组布局存储在 Flash 中。当前活跃权重、预加载权重和缓存权重存储在 DRAM 中。计算和加载并发执行。

#### Elastic and Optimized LLM Execution

本节讨论了如何确定最佳系统参数，以在给定的移动设备上实现最佳的 LLM 推理性能。需要优化的参数包括：

*   **LLM 稀疏性 (Sparsity)**：指模型中零值的比例。更高的稀疏性可以减少计算量和内存占用，但可能会降低模型精度。
*   **跨层组的层数 (Layer Number of a Cross-Layer Group)**：指在预加载过程中，一次性加载的层数。更多的层数可以提高预加载的效率，但也会增加内存占用。
*   **缓存大小 (Cache Size)**：指用于存储频繁访问的权重的内存大小。更大的缓存可以提高缓存命中率，减少 Flash 存储的访问次数，但也会增加内存占用。

在这些参数之间存在权衡关系。例如，增加 LLM 稀疏性可以减少内存占用，但可能会降低模型精度。增加跨层组的层数可以提高预加载效率，但也会增加内存占用。因此，需要找到一个平衡点，以在给定的内存预算下实现最佳的推理性能。

为了解决这个问题，作者定义了一个优化问题，目标是最小化解码延迟，同时将内存成本作为硬约束。

**优化目标**：

`Minimize  T_{decode} = T_{load} + T_{overlap} + T_{comp}  (1)`

其中，`$T_{decode}$` 是解码延迟，`$T_{load}$` 是首次加载时间，`$T_{overlap}$` 是重叠时间，`$T_{comp}$` 是计算时间。

**约束条件**：

`$M \le M_{max}$  (2)`

其中，`$M$` 是内存成本，`$M_{max}$` 是内存预算。

解码延迟由三个部分组成：首次跨层组加载时间 `$T_{load}$`、跨层组重叠时间 `$T_{overlap}$` 和最终跨层组计算时间 `$T_{comp}$`，如公式 (1) 所示。加载时间 `$T_{load}$` 是未命中缓存的权重除以 Flash 加载带宽 `$BW_{flash}^{small}$`，如公式 (3) 所示。最终跨层组计算时间 `$T_{comp}$` 是组内存大小 `$M_{cl}$` 除以内存带宽 `$BW_{mem}$`，如公式 (4) 所示。此外，重叠时间由两部分组成，即按需加载时间 `$T_{load}$` 和预加载延迟 `$max(T_{preload}, T_{comp})$`，如公式 (5) 所示。我们加载跨层不相似但缓存中没有的权重，延迟为 `$T_{load}$`，如公式 (6) 所示。这些权重通常具有较小的块大小，导致较低的带宽 `$BW_{flash}^{small}$`。另一方面，预加载在跨层组级别加载权重，仅获取缓存未命中的权重（公式 (7)）。由于此阶段的块大小相对较大，因此读取效率更高，带宽为 `$BW_{flash}^{large}$`。

`$T_{load} = \frac{M_{cl} \cdot (1 - hr)}{BW_{flash}^{small}}$  (3)`

`$T_{comp} = \frac{M_{cl}}{BW_{mem}}$  (4)`

`$T_{overlap} = T_{onload} + max(T_{preload}, T_{comp})$  (5)`

`$T_{onload} = \frac{S_l \cdot (1 - sp) \cdot (1 - hr) \cdot (1 - si)}{BW_{flash}^{small}}$  (6)`

`$T_{preload} = \frac{M_{cl} \cdot (1 - hr)}{BW_{flash}^{large}}$  (7)`

内存成本也由三个部分组成：跨层组内存 `$M_{cl}$`、权重缓存内存 `$M_{cache}$` 和 KV 缓存内存 `$M_{kv}$`（公式 (8)）。对于 KV 缓存，我们只考虑固定大小的情况。因此，只有前两个组件会动态影响内存成本。跨层组内存是活跃权重的大小，如公式 (9) 所示。

`$M = M_{cl} + M_{cache} + M_{kv}$  (8)`

`$M_{cl} = S_l \cdot (1 - sp) \cdot N$  (9)`

#### Dynamic LLM Weight Caching

为了进一步减少加载的权重数量，作者设计了动态 LLM 权重缓存，其核心思想是利用 LLM 推理过程中存在的 "热权重 (Hot Weights)" 现象。热权重指的是在推理过程中被频繁访问的权重。通过将这些热权重缓存在 DRAM 中，可以减少对 Flash 存储的访问次数，从而提高推理速度。

为了实现动态权重缓存，系统需要跟踪激活的频率统计，并在在线阶段驱逐最少使用的权重。具体来说，系统为每个层的权重维护独立的计数器，确保所有权重之间的缓存大小平衡。当需要加载一个新的权重时，系统会检查缓存中是否存在该权重。如果存在，则直接从缓存中读取。如果不存在，则需要从 Flash 存储中加载该权重，并将其添加到缓存中。如果缓存已满，则需要驱逐一个最少使用的权重，以便为新权重腾出空间。

`<图片 12>` 展示了动态权重缓存的工作原理。

#### Self-Distillation for Top-K Sparse LLM

即使 Top-K 激活稀疏性具有优越的性能，但它仍然引入了近似，特别是在高稀疏性下。为了解决这个问题，作者提出了一种 Top-K 稀疏感知自蒸馏方法。该方法是一种量化和微调流水线的扩展，旨在提高高稀疏性下模型的精度。

自蒸馏的核心思想是利用一个 "教师模型 (Teacher Model)" 和一个 "学生模型 (Student Model)"。教师模型是一个精度较高的密集模型，而学生模型是一个稀疏模型。自蒸馏的目标是让学生模型学习教师模型的输出分布，从而提高其精度。

具体来说，自蒸馏的过程如下：

1.  使用教师模型生成训练数据的输出。
2.  使用学生模型生成训练数据的输出。
3.  计算教师模型和学生模型输出之间的 Kullback-Leibler 散度 (KLD)。
4.  使用 KLD 作为损失函数，训练学生模型。

通过最小化 KLD，可以使学生模型的输出分布尽可能接近教师模型的输出分布，从而提高学生模型的精度。

`<图片 13>` 展示了教师模型和学生模型之间自蒸馏的前向和后向过程。

**KL 损失函数**：

`$\mathcal{D}_{KL}(P_T \parallel P_S) = \sum_{i} P_T(i) \log \frac{P_T(i)}{P_S(i)}$  (10)`

其中 `$P_T$` 是教师模型的输出分布，`$P_S$` 是学生模型的输出分布。

**梯度 STE**：

前向传播：

`$y = \text{Mask}(x)$  (11)`

反向传播：

`$\frac{\partial y}{\partial x} = I$  (12)`

总而言之，Active Weight Swapping Pipeline 通过计算-加载重叠、弹性优化、动态缓存和自蒸馏等技术，实现了在内存受限的移动设备上高效运行大型语言模型的目标。

## Implementation

好的，我将根据您提供的大纲和要求，生成详细的 Markdown 格式的笔记内容。


### Implementation

本节主要介绍 ActiveFlow 框架在软硬件层面的具体实现细节，包括 Flash 加载、交换管道、缓存以及自蒸馏的实现。

#### Flash Loading Implementation

*   **GGUF 格式修改**: 为了支持跨层组（cross-layer group）的权重加载，论文修改了 GGUF 格式中权重张量的存储方式。传统的 LLM 权重布局是按层和张量顺序存储的，而 ActiveFlow 将每个算子的权重以跨层组的方式组织，以便更高效地进行通道级别的稀疏权重加载。
*   **IO uring 异步 I/O**: 为了加速 Flash 存储的读取，ActiveFlow 采用了 IO uring 这一低开销的异步 I/O 机制。IO uring 允许应用程序提交多个 I/O 请求到内核，而无需等待每个请求完成。这通过重叠 I/O 操作和计算来提高整体性能。具体实现中，使用 `io_uring_prep_read` 和 `io_uring_submit` 函数异步请求读取 active weights，然后使用 `io_uring_wait_cqe` 函数同步 I/O 操作。
*   **稀疏加载到密集缓冲区**: 从 Flash 读取的 active weights 通常是稀疏的，即只有部分通道是激活的。为了优化内存布局，ActiveFlow 将这些稀疏的通道加载到 DRAM 中的密集缓冲区中。这样做可以减少内存碎片，并提高后续计算的效率。
*   **转置操作以兼容量化**: 为了保证与量化技术的兼容性，ActiveFlow 在加载权重时应用了转置操作。这允许在读取通道时完整地检索必要的缩放因子，从而简化了量化过程。

#### Swapping Pipeline and Caching Implementation

*   **专用权重加载线程**: 为了实现计算和加载的重叠，ActiveFlow 创建了一个专用的权重加载线程。该线程负责从 Flash 存储读取 active weights，并将其加载到 DRAM 中。
*   **CPU 核心绑定**: 为了优化资源利用率，权重加载线程通过 `sched_setaffinity` 函数绑定到 CPU 的一个小核心（little core）。这样做可以将计算密集型任务分配给大核心（big core），而将 I/O 密集型任务分配给小核心，从而实现更好的负载均衡。
*   **原子信号量同步**: 权重加载线程和主计算线程之间通过原子信号量进行同步。原子信号量提供了一种线程安全的机制，用于在线程之间传递信号。ActiveFlow 使用 `atomic_load_explicit` 和 `atomic_store_explicit` 函数来管理请求信号和完成信号，以确保计算和加载操作的正确顺序。
*   **动态 LLM 权重缓存**: 为了进一步减少 Flash 存储的访问，ActiveFlow 实现了动态 LLM 权重缓存。缓存为每个权重张量单独管理，并使用哈希表来高效地查询缓存的权重通道。在解码过程中，动态跟踪权重通道的激活频率，并使用 LFU（Least Frequently Used）策略来替换缓存中不常用的通道。

#### Self-distillation Implementation

*   **BitDistiller 稀疏模块**: 为了在 ActiveFlow 中实现自蒸馏，论文在 BitDistiller 中开发了一个即插即用的稀疏模块。BitDistiller 是一个用于量化感知 LLM 蒸馏的开源框架，可以方便地集成到 ActiveFlow 中。
*   **激活稀疏模块**: 在每次 LLM 权重计算之前，插入一个激活稀疏模块。该模块预加载激活的稀疏阈值，并通过将激活与阈值进行比较，在推理时生成 Top-K 掩码。
*   **梯度 STE 层**: 在反向传播期间，为每个 LLM 权重合并一个梯度 STE（Straight-Through Estimator）层。STE 是一种用于训练稀疏神经网络的技术，它允许梯度通过稀疏掩码反向传播，从而避免梯度消失的问题。



希望这份笔记对您有所帮助！

## Evaluation

### Evaluation

#### Evaluation Setup

*   **硬件设备**: 该论文在三种不同的移动设备上评估了 ActiveFlow 框架，这些设备覆盖了从高端到低端的范围，如 Table 2 所示。评估使用了 OnePlus 12、Pixel 6 和 Infinix ZERO 30，它们具有不同的 CPU、内存和闪存配置。
*   **模型**: 为了评估端到端性能，该论文测试了流行的 LLM，包括 Llama 和 Mixtral 系列，模型大小从 7B 到 56B 参数不等。所有 LLM 都使用 Q4\_0 进行了 4 bit 量化，这是一种广泛使用的技术，对准确性的影响最小。
*   **基线**: 该论文将 ActiveFlow 与 llama.cpp 在解码速度和内存使用方面进行了比较。对于困惑度和准确性评估，使用了原始 LLM、ProSparse 和 TEAL 作为基线。ProSparse 和 TEAL 分别代表了最先进的 ReLU 稀疏和 Top-K 稀疏 LLM。

#### End-to-end Performance Results

*   **解码速度**: 如图 14a 所示，ActiveFlow 在不同的设备上实现了与全权重内存设置相同的性能，同时降低了 40% 的内存成本。例如，对于 Device 2 和 Device 3，使用 LLaMA-2-7B 模型时，实现了与全权重内存设置相同的性能，同时降低了 40% 的内存成本。
*   **内存成本降低**: 在降低 75% 的内存成本时，ActiveFlow 实现了比设备 2 和设备 3 上的全权重内存设置分别快 1.9 倍和 1.5 倍的速度。这种加速主要是由于计算-加载流水线，即使在较低的内存成本约束下也能实现更高的解码速度。
*   **Mixtral 模型解码**: ActiveFlow 成功地在 6GB 内存下实现了 Mixtral 模型的解码。当内存成本为 4.3GB 时，Device 1、Device 2 和 Device 3 上的解码速度分别为 1.3、1.0 和 0.4 tokens/s。当内存成本降低到 2.9GB 时，性能提高到 2.3、1.8 和 0.8 tokens/s，在三个设备上实现了 1.8 倍到 2.0 倍的加速。
*   **困惑度和下游任务**: 如图 14b 所示，ActiveFlow 表明，大型语言模型可以在显著降低内存成本的情况下保持较低的困惑度，例如，在仅使用 60% 内存的情况下，LLaMA-2-7B 和 LLaMA-3-8B 的性能与全权重设置相当，并且仅使用 4.4GB 即可匹配 Mixtral-8x7B 基线（24.6GB）。虽然在更积极的稀疏性下困惑度会增加，但自蒸馏策略有效地缓解了性能下降，在五个下游任务中实现了对 TEAL 的持续改进，在 70% 稀疏性下增益高达 10.98%，并且在表 4 中，跨稀疏性级别的平均改进范围为 2.64% 到 10.21%。

#### Technique Breakdown and Ablation Studies

*   **消融实验**: 为了验证系统中每个技术的有效性，进行了消融实验，评估了它们对解码速度、困惑度和命中率的影响。
*   **跨层组流水线**: 如图 16 所示，跨层组流水线提高了闪存内存读取的效率。当跨层组中的层数设置为 1 时，所有三个设备的平均加速为 10%。但是，将层数增加到 4 会导致 120% 的性能提升，因为它提高了闪存内存读取的效率。最后，通过添加动态缓存，该方法在三个设备上分别实现了比基线快 2 倍、2.3 倍和 3 倍的加速。
*   **跨层加载的权衡**: 如图 17 所示，评估了跨层加载的权衡。在图 17(a) 中，测量了当跨层组中的层数设置为 1 时，在不同余弦相似度值下，单层的加载和预加载开销。结果表明，当余弦相似度低于 0.2 时，预加载延迟低于按需加载延迟。但是，当余弦相似度超过 0.4 时，按需加载延迟低于预加载延迟。由于大多数层的余弦相似度高于 0.8，因此跨层方法有效地重叠了预加载和计算，从而优化了性能。
*   **不同层数的延迟和内存成本**: 在图 17(b) 中，评估了 LLaMA-2-7B 的 8 层解码器，测量了在跨层组中不同层数下的预加载、加载和总延迟以及内存成本。当层数为 0 时，计算和闪存加载按顺序发生，导致总延迟较高。当层数增加到 1 时，计算开始与预加载重叠，从而将总延迟降低了 52%。随着层数进一步增加到 4，改进的预加载效率使得速度比大小为 0 的设置提高了 4.1 倍。但是，增加层数也会导致更高的内存成本，从而带来额外的开销。总体而言，增加跨层组中的层数可有效提高解码性能，而额外的内存开销仍然相对较低。
*   **上下文缓存策略**: 如图 18 所示，上下文缓存策略提高了缓存命中率。在 BoolQ 上，当 token 长度 = 10 时，上下文缓存的命中率为 77%，比任务级别高 13%。随着长度增加到 40，命中率略微下降到 74%，但仍然高 10%。在跨下游任务中（图 18b），任务级别的命中率在 54-74% 之间变化，而上下文级别始终适应，平均提高了 12%。
*   **缓存效率**: 如图 19 所示，扩大缓存大小会显著减少闪存访问。使用 50% 的缓存时，闪存操作会减少到权重的 18%，与完全加载相比，内存访问减少了 5.2 倍。更大的缓存会进一步提高命中率，但也会增加内存占用；因此，根据可用的设备内存动态调整缓存大小。

#### Power and Energy Consumption

*   **功耗和能耗**: 如图 20 所示，ACTIVEFLOW 降低了平均功耗，并进一步降低了每个 token 的能量，因为内存成本降低了。由于重叠流水线中减少了计算等待时间，ACTIVEFLOW 与 llama.cpp 相比，平均功耗降低了 27.34%，并且随着内存成本的降低，进一步降低了每个 token 的能量，在 1.3GB 内存使用情况下，实现了高达 53% 的降低。

## Related Works

### Related Works

本文的 "Related Works" 部分主要讨论了与 ActiveFlow 相关的研究工作，涵盖了 LLM 稀疏性、高效 LLM 推理系统以及静态剪枝技术等方面。

#### Sparsity in LLMs

LLM (Large Language Model) 的稀疏性一直是研究的热点。稀疏性的目标是减少模型中的非必要参数，从而降低计算和存储成本，同时尽量保持模型的性能。

*   **研究现状**: 许多研究工作都集中在 LLMs 的稀疏性上，例如：
    *   Mirzadeh 等人 [19] 提出替换 LLMs 中的 ReLU 激活函数以减少计算和权重传输。
    *   HiRE [11] 引入了高召回率的近似 Top-K 估计。
    *   Prosparse [22] 利用 ReLU 和 FFN 中的门控分支的稀疏性来预测模型稀疏性。
    *   Q-Sparse [29] 从头开始训练稀疏 LLMs。
    *   TEAL [15] 应用基于幅度的稀疏性，无需重新训练。
*   **局限性**:
    *   一些方法依赖于基于 ReLU 的架构，但现代 LLMs (如 LLaMA 和 Mixtral) 更多采用非 ReLU 激活函数以提高准确性。
    *   某些方法缺乏在高稀疏性下恢复准确性的机制，导致模型性能下降。
*   **KV 缓存优化**: InfiniGen [13] 和 FlexGen [21] 等工作主要关注 KV 缓存优化，KV 缓存主要影响长文本场景（>32K tokens）的内存占用。本文则侧重于权重内存优化，权重内存通常由 LLM 的权重决定。

#### Efficient LLM Inference System

高效 LLM 推理系统的目标是在资源受限的设备上快速且低成本地运行大型语言模型。

*   **研究现状**: 一些系统级工作侧重于利用稀疏性来实现高效推理，例如：
    *   DejaVu [17] 使用轻量级算法预测上下文稀疏性。
    *   Alizadeh 等人 [3] 优化了有限内存设备上的推理。
    *   PowerInfer [23] (及其扩展 PowerInfer-2) 设计了 CPU-GPU 混合引擎。
*   **局限性**:
    *   这些工作主要针对基于 ReLU 的模型和 FFN 层，通常依赖于重型预测器（GB 级内存）来跳过零激活。
    *   现代 LLM（如 LLaMA 和 Mixtral）采用非 ReLU 激活以提高准确性 [27]，限制了这些方法的适用性。
*   **LLM-in-Flash**: LLM-in-Flash [1] 通过细粒度的预取从闪存中流式传输权重以减少 DRAM 使用，但其效率受到闪存带宽和延迟的限制，尤其是在计算密集型层中。

#### Our Distinction

ActiveFlow 的主要创新点在于解决了现有方法的局限性，实现了更高效的 LLM 推理。

*   **消除 ReLU 依赖和预测器开销**: ActiveFlow 通过针对现代非 ReLU LLM 中的所有权重（Attention 和 FFN）来消除 ReLU 依赖性和预测器开销。
*   **跨层活跃权重预加载**: 引入了跨层活跃权重预加载，利用跨层相似性来提前加载权重，从而实现计算和加载的重叠。
*   **稀疏感知自蒸馏**: 提出了稀疏感知自蒸馏，以在高稀疏性下恢复准确性。
*   **LFU 缓存**: 结合由激活统计驱动的基于 LFU (Least Frequently Used) 的缓存，始终实现更高的命中率（例如，>70% vs. ∼55%），并确保严格的内存预算，从而实现可靠的边缘部署。

#### Static pruning techniques

静态剪枝和量化是压缩大型语言模型的常用方法。

*   **研究现状**: 静态剪枝和量化是压缩大型语言模型的既定方法 (例如，CFSP [33], DB-LLM [6], 和 RIA [5])。
*   **局限性**: 这些静态方法需要离线处理模型权重，这限制了动态任务的灵活性。
*   **ActiveFlow 的优势**: ActiveFlow 不仅与这些静态技术兼容，而且还独特地支持动态处理以应对这一挑战。

## Conclusion

好的，我已经阅读了论文内容，现在开始根据提供的大纲段落生成详细的笔记内容。


### Conclusion

#### Summary of ActiveFlow

*   **ActiveFlow 是移动设备上的第一个 LLM 推理系统，支持自适应 DRAM 使用，以扩展可部署的模型大小**

    ActiveFlow 旨在解决移动设备上 DRAM 容量有限的问题，通过动态调整 DRAM 的使用，使得可以在移动设备上部署更大的 LLM 模型。 这种自适应性允许系统根据当前可用的 DRAM 容量进行调整，而无需用户手动干预。

*   **基于 DRAM 和闪存之间的活跃权重交换，集成了三种新技术：跨层活跃权重预加载、稀疏感知自蒸馏和活跃权重交换流水线**

    ActiveFlow 的核心思想是 "活跃权重 DRAM-Flash 交换"。 只有在推理过程中 "活跃" 的权重才会被加载到 DRAM 中，不活跃的权重则存储在闪存中。 这种方法通过以下三种技术实现：

    1.  **跨层活跃权重预加载 (Cross-layer active weight preloading)**： 利用当前层的激活值来预测后续几层的活跃权重，从而实现计算和数据加载的重叠，并促进更大的 I/O 传输。 这种预加载机制减少了推理过程中的等待时间。
    2.  **稀疏感知自蒸馏 (Sparsity-aware self-distillation)**： 调整活跃权重以对齐密集模型的输出分布，从而补偿上下文稀疏性引入的近似误差。 自蒸馏过程提高了模型的准确性，尤其是在高稀疏度下。
    3.  **活跃权重交换流水线 (Active weight DRAM-flash swapping pipeline)**： 根据可用内存，协调热权重缓存、预加载的活跃权重和计算涉及的权重之间的 DRAM 空间分配。 这种流水线优化了内存管理，确保高效的数据传输。

*   **与其他效率优化方法相比，实现了推理性能-成本帕累托前沿**

    实验结果表明，ActiveFlow 在推理性能和成本之间实现了最佳平衡，优于现有的量化、剪枝和上下文稀疏等优化方法。 这意味着在给定的性能水平下，ActiveFlow 可以降低内存成本，或者在给定的内存成本下，ActiveFlow 可以提高性能。

#### Impact and Future Directions

*   **打破了 LLM 部署的 DRAM 限制，为在移动设备上部署服务器级 LLM 开辟了新的机会**

    ActiveFlow 的成功表明，通过有效的内存管理和权重交换策略，可以在资源受限的移动设备上部署更大、更复杂的 LLM 模型。 这为移动设备上的 AI 应用开辟了新的可能性，例如更强大的自然语言处理、机器翻译和智能助手。

*   **未来的工作可以探索更高级的权重预测技术和更有效的缓存策略**

    为了进一步提高 ActiveFlow 的性能，未来的研究可以集中在以下几个方面：

    1.  **更高级的权重预测技术**： 提高活跃权重预测的准确性，减少预加载的错误率，从而减少按需加载的次数。 这可以通过使用更复杂的模型或利用更多的上下文信息来实现。
    2.  **更有效的缓存策略**： 优化热权重缓存的管理，提高缓存命中率，减少从闪存加载数据的次数。 这可以通过使用更智能的缓存替换算法或根据上下文动态调整缓存大小来实现。

*   **可以进一步优化 ActiveFlow，以支持更广泛的 LLM 架构和设备**

    ActiveFlow 目前主要针对 LLaMA 和 Mixtral 等模型进行了优化。 未来的工作可以扩展 ActiveFlow，以支持更多的 LLM 架构，例如 Transformer-XL、GPT-3 等。 此外，还可以针对不同的移动设备进行优化，以充分利用其硬件特性。
