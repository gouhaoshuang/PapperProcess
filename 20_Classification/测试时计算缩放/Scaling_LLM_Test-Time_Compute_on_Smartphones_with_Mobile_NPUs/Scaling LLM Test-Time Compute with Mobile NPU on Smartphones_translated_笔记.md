---
title: "Scaling LLM Test-Time Compute on Smartphones with Mobile NPUs"
authors: "Zi Xuhao, Huang Minxing, Wei Jianyu, Wang Tuowei, Jiang Huiqiang, Jiang Shiqi, Cao Ting, Ju Ren"
year: "2026"
created: "2026-01-20"
tags: [paper-notes, auto-generated]
status: "generated"
---

# Scaling LLM Test-Time Compute on Smartphones with Mobile NPUs


## 背景与动机：移动端测试时缩放的潜力

这是根据您的要求生成的关于“背景与动机：移动端测试时缩放的潜力”的详细笔记。

---

### 测试时计算缩放（Test-Time Scaling）

#### 测试时缩放的核心概念
**测试时计算缩放（Test-Time Scaling, TTS）** 是一种新兴的范式，旨在不改变模型参数量（即不进行更大规模的预训练）的前提下，通过增加推理阶段的计算投入来提升模型的生成质量。

传统的提升模型性能的方法通常是“模型缩放（Model Scaling）”，即使用更大的参数量（例如从 1B 扩展到 7B），但这会显著增加内存占用和带宽需求。相比之下，TTS 侧重于在运行时生成多个路径，并从中筛选出最佳答案。

#### 常见的缩放方法及其表现
论文主要关注两种在数学推理等复杂任务中表现优异的并行采样策略：

1.  **Best-of-N**:
    *   **机制**: 模型针对同一提示（Prompt）并行生成 $N$ 个独立的回答样本。然后使用验证器（Verifier）或奖励模型（Reward Model）对这 $N$ 个样本进行评分，选择得分最高的一个作为最终输出。
    *   **优势**: 在具有可验证结果的领域（如数学问题、编程），能够显著提升准确率。

2.  **Beam Search (波束搜索)**:
    *   **机制**: 类似于树搜索，在生成的每一步保留前 $k$ 个最优的候选路径（Top-$k$），并基于这些路径继续生成。
    *   **配合**: 通常结合过程奖励模型（Process Reward Model, PRM）来评估中间步骤的质量，动态丢弃低质量路径。


<!-- Figure 1 未找到匹配图片 -->


**图片 1 分析**:
该图直观地对比了两种测试时缩放方法：
*   **左侧 (Best-of-N)**: 展示了并行生成多个独立的完整路径（从提示到最终完成），最后选择一个“选定的完成（Selected completion）”。这种方法结构简单，易于并行化。
*   **右侧 (Beam Search)**: 展示了分步生成的树状结构。在每个步骤（层级），模型会评估多个候选节点，修剪掉（Pruning，橙色节点）低分路径，仅保留高分路径（绿色节点）进入下一轮生成。这种方法更节省计算资源，因为它尽早放弃了错误的路径。

#### 移动端应用的反直觉性
在直觉上，将 TTS 应用于智能手机似乎是不切实际的。
*   **资源受限**: 移动设备受限于电池容量和散热限制（Thermal constraints）。
*   **高消耗**: LLM 推理本身就是高能耗任务。
*   **矛盾**: TTS 的本质是“用更多的计算换取质量”，这通常意味着更高的延迟和能耗，看似与移动端追求低延迟、低功耗的目标背道而驰。

然而，论文通过实验证明，这种权衡在特定硬件条件下是可以打破的。


<!-- Figure 5 未找到匹配图片 -->


**图片 5 分析**:
该图展示了在 MATH500 数据集上，随着 **生成预算（Generation Budget，即解码阶段的最大批处理大小）** 的增加，模型准确率的变化趋势。
*   **趋势**: 无论是 Llama 3.2 还是 Qwen 2.5，随着并行采样数量的增加，准确率均有显著提升。
*   **结论**: 这证明了测试时缩放是提升小模型性能的有效手段，甚至可能让小模型达到大模型的性能水平。

---

### 移动NPU的架构特性与机遇

#### 移动NPU的典型架构
移动端 NPU（如高通 Hexagon NPU）为了追求极致的能效比，采用了与 GPU 截然不同的架构设计。它主要由两部分异构单元组成：

1.  **矩阵单元（Matrix Unit, HMX）**:
    *   专为加速矩阵乘法（GEMM）设计。
    *   提供极高的峰值计算吞吐量（例如 FP16 性能可达 12 TFLOPS）。
    *   **关键特性**: 基于 **Tile（瓦片）** 运行，例如处理 $32 \times 32$ 大小的矩阵块。

2.  **向量单元（Vector Unit, HVX）**:
    *   负责处理通用计算，如激活函数、归一化（LayerNorm）、Softmax 等。
    *   采用 VLIW（超长指令字）架构，计算能力和带宽远低于矩阵单元。


<!-- Figure 2 未找到匹配图片 -->


**图片 2 分析**:
该图对比了 GPU 和 NPU 的执行模型：
*   **(a) GPU (SIMT)**: 单指令多线程模型。线程可以灵活处理分支和内存访问，适合并行度高但不规则的任务。
*   **(b) NPU (SIMD)**: 单指令多数据模型。单个指令操作巨大的向量或矩阵块（如图中 $A_0-A_3$ 一起操作）。这种设计牺牲了灵活性（难以处理分支跳转），但换取了极高的能效和面效（Area Efficiency），非常适合神经网络中规则的张量运算。

#### 解码阶段的计算浪费与核心洞察
论文指出了 LLM 推理在移动 NPU 上存在严重的 **计算资源浪费** 现象，这正是引入 TTS 的契机。

*   **LLM 推理阶段**: 分为 Prefill（预填充，处理输入的 Prompt）和 Decode（解码，逐个生成 Token）。
*   **GEMV 退化问题**: 在 Decode 阶段，每次只生成 1 个 Token（Batch Size = 1）。此时，矩阵乘法（GEMM）退化为矩阵-向量乘法（GEMV）。
    *   权重矩阵形状：$[K, N]$
    *   激活向量形状：$[1, K]$
*   **硬件不匹配**: HMX 矩阵单元的基本计算粒度是 **$32 \times 32$** 的瓦片。
    *   当 Batch Size = 1 时，输入只有 1 行有效数据。
    *   **浪费**: 硬件必须加载整个 $32 \times 32$ 的瓦片进行计算，这意味着 $31/32$（约 97%）的算力在空转！

**核心洞察（Core Insight）**:
既然 NPU 在解码单个 Token 时有大量的算力被浪费，我们可以在不显著增加延迟的情况下，通过 **并行测试时缩放（Parallel Test-Time Scaling）** 来填充这些空闲的计算槽位。
*   如果我们将 Batch Size 从 1 增加到 8 或 16（用于并行采样），我们实际上是在利用那些原本被浪费的 HMX 计算能力。
*   理论上，计算 1 个 Token 和计算 16 个 Token 的矩阵乘法耗时在 NPU 上是几乎一样的。

---

### 面临的主要挑战

尽管利用空闲算力进行 TTS 理论可行，但在实际落地中面临三大挑战：

#### 1. 精度挑战：缺乏细粒度量化支持
*   **背景**: 移动设备内存有限，无法运行 FP16 模型，通常需要 4-bit 量化。
*   **需求**: 为了保持 LLM 在数学推理等复杂任务上的精度，必须使用 **细粒度组量化（Fine-grained Group Quantization）**（例如 group size = 128），而不是粗糙的逐通道（Per-channel）量化。
*   **冲突**: 移动 NPU（如 Hexagon）的设计初衷是加速传统的 CNN 模型，原生硬件只支持粗粒度的 INT8 量化。
*   **数据佐证**: 如表 1 所示，使用 QNN 框架的标准量化方案，Llama 3.2 1B 在 MATH500 上的准确率从 15.9% 跌至 2.1%，完全不可用。

#### 2. 效率挑战：向量单元瓶颈
*   **短板效应**: 虽然 HMX（矩阵单元）非常强大，但 NPU 的 HVX（向量单元）相对较弱。
*   **带宽差距**: HMX 可以通过 DMA 引擎获得 60 GB/s 的带宽，而 HVX 读取内存的带宽不到 30 GB/s。
*   **计算差距**: 单个 HVX 线程的 FP16 算力仅为 HMX 的 $1/300$。
*   **问题**: 在 TTS 场景下，除了矩阵乘法，还需要大量的非矩阵操作（如 Softmax、以及为了解决精度问题引入的 **反量化/Dequantization** 操作）。如果这些操作堆积在效率低下的 HVX 上，会成为严重的性能瓶颈，抵消 HMX 带来的加速。

#### 3. 软件栈局限性
*   **QNN 限制**: 高通的 AI Engine Direct (QNN) 是闭源的专有框架。
*   **不灵活性**: 它不支持自定义的高性能内核，且仅支持标准的量化模式（Per-tensor 或 Per-channel）。
*   **开发困境**: 研究人员无法直接通过 QNN 实现所需的细粒度分组量化算子，必须绕过标准栈，直接对 NPU 进行底层编程（利用 Hexagon SDK 和逆向工程指令）。

## 核心技术一：硬件感知的瓦片量化方案

### 核心技术一：硬件感知的瓦片量化方案

本节深入探讨了论文为解决移动端 NPU 硬件特性与现代 LLM 细粒度量化需求之间的不匹配而提出的核心解决方案。该方案通过重新设计数据的内存布局，实现了高效的内存访问和计算利用率。

### NPU 硬件约束分析

在移动端 NPU（特别是 Qualcomm Hexagon NPU）上实现高效推理，首先必须理解其独特的硬件架构限制。

#### Hexagon NPU 的内存层级与 HMX Tile 布局
Qualcomm Hexagon NPU 采用"向量 + 矩阵"的混合架构。其中，矩阵扩展单元（HMX）是算力的核心，但它对数据布局有严格要求：
1.  **基本计算单元（Tile）**：HMX 的基本操作对象不是标量或向量，而是一个 $32 \times 32$ 的矩阵块（Tile）。
2.  **存储占用**：每个 FP16 格式的 Tile 占用 2 KiB 的空间。
3.  **特殊的内存布局**：如图 4 所示，Tile 在内存中并非简单的行优先或列优先存储，而是采用了两级层级结构。
    *   **Level 1（外部布局）**：矩阵被划分为若干个 $32 \times 32$ 的块。
    *   **Level 2（内部布局）**：在每个块内部，每两行进行一次交叉洗牌（Shuffle），这种布局是为了适配硬件底层的内积操作电路。


<!-- Figure 4 未找到匹配图片 -->


**图 4 分析**：
*   该图展示了 FP16 HMX Tile 的内存物理布局。
*   **(a) Level 1**：显示了宏观上的分块策略，矩阵被分割成 $32 \times 32$ 的子块。
*   **(b) Level 2**：展示了子块内部的微观布局。可以看到数据并非线性排列，而是成对的行（2 rows）交织在一起。这意味着逻辑上相邻的元素在物理内存中可能并不相邻。

#### 传统布局与离散访问问题
传统的 CPU 或 GPU 推理（如 llama.cpp）通常假设权重矩阵采用**列主序（Column-major）**布局，并沿着列维度进行分组量化（例如每 32 个元素为一组）。
*   **冲突点**：传统的线性分组在逻辑上是连续的，但如果直接映射到 NPU 要求的 HMX Tile 布局中，这些逻辑上连续的"组"会被打散到物理内存的不同位置。
*   **后果**：这种不匹配导致 NPU 必须执行昂贵的**Gather（收集）**操作来从离散的内存地址中获取数据，严重降低了 TCM（紧耦合内存）的读写效率和 DMA 的带宽利用率。

### 瓦片-组量化（Tile-Group Quantization）

为了解决上述布局不匹配问题，论文提出了一种"顺应硬件"的量化策略，而不是让硬件去适配软件布局。

#### 离线权重重排策略
作者设计了一套离线的权重预处理流水线，包含以下步骤：
1.  **预量化变换**：在量化之前，先将权重矩阵的数据排列顺序调整为 HMX 单元所期望的 Tile 布局（即包含上述的 Level 1 和 Level 2 变换）。
2.  **瓦片内量化**：在新的内存顺序下进行分组量化。
3.  **后量化变换**：根据向量单元的访问需求进行最终的数据打包。

通过这种方式，运行时加载到 NPU TCM 中的数据在物理上已经是连续的，消除了运行时的重排开销。

#### Tile 内的量化执行
在重排后的布局中，量化不再沿着原始矩阵的长列进行，而是适应 Tile 的结构：
*   **分组策略**：对于大小为 32 的量化组，算法有效地以 $2 \times 16$ 的子块为单位进行分组。
*   **统计特性保持**：由于预训练模型的权重通常近似服从零均值的高斯分布，且是局部相关的。实验证明，将分组形状从 $1 \times 32$（列向量）变为 $2 \times 16$（小矩阵块）并不会显著改变组内的统计分布特性，因此量化误差保持在可接受范围内。


<!-- Figure 6 未找到匹配图片 -->


**图 6 分析**：
*   此图直观对比了布局不匹配带来的影响及解决方案。
*   **上半部分（Before）**：展示了如果坚持使用传统的列主序量化组（Quant Group 0），在映射到 HMX Tile 布局时，数据在片上内存（On-chip Memory）中是由于 Scatter 操作而处于"分散"状态的（红色块被隔开）。
*   **下半部分（After）**：展示了论文提出的方案。量化组（Group 1, Group 2...）直接按照 HMX 的预期顺序（HMX Expected Order）排列。这样，每个量化组在内存中都是连续存储的，NPU 可以进行高效的连续读取。

### 宽向量访问优化

除了解决矩阵单元（HMX）的布局问题，该方案还优化了向量单元（HVX）的内存访问效率。

#### SIMD 与细粒度量化组的不匹配
*   **硬件能力**：Hexagon NPU 的 HVX 向量寄存器宽度为 **128 字节（1024 位）**。
*   **数据粒度**：在 4-bit 量化且组大小为 32 的情况下，一组量化数据仅占用 $32 \times 0.5 = 16$ 字节。
*   **问题**：如果每次只读取一个量化组（16 字节），则向量加载指令的利用率极低（远小于 128 字节的带宽能力），且处理大量小数据块会引入额外的指令开销。

#### 超组（Super-group）聚合策略
为了充分利用宽向量寄存器，论文提出了**超组（Super-group）**的概念：
1.  **聚合**：将 8 个连续的细粒度量化组合并为一个"超组"。
2.  **容量匹配**：$8 \text{ groups} \times 16 \text{ Bytes/group} = 128 \text{ Bytes}$。这恰好填满一个 HVX 向量寄存器。
3.  **数据重组**：将这 8 个组的 256 个 INT4 元素重新打包，使得它们在加载时能一次性填满寄存器。

#### AoS 与 SoA 的权衡
在内存布局的选择上，论文针对 NPU 进行了特定的优化：
*   **避免纯 AoS（结构数组）**：默认的 AoS 布局会将量化值（INT4）和比例因子（Scale, FP16）交错存储。这对 NPU 的 DMA 预取不友好。
*   **避免纯 SoA（数组结构）**：完全将所有值和所有比例因子分开太远也不利于局部性。
*   **折中方案**：采用局部聚合的方式。在超组内部，量化数据被紧密打包在一起，适合向量单元的**vlut**（向量查找表）指令进行快速解码。


<!-- Figure 7 未找到匹配图片 -->


**图 7 分析**：
*   该图展示了超级组（Super-group）的打包过程。
*   **左侧**：8 个独立的细粒度量化组（Fine-grained Quant Groups），每个都很小。
*   **右侧**：这些组被重组（Re-packed）进一个 128 字节的块中。
*   **关键点**：注意图中底部的 **Register v0**，所有的 INT4 量化值（q0...q7）被连续放置，填满了整个 128B 的空间。而对应的比例因子（s0...s7）被单独提取出来，形成了紧凑的 16B 块。这种布局极大地提升了向量指令的吞吐量。

## 核心技术二：基于LUT的高效算子优化

基于论文《Scaling LLM Test-Time Compute on Smartphones with Mobile NPUs》，以下是关于核心技术二“基于LUT的高效算子优化”的详细笔记。

### 核心技术二：基于LUT的高效算子优化

为了在移动 NPU 上高效支持测试时计算缩放（Test-time scaling），仅仅优化矩阵乘法是不够的。随着推理计算量的增加（如 Batch Size 增大），通用算子（如 Softmax 和去量化操作）逐渐成为新的性能瓶颈。本文通过引入广义查找表（LUT）指令，巧妙地解决了 NPU 向量单元（HVX）计算能力不足的问题。

#### 1. 瓶颈分析：Softmax 与 FlashAttention

在传统的 LLM 推理中，矩阵乘法（GEMM）通常占据绝大多数计算时间。然而，在测试时计算缩放的场景下，情况发生了变化。

*   **测试时缩放的特征**：为了通过并行采样（如 Best-of-N 或 Beam Search）提升模型推理质量，系统通常需要使用较大的 **Batch Size**（例如 8 或 16）以及更长的上下文窗口。
*   **性能瓶颈转移**：
    *   **GEMM**：由于 NPU 拥有专用的矩阵加速单元（HMX），适度增加 Batch Size 并不会显著增加 GEMM 的延迟。
    *   **Attention**：Attention 机制的计算复杂度随着 Batch Size 和上下文长度的增加而显著增长。
    *   **Softmax**：在 FlashAttention 的实现中，Softmax 操作需要在 $\Theta(N \times L)$ 个元素上执行指数运算（Exp）。


<!-- Figure 8 未找到匹配图片 -->


**图片分析 (Figure 8)**：
该图展示了在 Hexagon NPU 上运行 FlashAttention 的延迟分解（Prompt 长度为 4096）。
*   **横轴**：Batch Size（查询长度）。
*   **纵轴**：各部分操作占总延迟的比例。
*   **结论**：随着 Batch Size 的增加（从 4 到 32），矩阵乘法（MatMul）的占比逐渐减小，而 **Softmax** 的占比迅速上升，最终主导了整个 Attention 的执行时间（占比高达 84.6%）。这表明 Softmax 中的指数计算是制约并行推理性能的关键瓶颈。

**根本原因**：
Hexagon NPU 的向量扩展单元（HVX）缺乏对特殊数学函数（如 $e^x$）的硬件支持。传统的软件实现通常采用多项式近似（如泰勒展开），这不仅计算量大，而且包含数据依赖链，限制了 VLIW（超长指令字）架构下的指令级并行度。

#### 2. 基于 LUT 的快速 Softmax 实现

为了克服上述硬件限制，论文提出用 **查找表（Look-Up Table, LUT）** 替代实时的指数计算。

**核心指令：`vgather`**
Hexagon NPU 提供了 `vgather` 指令，能够根据索引向量从 TCM（紧耦合内存）中的非连续位置收集数据到向量寄存器中。

**挑战与优化策略**：
*   **LUT 大小限制**：对于 FP16 数据类型，完整的查找表需要覆盖 $2^{16} = 65536$ 个条目，占用 128 KB 内存。然而，`vgather` 指令寻址范围有限，且过大的 LUT 会挤占 TCM 中用于存储矩阵块的空间。
*   **利用 Softmax 数学特性**：
    Softmax 函数具有平移不变性，即对于任意常数 $C$，有：
    $$ \text{Softmax}(x)_i = \frac{e^{x_i}}{\sum_j e^{x_j}} = \frac{e^{x_i - C}}{\sum_j e^{x_j - C}} $$
    为了数值稳定性，通常取 $C = \max(x)$。这意味着输入到指数函数的数值 $x_i - \max(x)$ 必然满足 **$\le 0$**。
*   **表大小缩减**：基于上述特性，我们只需要存储 FP16 中符号位为 1（负数）或数值为 0 的部分。通过忽略符号位并左移一位作为索引，LUT 的大小被缩减为 **32768 个条目（64 KB）**，完美适配 TCM 容量，仅占用总 TCM 的约 0.8%。

**计算流程与精度**：
1.  **输入**：FP16 格式的 Attention Logits。
2.  **预处理**：减去行最大值，确保所有输入 $\le 0$。
3.  **查表**：使用 `vgather` 指令直接获取 $e^x$ 的值。
4.  **精度保持**：虽然查表输入输出为 FP16，但在后续的求和（Sum）与归一化步骤中，累加器被提升至 **FP32** 以防止精度溢出或下溢。

**实验效果**：
相比于传统的 FP32 多项式近似计算，基于 LUT 的方法实现了 **1.26倍 ~ 2.19倍** 的加速，且由于预计算表可以使用高精度生成，其实际精度甚至优于 FP16 的多项式拟合。

#### 3. 以 LUT 为中心的去量化（Dequantization）

在混合精度推理中，权重以 4-bit（INT4）存储，但在计算时需转换为 FP16。由于 NPU 缺乏细粒度去量化的原生支持，这一过程在向量单元上的开销巨大。

**核心指令：`vlut16`**
该指令允许对源向量中的每个 8-bit 索引，在一个包含 16 个元素的微型表中进行查找，直接生成 16-bit 的结果。


<!-- Figure 9 未找到匹配图片 -->


**图片分析 (Figure 9)**：
该图对比了“朴素转换”与“基于 LUT 的转换”两种去量化流程。
*   **左侧（朴素转换）**：
    1.  **拆包**：将紧凑存储的 INT4 数据拆分为两个 INT8。
    2.  **类型转换**：INT8 $\rightarrow$ INT16 $\rightarrow$ FP16。
    3.  **数值修正**：减去零点偏差（如 -8）。
    4.  **转换**：最后得到 QF16/FP16。
    *该过程指令繁多，流水线长。*
*   **右侧（基于 LUT 的转换）**：
    1.  **直接查表**：使用 `vlut16` 指令。
    2.  **一步到位**：直接根据 4-bit 的值（作为索引），从预设的表中读出对应的 FP16 数值。
    *消除了所有的位掩码、移位、类型转换和算术指令，极大简化了计算图。*

**创新点：Scaling Factor 的广播优化**
在细粒度量化中，每组权重（如 32 个）共享一个 Scaling Factor。传统的广播方式需要将标量复制填充到整个向量寄存器。
*   **LUT 技巧**：作者将 4 组不同的 Scaling Factors 放入 LUT 中，然后构造一个固定的索引向量。
*   **效果**：通过一条 `vlut16` 指令，即可完成 4 组 Scaling Factor 到整个向量寄存器的广播，进一步减少了指令数。

**总结**：
通过将复杂的数学运算（Exp）和繁琐的位操作（Dequantization）转化为内存查找操作，该方案充分利用了 Hexagon NPU 的 `vgather` 和 `vlut16` 特性，有效规避了向量单元通用计算能力的短板，为端侧 LLM 的测试时缩放奠定了坚实的算力基础。

## 系统实现与评估

### 系统实现与评估

本部分详细阐述了论文提出的移动端 NPU 推理系统的实现方案、实验设置以及全方位的性能评估。作者不仅展示了系统在吞吐量上的提升，更核心地验证了“测试时计算缩放”（Test-Time Scaling）在移动设备上的可行性与高效性。

#### 实现细节与实验设置

为了克服现有专有框架的局限性并充分释放 NPU 性能，作者设计了一套定制化的推理系统。

**1. 基于 llama.cpp 与 Hexagon SDK 的无 QNN 依赖实现**
传统的移动端 NPU 开发通常依赖高通的 QNN（Qualcomm AI Engine Direct）框架，但 QNN 闭源且主要支持静态计算图，难以满足 LLM 推理中动态形状（Dynamic Shape）的需求。
*   **自主后端开发**：作者在开源项目 `llama.cpp` 的基础上，利用 Hexagon SDK（包含 LLVM 工具链）直接编写 C/C++ 和内联汇编代码，构建了一个独立的 Hexagon NPU 后端。
*   **灵活性优势**：这种“无 QNN 依赖”的设计使得开发者能够自定义底层算子（如前文提到的瓦片量化 GEMM 和 LUT Softmax），绕过了专有软件栈对算子形状和精度的限制。

**2. 利用 rpcmem 共享内存减少 CPU-NPU 数据传输开销**
在异构计算中，主机（CPU）与加速器（NPU）之间的数据传输往往是性能瓶颈。
*   **零拷贝通信**：系统利用 `rpcmem`（底层基于 dmabuf）实现了 CPU 和 NPU 之间的物理内存共享。相关的内存分配与映射通过 `libcdsprpc` 接口完成。
*   **缓存一致性维护**：由于 Snapdragon SoC 上的 CPU 和 NPU 之间仅存在单向一致性或无硬件一致性，作者在 CPU 写入数据后，手动执行缓存清除（Cache Flush）操作，随后 NPU 直接读取共享内存区域，消除了不必要的数据拷贝。

**3. 实验平台与模型配置**
为了全面评估系统的兼容性与性能，作者在三代高通旗舰平台上进行了测试：

| 设备型号 | SoC 芯片 | NPU 架构版本 | 备注 |
| :--- | :--- | :--- | :--- |
| 一加 Ace3 | Snapdragon 8 Gen 2 | V73 | 存在 32 位地址空间限制，无法运行 >3B 模型 |
| 一加 12 | Snapdragon 8 Gen 3 | V75 | 主力测试平台 |
| 一加 Ace5 Pro | Snapdragon 8 Elite | V79 | 最新一代平台 |

*   **测试模型**：主要选用 **Qwen 2.5** (1.5B, 3B) 和 **Llama 3.2** (1B, 3B)。这些模型尺寸适合移动端部署。
*   **基准对比**：
    *   **GPU 基线**：`llama.cpp` 的 OpenCL 后端（针对 Adreno GPU 优化的 Q4_0 内核）。
    *   **QNN 参考**：虽然 QNN 精度不足，但也列出了其 FP16 性能作为参考。

---

#### 整体性能表现

这一部分主要回答了“在移动端进行测试时缩放是否划算”的问题。

**1. 精度-延迟权衡：测试时缩放实现了新的帕累托前沿**

作者展示了通过增加推理计算量（即测试时缩放）来换取更高生成质量的结果。


<!-- Figure 10 未找到匹配图片 -->


*   **图片分析**：
    *   图表展示了 **Best-of-N**（上排）和 **Beam Search**（下排）两种策略在 MATH500 和 GSM8K 数据集上的表现。
    *   横轴为平均解码延迟（成本），纵轴为准确率（收益）。
    *   **关键发现**：曲线向左上角凸起，形成了优于基线模型的帕累托前沿（Pareto Frontier）。这意味着在相同的延迟预算下，测试时缩放能提供更高的精度；或者在相同的精度要求下，它能以更低的延迟实现。

**2. 关键结论：小模型 + 测试时缩放 > 大模型 + 常规解码**
实验数据有力地支持了一个反直觉的结论：
*   **以小博大**：例如，Qwen2.5-1.5B 模型配合 Best-of-N 策略（Batch Size > 1），其数学推理准确率可以超过直接运行 Qwen2.5-3B 甚至 7B 模型的基线准确率。
*   **成本优势**：由于 NPU 并行计算的高效性，1.5B 模型并行解码 8 个样本的能耗，竟然低于 3B 模型串行解码 1 个样本的能耗。这证明了移动端资源受限环境下，"小模型 + 强搜索"是一条比单纯"增大模型"更优的路径。

**3. 解码吞吐量随 Batch Size 的扩展性分析**


<!-- Figure 11 未找到匹配图片 -->


*   **图片分析**：
    *   图表展示了不同设备（8G2, 8G3, 8G4）上，解码吞吐量（Tokens/s）随 Batch Size（1 到 16）的变化趋势。
    *   **线性增长**：随着 Batch Size 增加，吞吐量呈现近乎线性的增长。例如在 Snapdragon 8 Gen 4 上，Batch Size 从 1 增加到 16，吞吐量提升显著。
    *   **原因解析**：这是因为在 Batch=1 时，NPU 强大的矩阵乘法单元（HMX）处于极度“吃不饱”的状态（GEMV 操作）。增加 Batch Size 将计算转变为 GEMM，有效利用了原本浪费的算力，而几乎不增加额外的计算时间。
    *   **现有瓶颈**：增长并非完美的纯线性，主要受限于 CPU 端的 `lm_head`（词表投影）计算。由于 NPU 地址空间限制，部分大张量仍需 CPU 处理。

---

#### 功耗分析与消融实验

为了验证系统的实用性，作者进一步分析了能效比，并通过消融实验量化了各项优化技术的贡献。

**1. 能效比分析：利用闲置算力**


<!-- Figure 12 未找到匹配图片 -->


*   **图片分析**：
    *   左图（功率）：随着 Batch Size 增加（1 到 16），瞬时功率确实有所上升（例如 Qwen2.5-1.5B 从 <4W 升至 ~4.8W），但始终保持在移动设备可接受的热功耗范围内（<5W）。
    *   右图（能量）：这是更关键的指标（功率 $\times$ 延迟）。由于吞吐量大幅提升，解码每个 Token 的平均能耗并没有因为并行采样而爆炸式增长。
    *   **结论**：在移动 NPU 上，利用“闲置算力”进行并行采样是非常经济的。

**2. 消融研究：硬件感知布局带来的 GEMM 加速**

作者对比了不同优化阶段的 GEMM 算子性能，验证了“硬件感知瓦片量化”的有效性。


<!-- Figure 15 未找到匹配图片 -->


*   **图片分析**：
    *   **基线（蓝色）**：使用常规内存布局，运行时需要大量的 `scatter`（分散）操作来适应 NPU 瓦片格式，性能极差。
    *   **w/HMX 布局（绿色）**：仅应用静态的权重重排，减少了部分开销。
    *   **Ours（红色）**：结合了 HMX 布局 + 瓦片量化 + 量化组聚合。性能相比基线实现了 **9.65 倍到 19.04 倍** 的惊人加速。
    *   **No Dequant（橙色）**：这是理论性能上限（假设去量化不花时间，纯内存拷贝）。"Ours" 的性能已经非常接近这一上限，说明作者基于 LUT 的去量化几乎消除了计算开销。

**3. LUT Softmax 带来的 Attention 算子加速效果**

针对 Attention 模块中的 Softmax 瓶颈（主要是指数运算 `exp`），作者提出了基于查表（LUT）的方案。


<!-- Figure 14 未找到匹配图片 -->


*   **图片分析**：
    *   图表对比了 F32 exp（标准实现）、F16 exp（多项式近似）和 LUT16（本文方法）的延迟。
    *   **性能提升**：在不同的上下文长度（KV length）下，LUT 方法相比常规 F32 exp 实现了 **1.26 倍到 2.19 倍** 的加速。
    *   这证明了在通用计算能力较弱的向量单元（HVX）上，用内存查找代替复杂算术运算是极其有效的策略。