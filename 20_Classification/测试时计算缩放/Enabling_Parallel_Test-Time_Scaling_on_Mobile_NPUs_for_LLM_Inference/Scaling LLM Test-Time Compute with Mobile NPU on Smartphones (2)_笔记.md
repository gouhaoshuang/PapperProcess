---
title: "Scaling LLM Test-Time Compute with Mobile NPU on Smartphones"
authors: "Zixu Hao, Jianyu Wei, Tuowei Wang, Minxing Huang, Huiqiang Jiang, Shiqi Jiang, Ting Cao, Ju Ren"
year: "2026"
created: "2026-01-19"
tags: [paper-notes, auto-generated]
status: "generated"
---

# Scaling LLM Test-Time Compute with Mobile NPU on Smartphones

> **论文信息**
> - **作者**: Zixu Hao, Jianyu Wei, Tuowei Wang, Minxing Huang, Huiqiang Jiang, Shiqi Jiang, Ting Cao, Ju Ren
> - **年份**: 2026


## Introduction


### Introduction

本文针对在移动设备上部署大型语言模型 (LLM) 所面临的挑战，即小模型性能不足和大模型资源消耗过多的问题，提出了利用移动端神经处理单元 (NPU) 的并行测试时计算缩放 (Test-Time Scaling) 技术来提升小模型性能的方法。

#### 背景

*   **移动端 LLM 部署挑战**: 在移动设备上部署 LLM 面临着小模型性能不足和大模型资源消耗过多的问题。
*   **测试时计算缩放 (Test-Time Scaling)**: 一种通过增加推理时计算量来提升 LLM 性能的新范式。并行测试时缩放方法涉及生成多个路径，并从中选择最佳样本。
*   **移动端 NPU 的潜力**: 移动片上系统 (SoC) 中集成的 NPU 具有较高的峰值计算能力，但在典型的 LLM 推理过程中，其矩阵乘法单元的利用率较低，存在闲置算力。

#### 核心思想

利用移动端 NPU 的闲置算力，通过并行测试时计算缩放技术来提升小模型的性能，同时实现更优的性能-成本权衡。

#### 主要技术

为了克服 NPU 在精度和效率方面的挑战，本文提出了以下关键技术：

*   **硬件感知量化方案 (Hardware-aware Tile Quantization Scheme)**: 一种将分组量化与 NPU 内存访问模式对齐的方案，通过权重布局转换，在硬件友好的 tile 上应用细粒度分组量化，从而最小化运行时内存访问开销，最大化向量计算利用率。
*   **高效的基于 LUT 的计算 (Efficient LUT-Based Computation)**: 使用高效的查表 (LUT) 操作替换复杂的运算，例如 Softmax 和反量化，从而缓解向量单元上的计算瓶颈。

#### 实验结果

实验结果表明，所提出的方法在高通骁龙平台上实现了显著的加速：

*   混合精度 GEMM 加速高达 19.0 倍。
*   Softmax 加速高达 2.2 倍。
*   使用测试时缩放的小模型可以达到甚至超过大型模型的精度，实现了新的性能-成本 Pareto 前沿。

#### 贡献

*   分析了现代移动 NPU 的架构，并指出了 LLM 解码阶段专用矩阵单元的利用不足。
*   提出了硬件感知 tile 量化方案和基于 LUT 的计算两种技术，以加速移动 NPU 上的 LLM 测试时缩放。
*   设计并实现了一个端到端的 LLM 推理系统，该系统利用移动 NPU 支持测试时缩放工作负载，且对专有软件栈的依赖性最小。
*   证明了测试时缩放可以有效地利用 NPU 的闲置计算能力，从而提高设备上小型语言模型的生成质量，与传统的模型缩放相比，在准确性和成本方面实现了 Pareto 前沿性能。 这为在移动设备上部署 LLM 开辟了新的机会。


## Background


## Background

本节主要介绍了测试时计算缩放（Test-Time Scaling）和神经处理单元（NPU）的相关背景知识，为后续提出的优化方案奠定基础。

### Scaling LLM Computation at Test-Time

测试时计算缩放是一种新兴的、有效的提升模型精度的方法，它不修改模型参数，而是在测试时投入更多的计算资源。

#### 典型方法

*   **Majority-voting 和 Self-consistency**:  从多个生成的样本中选择最一致的答案。
*   **Best-of-N**:  对于有可验证结果的数学或编程问题，或者具有奖励模型的领域（例如，Outcome Reward Models），从完成的样本集中选择得分最高的选项。
*   **Process Reward Models (PRMs)**:  类似于 Monte Carlo Tree Search (MCTS) 的方法，从部分生成的序列中选择最优路径。PRM 可以直接对中间结果进行评分。
*   **Beam Search**:  在 PRM 的辅助下，动态地丢弃低质量的生成路径，以平衡探索和利用。

<图片 1> 展示了 Best-of-N 和 Beam Search 两种典型的测试时计算缩放方法的算法流程。

### Neural Processing Units

随着 AI 工作负载的增长，现代 SoC 越来越多地集成 NPU 以加速神经网络推理。NPU 专门加速低精度、计算密集型的核心神经网络操作（例如，GEMM），从而在保持良好功率效率的同时提供极高的计算吞吐量。

#### 架构特点

一种广泛采用的 NPU 架构采用 "向量 + 矩阵" 组合，其中矩阵单元加速矩阵乘法和卷积等操作，向量单元处理通用计算，例如归一化和复杂的激活函数。 常见的例子包括 Qualcomm 的 Hexagon NPU、华为的 Ascend NPU、AMD 的 XDNA NPU、Intel NPU 和 Intel 的 Gaudi HPU。

#### 执行模型

NPU 在硬件执行模型上与常见的 GPU 显著不同。如 <图片 2> 所示，在 GPU 的 SIMT 模型中，不同的线程可以独立地执行分支、内存访问和计算，而在 NPU 的基于 SIMD 的执行模型中，单个线程对大的向量或矩阵数据块进行操作。 在硬件层面，NPU 通常采用较少的硬件线程，并使用 VLIW 架构来减少控制逻辑开销。与 GPU 相比，NPU 牺牲了编程灵活性和易用性，以换取更高的执行效率和能源效率。


## Motivation and Challenges


### Motivation and Challenges

本节深入分析了在移动端 NPU 上实现 LLM 测试时计算缩放所面临的机遇与挑战，主要集中在高通骁龙 NPU 的架构特性上。

#### Opportunities: Free Matrix Computation During LLM Decoding

在自回归生成过程中，LLM 的输入通常对应于单个 token，导致 GEMM 操作退化为 GEMV。 例如，形状为 `[1, hidden_dim]` 的激活矩阵与形状为 `[hidden_dim, proj_dim]` 的权重矩阵相乘。 在使用 FP16 HMX 的情况下，每个计算 tile 的有效大小为 `[1,32]×[32,32]`。 由于硬件计算的基本单元是 32 × 32 tile，因此输入激活 tile 中的 31 行不对应于实际有用的内容，从而导致矩阵单元的利用率低和计算能力的浪费。

同时，一些测试时计算缩放算法可以通过增加生成过程中的计算来实现更好的生成质量，包括并行采样方法，例如 Self-Consistency、Best-of-N 和 Beam Search。 它们的特点是使用大于 1 的 batch size 探索多个生成路径，并使用某些方法（例如，外部验证器）来选择更好的生成路径。

<图片 5> 展示了使用 Best-of-N 进行测试时计算缩放的示例。 随着生成预算（即解码阶段的最大 batch size）的增加，MATH500 数据集上的模型准确性显着提高。

基于这些，作者提出在移动 NPU 上运行 LLM 的测试时计算缩放工作负载。 这样，可以有效地利用在传统 LLM 生成过程中浪费的 NPU 计算能力。 理论上，解码开销不会显着增加，并且可以在运行时提高模型的生成质量，而无需修改模型权重。

#### Challenges

尽管在理论上利用移动 NPU 进行测试时计算缩放是可行的，但有效的实现面临着许多硬件挑战。 主要概括为以下几点：

1.  **Insufficient Precision (精度不足)**:

    HMX 单元虽然支持 FP16 GEMM，但在资源受限的设备上部署 FP16 模型仍然不切实际，因此量化模型是典型的替代方案。 大多数移动 NPU（包括 HMX）中的矩阵单元最初旨在加速采用粗粒度量化方案（例如，per-tensor 或 per-channel 量化）的整数量化 DNN 模型。 作为一个具体的例子，Hexagon NPU 缺乏对现代 LLM 部署中必不可少的细粒度 group quantization 的原生硬件支持。 这种限制进一步反映在软件堆栈中：QNN 仅支持 per-tensor 或 per-channel 权重量化。 直接将粗粒度低比特量化应用于 LLM 权重可能导致显着的精度下降。

    如表 1 所示，Llama 3.2 1B-Instruct 模型在 QNN 的 per-channel 量化和 AWQ per-group 4-bit 量化（均在 W4A16 设置下）下的准确性结果表明，per-channel 量化模型在具有挑战性的数学推理任务中遭受严重的性能下降。 不幸的是，由于测试时计算缩放方法应用于此类任务，因此 QNN 达到的基线准确性甚至未能满足性能缩放的最低要求。

2.  **Weak General Purpose Compute and Memory Bandwidth (通用计算能力和内存带宽弱)**:

    在没有对细粒度 group quantization 的原生硬件支持的情况下，一种常见的方法是依靠通用计算单元来处理此类计算。 然而，作者发现 NPU 中通用向量单元的计算和内存访问能力与专用矩阵单元之间存在显着差距。 使用 1024×1024×1024 GEMM 操作测量了 Hexagon V75 NPU 上 HVX 和 HMX 的 FP16 GEMM 性能，所有输入和输出都驻留在片上 TCM 中，以反映硬件的峰值性能。 如表 2 所示，矩阵单元的 FP16 GEMM 吞吐量高达 12 TFLOPS，比单个向量线程高 300 多倍。 在内存带宽方面，专用 DMA 引擎实现了超过 60 GB/s 的 DDR 读取带宽，而向量单元通过核心数据路径的内存读取带宽仍然低于 30 GB/s。 然而，DMA 提供的高带宽仅限于大型、规则的 1D 或 2D 数据块，无法有效地处理小型或不规则的内存访问。 这些观察结果表明，向量单元的通用计算和内存带宽不足以跟上专用矩阵单元的计算吞吐量，这对在细粒度量化下实现高性能混合精度 GEMM 内核提出了重大挑战。

    <图片 3> 展示了 Hexagon NPU 的架构，包括向量单元 HVX 和矩阵单元 HMX，以及共享的 L2 缓存和 TCM。 强调了 NPU 的混合架构以及内存子系统的特点。

    <图片 4> 展示了 FP16 HMX tile 的内存布局，每个 tile 包含一个 32 * 32 的矩阵，占据 2 KiB 的空间。 这种特殊的内存布局对后续的量化和计算提出了挑战。


## Design Overview


### Design Overview

本文提出了一种 LLM 推理系统，专为移动 NPU 设计，并针对测试时计算缩放工作负载进行了优化。

为了解决精度问题，主要权重采用 4-bit 细粒度分组量化，而激活保持浮点格式。 在运行时，动态地将权重反量化为浮点值，从而利用 NPU 强大的 FP16 矩阵计算能力，以有效地支持测试时计算缩放任务。

对于不可避免的通用计算（向量处理单元表现出有限的内存带宽和计算吞吐量），核心策略包括：

- 采用硬件感知的离线设计，以最大限度地减少运行时计算开销；
- 充分利用 SIMD 向量单元的内在能力，以弥合专用硬件和灵活软件需求之间的差距。

具体来说，引入了以下技术：

#### Hardware-aware Fine-grained Tile Quantization Scheme

提出了一种新颖的量化布局，该布局在细粒度的矩形 tile 中执行分组量化，而不是沿累积轴分组的传统方法。 为了与矩阵和向量单元的内存访问模式对齐，引入了一个离线 pipeline，包括权重预量化转换、量化和后量化转换。 这增强了运行时内存访问的连续性，并消除了不必要的计算开销。

#### Efficient LUT-Based Computation

对于更复杂的运行时操作，利用向量单元的查找表 (LUT) 指令和广义 LUT 机制来替换复杂的转换逻辑。 这种方法加速了测试时计算缩放工作负载中的关键瓶颈操作，包括混合精度 GEMM 中的反量化和 Attention 中的 Softmax 操作。


## System Design


### System Design

本节详细介绍了为移动端 NPU 设计的 LLM 推理系统，该系统针对测试时计算缩放（Test-time scaling）工作负载进行了优化。主要包括硬件感知的细粒度分块量化方案和基于 LUT 的高效计算方法。

#### Hardware-aware Fine-grained Tile Quantization Scheme

为了解决精度问题，该系统采用 4-bit 细粒度分组量化用于主要权重，同时保持激活值为浮点数。在运行时，动态地将权重反量化为浮点数值，利用 NPU 强大的 FP16 矩阵计算能力来高效地支持测试时计算缩放任务。

针对向量处理单元的通用计算，由于其内存带宽和计算吞吐量有限，核心策略包括：

*   采用硬件感知的离线设计，以最小化运行时计算开销。
*   充分利用 SIMD 向量单元的内在能力，以弥合专用硬件和灵活软件需求之间的差距。

具体来说，引入了以下技术：

##### Tile-Group Quantization

传统的量化 GEMM 中，权重矩阵通常以列优先布局存储，这与基于 CPU 的矩阵乘法中使用的向量点积运算对齐。权重被分成连续的量化组（通常大小为 32），沿着列维度。在该组内，值被量化，并且生成的整数权重，连同它们对应的比例和零点参数，以交错方式存储在内存中，保留了矩阵原始的列式顺序。

然而，在具有特殊矩阵单元的 NPU 上，传统的组布局通常与硬件要求不一致。如 <图片 6> 所示，在传统布局中连续的元素在片上 TCM 中变得分散。对于 SIMD 向量单元，这种非连续的访问模式是有问题的。虽然现代向量引擎提供 gather/scatter 操作来缓解分散的访问，但这些操作仍然很昂贵。简单地转置权重矩阵并不能解决这种不匹配，因为矩阵单元期望的复杂的多级数据布局仍然会导致非连续的内存访问。

为了解决这个问题，首先将权重排列成矩阵单元期望的布局，然后逐组应用 round-to-nearest 量化。对于大小为 32 的组，此方法有效地以 $2 \times 16$ tiles 为单位执行组量化。考虑到典型模型中的预训练权重近似地遵循零均值高斯分布，与传统分组相比，在这些重塑的 tile 组内进行量化不会显著改变每个组内的统计属性。因此，产生的量化误差保持可比性。

具体来说，在量化之前，根据 <图片 4> 中所示的布局排列权重，该布局分层结构化为两个级别：tile 的外部列优先排序，匹配矩阵单元的 tile 级内积运算，以及每个 tile 内每两行的内部混洗。然后在新的内存顺序中逐组量化权重。

##### Coalescing Quantization Groups for Wide Vector Accesses

默认情况下，量化的权重以结构数组（Array of Structures, AoS）布局存储。以 Q4\_0 对称量化为例，每组 32 个元素由 16 字节的 INT4 量化值和 2 字节的 FP16 比例值组成，量化值和比例值在内存中交错。由于 NPU 架构上的内存访问严重依赖于软件管理的本地 1D 或 2D 预取，因此避免了数组结构（Structure of Arrays, SoA）布局，其中量化值和比例值驻留在单独的大型连续数组中，以便更好地与硬件首选的访问模式对齐。

然而，细粒度的量化组与原生向量处理粒度不匹配：单个量化组太小，无法填充 128 字节宽的向量寄存器。访问这样的小组将需要多个内存操作或额外的指令来合并来自多个寄存器的数据，导致低效的内存带宽使用和计算开销。

为了解决这个问题，将 8 个量化组合并成一个更大的超级组，并重新组织其内容，使得来自 256 个连续元素的 INT4 值正好占据一个完整的 HVX 寄存器。这个过程如 <图片 7> 所示。

#### LUT-Based Computations

考虑到向量单元有限的通用计算性能，提出使用广义查找表（look-up table, LUT）指令来替换复杂的计算，从而减少指令数量和计算开销。基于 LUT 的计算对于加速测试时计算缩放工作负载中的关键操作特别有效，例如 Softmax 中的指数函数和反量化过程。

##### Fast Softmax via Vector Gather

测试时计算缩放方法通常会增加采样并行性，从而导致更大的批大小和更长的上下文长度。分析了这些缩放因子对基于 Transformer 的 LLM 在生成过程中主要算子的影响：

*   GEMM：基于先前描述的 NPU 硬件特性，适度增加测试时计算缩放工作负载中的批大小不会显著增加 GEMM 延迟。此外，GEMM 延迟与上下文长度无关。
*   Misc. Ops：对于诸如激活函数、LayerNorm、残差 Add 和 RoPE 之类的算子，虽然它们的计算开销大致与输入大小成正比，但由于它们的计算量和内存访问量都很小，因此忽略了它们的影响。
*   Attention：Attention 的理论计算复杂度随批大小和上下文长度而变化，使其成为测试时计算缩放场景中的潜在性能瓶颈。

在 Hexagon NPU 上使用 FP16 HMX 实现了 FlashAttention，并测量了其在提示长度为 4096 时的延迟组成，如 <图片 8> 所示。结果表明，矩阵乘法对整体延迟的贡献很小，而 Softmax 随着查询长度的增加而主导 Attention 的执行时间。

分析表明，片上 Softmax 的主要瓶颈在于指数计算，必须将其应用于 $\Theta(N_q \times N_{kv})$ 个元素。此外，这些昂贵的指数运算必须在 HVX 上执行，而 HVX 缺乏对特殊数学函数的专用硬件支持。按照常见的做法，用 exp2 替换 exp，并在 $QK^T$ 缩放因子 $\frac{1}{\sqrt{d}}$ 中吸收系数 $\log_2 e$。对于分解为整数部分 k 和小数部分 f 的输入元素 x，$2^f$ 使用泰勒级数多项式展开来近似，而 k 直接添加到 $2^f$ 的 IEEE-754 表示的指数字段中。然而，多项式评估涉及顺序依赖性，限制了 VLIW 架构下的指令级并行性。

为了缓解指数计算瓶颈，探索了用预先计算的查找表（LUT）替换显式指数计算。HVX 提供了 vgather 指令，该指令可以将 TCM 中分散位置的值收集到连续的 128 字节 TCM 区域中。虽然 vgather 可以实现大型 LUT，但使用 LUT 进行 exp 仍然具有挑战性：存储 2<sup>32</sup> 个元素用于 32 位浮点数是不切实际的。此外，vgather 本身会引入大量的延迟——在 Hexagon V75 上为 24 到 48 个指令包，因此必须最小化其使用。

为了实现实用的基于 LUT 的 exp，设计了以下方法。首先，在整个 FlashAttention 中广泛使用 FP16，片上计算过程如算法 1 所示。矩阵 S, P, O 和向量 $\vec{m}, \vec{l}$ 存储在 16 位浮点数中，exp 计算的输入和输出均为 16 位浮点数。特别是，FP16 HMX 在内部使用更高精度的浮点数进行累加，并将元素向上转换为 32 位精度，用于关键操作，例如矩阵 P 的逐行求和。

使用 16 位输入和输出将 LUT 限制为 65536 个条目，需要 128 KiB 的存储空间，这适合 TCM。vgather 的一个变体支持在一个指令中收集 64 个 2 字节的元素，最大地址偏移量为 65536 字节。但是，65536 个 FP16 条目占用 128 KiB，使得一半的条目无法通过直接寻址访问。为了解决这个问题，利用安全 softmax 的属性，该属性通过减去逐行最大值 $m_i$ 来确保 exp 的所有输入均为非正数。因此，只存储 $x \le 0$ 的值，从而产生一个具有 32768 个条目（64 KiB）的 LUT。在基于 LUT 的 exp 计算期间，忽略 FP16 输入的 MSB（符号位），并将输入左移一位以生成 vgather 所需的字节偏移量。

LUT 在系统初始化期间预先计算，在模型推理期间不会引入额外的开销。它占用 TCM 中固定的 64 KiB 区域，仅占总 TCM 容量的 $64KiB/8MiB \approx 0.8\%$，因此对其他操作的 TCM 可用性影响最小。

##### LUT-Centric Efficient Dequantization

运行时 HVX 反量化需要仔细设计，以避免额外的开销。提出了一种基于 HVX 查找表指令的高效反量化过程。vlut16 指令能够在源向量寄存器中为每个 8 位索引在 16 个元素的表中执行表查找。每个输入字节都转换为 16 位值，因此 vlut16 会产生一对寄存器。

通过表查找快速将 INT4 转换为 FP16。使用 vlut16 指令，直接将 4 位量化值转换为 [-8, 7] FP16 值，用于 Q4\_0 量化方案，避免了传统的 mask-unpack-convert 指令序列。<图片 9> 展示了两种方法的比较。对于 V79 之前的 Hexagon NPU，所有 HVX 浮点运算都会以一种称为 qfloat 的内部格式生成结果，这需要额外的指令才能转换回标准的 IEEE-754 格式。使用表查找消除了这些开销。这种以 LUT 为中心的设计可以通过简单地调整表内容来轻松支持不同的 4 位编码方案（例如，llama.cpp 中使用的 FP4、NF4、IQ4\_NL）。

通过表查找进行比例广播。一个 128 字节的 HVX 寄存器可以容纳两个大小为 32 的 FP16 量化组。因此，传统的方法是将标量比例广播到整个向量寄存器，然后连接两个寄存器，以便随后与量化值相乘。但是，通过使用四个组的比例作为 LUT 内容并应用预定义的常量索引，只需一条 vlut16 指令即可实现四个组的比例的广播。


## Implementation


### Implementation

本节介绍论文中推理系统的具体实现细节，该系统构建于 `llama.cpp` 之上，并针对移动端 NPU 进行了优化。

#### 总体框架

*   **基础框架**: 系统基于 `llama.cpp` [16] 实现，使用 C/C++ 和内联汇编编写，代码量约为 7K 行。
*   **编译工具**: 使用 Hexagon SDK (version 6.0.0.2) 中的 LLVM 工具链为 Hexagon NPU 生成代码。
*   **避免依赖**: 该系统不依赖于 Qualcomm 的 QNN，避免了静态固定形状计算图的限制，从而提高了灵活性。

#### 模块组成

系统主要由两个模块组成：

1.  **NPU 算子库**:
    *   编译成独立的 Hexagon DSP 共享对象。
    *   包含计算内核、电源管理、硬件资源管理和计算线程池。
2.  **CPU 端集成**:
    *   与 `llama.cpp` 在 CPU 端集成。
    *   使用 `rpcmem` 共享内存作为底层缓冲区类型。`rpcmem` 是内核 `dmabuf` 内存的封装，支持 CPU 和 NPU 之间共享物理内存。
    *   `libcdsprpc.so` 提供相关的分配、释放和映射接口。

#### CPU-NPU 协同

*   **共享内存**: 利用共享内存缓冲区，避免不必要的数据拷贝，并尽可能重用现有的内存管理系统。
*   **CPU 调度**: 未在 NPU 上实现的算子，可以调度到 CPU 上运行，实现与上层应用的无缝集成。

#### 通信机制

*   **FastRPC**: 在后端初始化阶段，调用 Hexagon SDK 的 FastRPC [50] 机制启动远程 NPU 会话，并初始化共享内存区域用于通信。
*   **轮询机制**: NPU 侧的线程持续轮询共享内存区域，以接收来自 CPU 的计算请求。
*   **缓存一致性**: 由于 Snapdragon SoC 上 CPU 和 NPU 之间只有单向一致性，因此在 CPU 写入数据到共享内存后，需要手动清除缓存，以保证 NPU 能够读取到最新的数据。模型激活的共享缓冲区也需要进行类似的缓存维护操作。


## Evaluation


### 实验评估

本节对论文中提出的方法进行了详细的实验评估，旨在验证在移动端 NPU 上进行测试时计算缩放（Test-Time Scaling, TTS）的有效性。

#### 实验设置

*   **设备**: 实验使用了三款安卓设备，搭载了不同型号的高通骁龙 SoC，具体型号和 NPU 架构如 <表格 3> 所示：
    *   OnePlus Ace3 (骁龙 8 Gen 2, V73)
    *   OnePlus 12 (骁龙 8 Gen 3, V75)
    *   OnePlus Ace5 Pro (骁龙 8 Elite, V79)
*   **模型**: 选择了 Qwen 2.5 和 Llama 3.2 模型家族中的不同尺寸模型，包括 1.5B、3B (Qwen 2.5) 和 1B、3B (Llama 3.2)。在评估 TTS 的性能-成本权衡时，额外考虑了 Qwen 2.5 7B 模型。数学推理任务使用了 Instruct 模型变体。对于 Best-of-N 搜索和 step-level beam search，使用 Skywork-1.5B-PRM 作为 outcome-reward 和 process-reward scorer。
*   **数据集和指标**: 使用 MATH500 和 GSM8K 数据集评估数学推理任务的 pass@1 准确率，使用 0-shot CoT prompt。其他准确率指标包括 WinoGrande 准确率、MMLU 准确率和 Wikitext-2 困惑度。
*   **基线**: 选择 llama.cpp 的 OpenCL 后端作为 GPU 基线系统，该后端针对骁龙 Adreno GPU 优化了 Q4\_0 矩阵乘法内核。
*   **设置**: 在 GEMM 的算子层面评估中，选择了 Qwen2.5-1.5B、Qwen2.5-3B、Llama3.2-1B 和 Llama3.2-3B 对应线性层的权重矩阵大小，包括 Attention 投影矩阵和 FFN 中的矩阵。大多数矩阵采用 Q4\_0 量化方案（4.5 BPW），FFN down 矩阵采用 Q8\_0 量化方案（8.5 BPW）。

#### 整体性能

*   **测试时计算缩放的精度-延迟权衡**: <图片 10> 展示了 TTS 方法的性能-成本权衡。使用 MATH500 和 GSM8K 的准确率作为生成质量的指标，使用设备上模型的平均解码延迟作为成本指标。结果表明，TTS 在特定配置下提供了更优越的 Pareto 前沿，实现了更好的性能-成本平衡。例如，在 Best-of-N 方法中，Qwen2.5 1.5B 和 3B 的缩放结果优于 3B 和 7B 模型的基线准确率。对于 Beam Search，Qwen2.5-1.5B 和 Llama3.2-1B 的效率与各自的 3B 变体相当或略好。
*   **设备上解码性能**: <图片 11> 展示了不同模型在不同批次大小下的设备上解码吞吐量。结果表明，随着批次大小的增加，系统的端到端解码吞吐量显著增加。这是因为 HMX 单元的空闲计算能力得到了利用。但是，解码吞吐量并没有完全线性地扩展，因为推理过程包含一些随着输入长度的增长而变慢的部分。具体来说，lm\_head 的权重和相关激活被放置在 CPU 上，这限制了解码速度。
*   **功耗和能耗**: <图片 12> 显示了 LLM 解码阶段的功耗和能耗。随着解码阶段批次大小的增加，运行 1.5B Qwen 模型的功耗增加，但设备的整体功耗仍在 5W 以内。相比之下，运行 3B Qwen 模型对应的功耗稳定在 4.3W 左右。能耗的缩放特性与解码延迟相似，因此用能耗代替成本指标也会产生相似的精度-成本权衡特性。
*   **与其他系统的比较**: <图片 13> 展示了该系统的解码和预填充性能，并与基于 GPU 的实现进行了比较，还添加了 FP16 QNN 的性能作为参考。在解码阶段，虽然 GPU 在批次大小为 1 时解码速度更快，但基于 NPU 的系统在更大的批次大小下表现出更高的解码吞吐量和更好的缩放特性，突出了在 TTS 工作负载中使用 NPU 的优势。该系统在预填充吞吐量方面也始终优于基于 GPU 的系统，在某些工作负载下实现了与专有 QNN 相当的性能。

#### 精度评估

*   **量化方案**: <表格 4> 评估了基于 HMX 布局的 tile 量化组和传统量化组对应的 Qwen2.5-1.5B 模型的准确率。结果表明，使用提出的量化布局的模型在 MMLU 中的准确率略高于使用传统布局的模型，并且在 Winogrande 和 Wikitext PPL 中只有略微下降。
*   **Attention 实现**: <表格 5> 显示了基于 LUT 的 FP16 Attention 和传统 FP32 Attention 对应的模型准确率。结果表明，用较低的 FP16 精度替换 Attention 中的非关键部分（除了累加）对模型的端到端准确率没有明显影响。

#### 消融实验

*   **Attention 中的 Softmax**: <图片 14> 显示了在不同的 attention 工作负载下，使用不同方法计算指数函数 exp 对应的片上 softmax 延迟。结果表明，基于 LUT 的指数计算比传统的 32 位浮点 exp 加速了 1.26 到 2.19 倍，比 16 位浮点 exp 加速了高达 1.60 倍。
*   **基于反量化的 GEMM**: <图片 15> 展示了 GEMM 反量化布局优化的消融实验。结果表明，与基线方法相比，该方法在不同的矩阵大小下实现了 9.65 到 19.04 倍的加速。

#### 开销和敏感性分析

*   **CPU 和内存使用情况**: <图片 16> 评估了在 OnePlus 12 上解码阶段 1.5B 和 3B Qwen2.5 模型的 CPU 利用率和内存消耗。
*   **Prompt 长度的影响**: <图片 17> 显示了 prompt 长度对解码吞吐量的影响。结果表明，在所有批次大小和两个模型中，随着 prompt 长度从 512 个 token 增加到 4096 个 token，解码吞吐量呈现出轻微的下降趋势。

#### 总结

实验结果表明，通过利用移动 NPU 的计算能力和 TTS 算法，小型设备上模型在生成质量和推理成本方面都有可能超过大型模型。


## Discussion and Conclusion


### Discussion

#### Generalizability to Other Hardwares

论文指出，NPUs 的 "vector + matrix" 架构具有一定的通用性，并且观察到 CPUs 和 NPUs 之间的界限逐渐模糊。 现代 CPUs 也开始集成专用的矩阵乘法单元，例如 Intel AMX 和 ARM SME，使其具有类似的 "vector + matrix" 架构。 此外，现代 AI 加速器通常在通用计算性能和专用低精度矩阵乘法能力之间存在显著差异（例如 NVIDIA GPUs）。 虽然具体的硬件架构可能不同，但论文提出的技术背后的核心思想具有广泛的适用性。

#### System Performance and Limitations

*   **Decoding Performance**: 当前系统的解码速度相对受限，主要是由于反量化的开销。 然而，这并不影响测试时缩放的有效性。 基于 QNN 的量化 GEMM 通常仅利用 DMA 和 HMX 组件，而不引入 HVX 计算开销。 类似于 T-MAC 的方法可能能够在 NPUs 上实现具有细粒度组量化的有效 GEMV，从而加速 LLM 解码过程。

*   **Prefill Performance**: 当前系统的 prefill 性能仍有改进空间。 将更多算子卸载到 NPU、通过算子融合减少内存访问和通信开销，以及优化矩阵乘法的 tiling 和 pipelining 策略，都有助于提高 prefill 性能。 这些优化将留待未来的工作。

*   **Model Size Constraints**: 当前的实现受到较旧设备上单个 NPU 会话的 32 位地址空间的限制。 采用多个 NPU 会话可能有助于缓解此问题。

#### Application Scope of Parallel Test-time Scaling

虽然并行测试时缩放方法目前在数学推理任务中占主导地位，但最近的研究表明，它们可以扩展到更广泛的推理和规划领域，具有很大的通用潜力。

### Conclusion

本文证明了利用移动 NPUs（特别是 Qualcomm Hexagon NPU）未充分利用的计算能力进行 LLMs 测试时缩放的可行性和有效性。 通过设计一个端到端推理系统，该系统结合了硬件感知 tile 量化、权重布局优化以及基于 LUT 的关键算子加速，表明使用测试时缩放增强的较小模型在准确性和延迟方面均优于传统部署的较大模型。 这种方法为在资源受限的移动设备上部署高性能语言模型提供了一条新途径，从而提高了设备上 AI 的效率和能力。
