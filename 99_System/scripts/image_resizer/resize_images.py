"""
å›¾ç‰‡å¼•ç”¨å°ºå¯¸è°ƒæ•´å·¥å…· - ä¸»å…¥å£

åŸºäºå›¾ç‰‡ä¿¡æ¯å¯†åº¦æ™ºèƒ½è°ƒæ•´ Markdown ä¸­çš„å›¾ç‰‡å¼•ç”¨å°ºå¯¸ã€‚

æ”¯æŒä¸‰ç§åˆ†ææ–¹æ¡ˆï¼š
1. blank_ratio - ç™½è‰²åƒç´ å æ¯”æ³•
2. edge_density - è¾¹ç¼˜æ£€æµ‹æ³•
3. entropy - ç†µå€¼æ³•

ä½¿ç”¨æ–¹å¼:
    # å¤„ç†å•ä¸ªæ–‡ä»¶
    python resize_images.py -i "ç¬”è®°.md" --dry-run

    # å¤„ç†ç›®å½•ä¸‹æ‰€æœ‰ç¬”è®°
    python resize_images.py -d "D:\\code\\ç»ˆç«¯æ¨ç†\\20_Classification" --dry-run

    # ä½¿ç”¨ç†µå€¼æ³•å¤„ç†ç›®å½•
    python resize_images.py -d "D:\\code\\ç»ˆç«¯æ¨ç†\\20_Classification" -A entropy
"""

import argparse
from pathlib import Path

from analyzers import (
    BlankRatioAnalyzer,
    EdgeDensityAnalyzer,
    EntropyAnalyzer,
)
from processor import process_markdown


# åˆ†æå™¨æ˜ å°„
ANALYZERS = {
    "blank_ratio": BlankRatioAnalyzer,
    "edge_density": EdgeDensityAnalyzer,
    "entropy": EntropyAnalyzer,
}

# ç¬”è®°æ–‡ä»¶åç¼€
NOTE_SUFFIX = "_ç¬”è®°.md"


def find_note_files(directory: Path) -> list[Path]:
    """
    é€’å½’æŸ¥æ‰¾ç›®å½•ä¸‹æ‰€æœ‰ç¬”è®°æ–‡ä»¶ã€‚

    Args:
        directory: æœç´¢ç›®å½•

    Returns:
        ç¬”è®°æ–‡ä»¶è·¯å¾„åˆ—è¡¨
    """
    note_files = []

    # é€’å½’æœç´¢æ‰€æœ‰ *_ç¬”è®°.md æ–‡ä»¶
    for md_file in directory.rglob("*_ç¬”è®°.md"):
        if md_file.is_file():
            note_files.append(md_file)

    return sorted(note_files)


def process_directory(
    directory: Path, analyzer, scale: float, min_effective_area: int, dry_run: bool
) -> dict:
    """
    å¤„ç†ç›®å½•ä¸‹æ‰€æœ‰ç¬”è®°æ–‡ä»¶ã€‚

    Returns:
        æ€»ä½“å¤„ç†ç»Ÿè®¡
    """
    note_files = find_note_files(directory)

    if not note_files:
        print(f"âŒ æœªæ‰¾åˆ°ç¬”è®°æ–‡ä»¶ (åŒ¹é… *{NOTE_SUFFIX})")
        return {}

    print(f"\n{'#'*60}")
    print(f"ğŸ“‚ æ‰«æç›®å½•: {directory}")
    print(f"ğŸ“„ æ‰¾åˆ° {len(note_files)} ä¸ªç¬”è®°æ–‡ä»¶")
    print(f"{'#'*60}")

    total_stats = {
        "files_processed": 0,
        "files_modified": 0,
        "total_images": 0,
        "total_compressed": 0,
    }

    for idx, note_file in enumerate(note_files, 1):
        print(f"\n[{idx}/{len(note_files)}] {note_file.relative_to(directory)}")

        stats = process_markdown(
            md_path=note_file,
            analyzer=analyzer,
            scale=scale,
            min_effective_area=min_effective_area,
            dry_run=dry_run,
        )

        total_stats["files_processed"] += 1
        total_stats["total_images"] += stats.get("found", 0)
        total_stats["total_compressed"] += stats.get("modified", 0)

        if stats.get("modified", 0) > 0:
            total_stats["files_modified"] += 1

    # æ‰“å°æ€»ä½“ç»Ÿè®¡
    print(f"\n{'#'*60}")
    print(f"ğŸ“Š æ€»ä½“ç»Ÿè®¡:")
    print(f"   å¤„ç†æ–‡ä»¶æ•°:   {total_stats['files_processed']}")
    print(f"   ä¿®æ”¹æ–‡ä»¶æ•°:   {total_stats['files_modified']}")
    print(f"   å¤„ç†å›¾ç‰‡æ•°:   {total_stats['total_images']}")
    print(f"   å‹ç¼©å›¾ç‰‡æ•°:   {total_stats['total_compressed']}")
    print(f"{'#'*60}")

    return total_stats


def main():
    parser = argparse.ArgumentParser(
        description="åŸºäºä¿¡æ¯å¯†åº¦çš„ Markdown å›¾ç‰‡å¼•ç”¨å°ºå¯¸è°ƒæ•´å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
åˆ†æå™¨è¯´æ˜:
  blank_ratio  - æ–¹æ¡ˆä¸€ï¼šç»Ÿè®¡ç™½è‰²/ç©ºç™½åƒç´ å æ¯”
  edge_density - æ–¹æ¡ˆäºŒï¼šé€šè¿‡è¾¹ç¼˜æ£€æµ‹è¯„ä¼°å†…å®¹ä¸°å¯Œåº¦
  entropy      - æ–¹æ¡ˆä¸‰ï¼šåŸºäºä¿¡æ¯ç†µè¯„ä¼°ä¿¡æ¯é‡

ç¤ºä¾‹:
  # å¤„ç†å•ä¸ªæ–‡ä»¶ (é¢„è§ˆæ¨¡å¼)
  python resize_images.py -i "ç¬”è®°.md" --dry-run
  
  # å¤„ç†ç›®å½•ä¸‹æ‰€æœ‰ç¬”è®° (é¢„è§ˆæ¨¡å¼)
  python resize_images.py -d "D:\\code\\ç»ˆç«¯æ¨ç†\\20_Classification" --dry-run
  
  # ä½¿ç”¨ç†µå€¼æ³•å¤„ç†ç›®å½•
  python resize_images.py -d "D:\\code\\ç»ˆç«¯æ¨ç†\\20_Classification" -A entropy
  
  # å®é™…æ‰§è¡Œä¿®æ”¹
  python resize_images.py -d "D:\\code\\ç»ˆç«¯æ¨ç†\\20_Classification" -A entropy --min-area 150000
        """,
    )

    # è¾“å…¥å‚æ•° (äºŒé€‰ä¸€)
    input_group = parser.add_mutually_exclusive_group()
    input_group.add_argument(
        "--input", "-i", type=str, help="è¾“å…¥çš„å•ä¸ª Markdown ç¬”è®°æ–‡ä»¶è·¯å¾„"
    )
    input_group.add_argument(
        "--input-dir", "-d", type=str, help="è¾“å…¥ç›®å½•ï¼Œé€’å½’å¤„ç†æ‰€æœ‰ *_ç¬”è®°.md æ–‡ä»¶"
    )

    parser.add_argument(
        "--analyzer",
        "-A",
        type=str,
        choices=list(ANALYZERS.keys()),
        default="entropy",
        help="åˆ†æå™¨ç±»å‹ (é»˜è®¤: entropy)",
    )

    parser.add_argument(
        "--min-area",
        type=int,
        default=200000,
        help="æœ€å°æœ‰æ•ˆé¢ç§¯é˜ˆå€¼ï¼Œä½äºæ­¤å€¼éœ€è¦å‹ç¼© (é»˜è®¤: 200000)",
    )

    parser.add_argument(
        "--scale",
        "-S",
        type=float,
        default=0.7,
        help="ç¼©å°å€ç‡ï¼Œå¦‚ 0.7 è¡¨ç¤ºç¼©å°åˆ°åŸæ¥çš„ 70%% (é»˜è®¤: 0.7)",
    )

    parser.add_argument(
        "--min-score",
        type=float,
        default=None,
        help="åˆ†æå™¨æœ€ä½åˆ†æ•°é˜ˆå€¼ (å¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨åˆ†æå™¨å†…ç½®å€¼)",
    )

    parser.add_argument(
        "--dry-run",
        "-n",
        action="store_true",
        help="é¢„è§ˆæ¨¡å¼ï¼Œä»…æ˜¾ç¤ºå°†è¦è¿›è¡Œçš„æ“ä½œï¼Œä¸å®é™…ä¿®æ”¹æ–‡ä»¶",
    )

    parser.add_argument(
        "--list-analyzers", action="store_true", help="åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„åˆ†æå™¨"
    )

    args = parser.parse_args()

    # åˆ—å‡ºåˆ†æå™¨
    if args.list_analyzers:
        print("\nå¯ç”¨çš„åˆ†æå™¨:\n")
        for name, cls in ANALYZERS.items():
            analyzer = cls()
            print(f"  {name:15s} - {analyzer.description}")
        print()
        return 0

    # æ£€æŸ¥æ˜¯å¦æä¾›äº†è¾“å…¥
    if not args.input and not args.input_dir:
        parser.error("éœ€è¦æä¾› --input/-i æˆ– --input-dir/-d å‚æ•°")

    # å‚æ•°éªŒè¯
    if not 0 < args.scale < 1:
        print(f"âš ï¸ è­¦å‘Š: ç¼©å°å€ç‡åº”åœ¨ 0-1 ä¹‹é—´ï¼Œå½“å‰å€¼: {args.scale}")

    # åˆ›å»ºåˆ†æå™¨
    analyzer_class = ANALYZERS[args.analyzer]
    analyzer_kwargs = {}
    if args.min_score is not None:
        analyzer_kwargs["min_score"] = args.min_score

    analyzer = analyzer_class(**analyzer_kwargs)

    # å¤„ç†å•ä¸ªæ–‡ä»¶
    if args.input:
        md_path = Path(args.input)
        if not md_path.exists():
            print(f"âŒ é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨ - {args.input}")
            return 1

        if not md_path.suffix.lower() == ".md":
            print(f"âš ï¸ è­¦å‘Š: æ–‡ä»¶ä¸æ˜¯ Markdown æ ¼å¼ - {args.input}")

        process_markdown(
            md_path=md_path,
            analyzer=analyzer,
            scale=args.scale,
            min_effective_area=args.min_area,
            dry_run=args.dry_run,
        )

    # å¤„ç†ç›®å½•
    elif args.input_dir:
        dir_path = Path(args.input_dir)
        if not dir_path.exists():
            print(f"âŒ é”™è¯¯: ç›®å½•ä¸å­˜åœ¨ - {args.input_dir}")
            return 1

        if not dir_path.is_dir():
            print(f"âŒ é”™è¯¯: ä¸æ˜¯ç›®å½• - {args.input_dir}")
            return 1

        process_directory(
            directory=dir_path,
            analyzer=analyzer,
            scale=args.scale,
            min_effective_area=args.min_area,
            dry_run=args.dry_run,
        )

    return 0


if __name__ == "__main__":
    exit(main())

"""
# å¤„ç†å•ä¸ªæ–‡ä»¶
python resize_images.py -i "ç¬”è®°.md" -A entropy --dry-run

# å¤„ç†ç›®å½•
python resize_images.py -d "D:\\code\\ç»ˆç«¯æ¨ç†\\20_Classification" -A entropy --dry-run
python resize_images.py -d "D:\code\ç»ˆç«¯æ¨ç†\20_Classification" -A entropy --min-area 150000 --scale 0.65
"""
